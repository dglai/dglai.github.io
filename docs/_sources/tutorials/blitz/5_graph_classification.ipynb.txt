{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Training a GNN for Graph Classification\n\nBy the end of this tutorial, you will be able to\n\n-  Load a DGL-provided graph classification dataset.\n-  Understand what *readout* function does.\n-  Understand how to create and use a minibatch of graphs.\n-  Build a GNN-based graph classification model.\n-  Train and evaluate the model on a DGL-provided dataset.\n\n(Time estimate: 18 minutes)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import os\n\nos.environ[\"DGLBACKEND\"] = \"pytorch\"\nimport dgl\nimport dgl.data\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Overview of Graph Classification with GNN\n\nGraph classification or regression requires a model to predict certain\ngraph-level properties of a single graph given its node and edge\nfeatures.  Molecular property prediction is one particular application.\n\nThis tutorial shows how to train a graph classification model for a\nsmall dataset from the paper [How Powerful Are Graph Neural\nNetworks](https://arxiv.org/abs/1810.00826)_.\n\n## Loading Data\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Generate a synthetic dataset with 10000 graphs, ranging from 10 to 500 nodes.\ndataset = dgl.data.GINDataset(\"PROTEINS\", self_loop=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The dataset is a set of graphs, each with node features and a single\nlabel. One can see the node feature dimensionality and the number of\npossible graph categories of ``GINDataset`` objects in ``dim_nfeats``\nand ``gclasses`` attributes.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(\"Node feature dimensionality:\", dataset.dim_nfeats)\nprint(\"Number of graph categories:\", dataset.gclasses)\n\n\nfrom dgl.dataloading import GraphDataLoader"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Defining Data Loader\n\nA graph classification dataset usually contains two types of elements: a\nset of graphs, and their graph-level labels. Similar to an image\nclassification task, when the dataset is large enough, we need to train\nwith mini-batches. When you train a model for image classification or\nlanguage modeling, you will use a ``DataLoader`` to iterate over the\ndataset. In DGL, you can use the ``GraphDataLoader``.\n\nYou can also use various dataset samplers provided in\n[torch.utils.data.sampler](https://pytorch.org/docs/stable/data.html#data-loading-order-and-sampler)_.\nFor example, this tutorial creates a training ``GraphDataLoader`` and\ntest ``GraphDataLoader``, using ``SubsetRandomSampler`` to tell PyTorch\nto sample from only a subset of the dataset.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from torch.utils.data.sampler import SubsetRandomSampler\n\nnum_examples = len(dataset)\nnum_train = int(num_examples * 0.8)\n\ntrain_sampler = SubsetRandomSampler(torch.arange(num_train))\ntest_sampler = SubsetRandomSampler(torch.arange(num_train, num_examples))\n\ntrain_dataloader = GraphDataLoader(\n    dataset, sampler=train_sampler, batch_size=5, drop_last=False\n)\ntest_dataloader = GraphDataLoader(\n    dataset, sampler=test_sampler, batch_size=5, drop_last=False\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "You can try to iterate over the created ``GraphDataLoader`` and see what it\ngives:\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "it = iter(train_dataloader)\nbatch = next(it)\nprint(batch)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "As each element in ``dataset`` has a graph and a label, the\n``GraphDataLoader`` will return two objects for each iteration. The\nfirst element is the batched graph, and the second element is simply a\nlabel vector representing the category of each graph in the mini-batch.\nNext, we\u2019ll talked about the batched graph.\n\n## A Batched Graph in DGL\n\nIn each mini-batch, the sampled graphs are combined into a single bigger\nbatched graph via ``dgl.batch``. The single bigger batched graph merges\nall original graphs as separately connected components, with the node\nand edge features concatenated. This bigger graph is also a ``DGLGraph``\ninstance (so you can\nstill treat it as a normal ``DGLGraph`` object as in\n[here](2_dglgraph.ipynb)_). It however contains the information\nnecessary for recovering the original graphs, such as the number of\nnodes and edges of each graph element.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "batched_graph, labels = batch\nprint(\n    \"Number of nodes for each graph element in the batch:\",\n    batched_graph.batch_num_nodes(),\n)\nprint(\n    \"Number of edges for each graph element in the batch:\",\n    batched_graph.batch_num_edges(),\n)\n\n# Recover the original graph elements from the minibatch\ngraphs = dgl.unbatch(batched_graph)\nprint(\"The original graphs in the minibatch:\")\nprint(graphs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Define Model\n\nThis tutorial will build a two-layer [Graph Convolutional Network\n(GCN)](http://tkipf.github.io/graph-convolutional-networks/)_. Each of\nits layer computes new node representations by aggregating neighbor\ninformation. If you have gone through the\n:doc:`introduction <1_introduction>`, you will notice two\ndifferences:\n\n-  Since the task is to predict a single category for the *entire graph*\n   instead of for every node, you will need to aggregate the\n   representations of all the nodes and potentially the edges to form a\n   graph-level representation. Such process is more commonly referred as\n   a *readout*. A simple choice is to average the node features of a\n   graph with ``dgl.mean_nodes()``.\n\n-  The input graph to the model will be a batched graph yielded by the\n   ``GraphDataLoader``. The readout functions provided by DGL can handle\n   batched graphs so that they will return one representation for each\n   minibatch element.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from dgl.nn import GraphConv\n\n\nclass GCN(nn.Module):\n    def __init__(self, in_feats, h_feats, num_classes):\n        super(GCN, self).__init__()\n        self.conv1 = GraphConv(in_feats, h_feats)\n        self.conv2 = GraphConv(h_feats, num_classes)\n\n    def forward(self, g, in_feat):\n        h = self.conv1(g, in_feat)\n        h = F.relu(h)\n        h = self.conv2(g, h)\n        g.ndata[\"h\"] = h\n        return dgl.mean_nodes(g, \"h\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Training Loop\n\nThe training loop iterates over the training set with the\n``GraphDataLoader`` object and computes the gradients, just like\nimage classification or language modeling.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Create the model with given dimensions\nmodel = GCN(dataset.dim_nfeats, 16, dataset.gclasses)\noptimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n\nfor epoch in range(20):\n    for batched_graph, labels in train_dataloader:\n        pred = model(batched_graph, batched_graph.ndata[\"attr\"].float())\n        loss = F.cross_entropy(pred, labels)\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\nnum_correct = 0\nnum_tests = 0\nfor batched_graph, labels in test_dataloader:\n    pred = model(batched_graph, batched_graph.ndata[\"attr\"].float())\n    num_correct += (pred.argmax(1) == labels).sum().item()\n    num_tests += len(labels)\n\nprint(\"Test accuracy:\", num_correct / num_tests)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## What\u2019s next\n\n-  See [GIN\n   example](https://github.com/dmlc/dgl/tree/master/examples/pytorch/gin)_\n   for an end-to-end graph classification model.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Thumbnail credits: DGL\n# sphinx_gallery_thumbnail_path = '_static/blitz_5_graph_classification.png'"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.14"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}