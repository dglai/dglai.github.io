{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Write your own GNN module\n\nSometimes, your model goes beyond simply stacking existing GNN modules.\nFor example, you would like to invent a new way of aggregating neighbor\ninformation by considering node importance or edge weights.\n\nBy the end of this tutorial you will be able to\n\n-  Understand DGL\u2019s message passing APIs.\n-  Implement GraphSAGE convolution module by your own.\n\nThis tutorial assumes that you already know :doc:`the basics of training a\nGNN for node classification <1_introduction>`.\n\n(Time estimate: 10 minutes)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import os\n\nos.environ[\"DGLBACKEND\"] = \"pytorch\"\nimport dgl\nimport dgl.function as fn\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Message passing and GNNs\n\nDGL follows the *message passing paradigm* inspired by the Message\nPassing Neural Network proposed by [Gilmer et\nal.](https://arxiv.org/abs/1704.01212)_ Essentially, they found many\nGNN models can fit into the following framework:\n\n\\begin{align}m_{u\\to v}^{(l)} = M^{(l)}\\left(h_v^{(l-1)}, h_u^{(l-1)}, e_{u\\to v}^{(l-1)}\\right)\\end{align}\n\n\\begin{align}m_{v}^{(l)} = \\sum_{u\\in\\mathcal{N}(v)}m_{u\\to v}^{(l)}\\end{align}\n\n\\begin{align}h_v^{(l)} = U^{(l)}\\left(h_v^{(l-1)}, m_v^{(l)}\\right)\\end{align}\n\nwhere DGL calls $M^{(l)}$ the *message function*, $\\sum$ the\n*reduce function* and $U^{(l)}$ the *update function*. Note that\n$\\sum$ here can represent any function and is not necessarily a\nsummation.\n\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "For example, the [GraphSAGE convolution (Hamilton et al.,\n2017)](https://cs.stanford.edu/people/jure/pubs/graphsage-nips17.pdf)_\ntakes the following mathematical form:\n\n\\begin{align}h_{\\mathcal{N}(v)}^k\\leftarrow \\text{Average}\\{h_u^{k-1},\\forall u\\in\\mathcal{N}(v)\\}\\end{align}\n\n\\begin{align}h_v^k\\leftarrow \\text{ReLU}\\left(W^k\\cdot \\text{CONCAT}(h_v^{k-1}, h_{\\mathcal{N}(v)}^k) \\right)\\end{align}\n\nYou can see that message passing is directional: the message sent from\none node $u$ to other node $v$ is not necessarily the same\nas the other message sent from node $v$ to node $u$ in the\nopposite direction.\n\nAlthough DGL has builtin support of GraphSAGE via\n:class:`dgl.nn.SAGEConv <dgl.nn.pytorch.SAGEConv>`,\nhere is how you can implement GraphSAGE convolution in DGL by your own.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "class SAGEConv(nn.Module):\n    \"\"\"Graph convolution module used by the GraphSAGE model.\n\n    Parameters\n    ----------\n    in_feat : int\n        Input feature size.\n    out_feat : int\n        Output feature size.\n    \"\"\"\n\n    def __init__(self, in_feat, out_feat):\n        super(SAGEConv, self).__init__()\n        # A linear submodule for projecting the input and neighbor feature to the output.\n        self.linear = nn.Linear(in_feat * 2, out_feat)\n\n    def forward(self, g, h):\n        \"\"\"Forward computation\n\n        Parameters\n        ----------\n        g : Graph\n            The input graph.\n        h : Tensor\n            The input node feature.\n        \"\"\"\n        with g.local_scope():\n            g.ndata[\"h\"] = h\n            # update_all is a message passing API.\n            g.update_all(\n                message_func=fn.copy_u(\"h\", \"m\"),\n                reduce_func=fn.mean(\"m\", \"h_N\"),\n            )\n            h_N = g.ndata[\"h_N\"]\n            h_total = torch.cat([h, h_N], dim=1)\n            return self.linear(h_total)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The central piece in this code is the\n:func:`g.update_all <dgl.DGLGraph.update_all>`\nfunction, which gathers and averages the neighbor features. There are\nthree concepts here:\n\n* Message function ``fn.copy_u('h', 'm')`` that\n  copies the node feature under name ``'h'`` as *messages* with name\n  ``'m'`` sent to neighbors.\n\n* Reduce function ``fn.mean('m', 'h_N')`` that averages\n  all the received messages under name ``'m'`` and saves the result as a\n  new node feature ``'h_N'``.\n\n* ``update_all`` tells DGL to trigger the\n  message and reduce functions for all the nodes and edges.\n\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Afterwards, you can stack your own GraphSAGE convolution layers to form\na multi-layer GraphSAGE network.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "class Model(nn.Module):\n    def __init__(self, in_feats, h_feats, num_classes):\n        super(Model, self).__init__()\n        self.conv1 = SAGEConv(in_feats, h_feats)\n        self.conv2 = SAGEConv(h_feats, num_classes)\n\n    def forward(self, g, in_feat):\n        h = self.conv1(g, in_feat)\n        h = F.relu(h)\n        h = self.conv2(g, h)\n        return h"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Training loop\nThe following code for data loading and training loop is directly copied\nfrom the introduction tutorial.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import dgl.data\n\ndataset = dgl.data.CoraGraphDataset()\ng = dataset[0]\n\n\ndef train(g, model):\n    optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n    all_logits = []\n    best_val_acc = 0\n    best_test_acc = 0\n\n    features = g.ndata[\"feat\"]\n    labels = g.ndata[\"label\"]\n    train_mask = g.ndata[\"train_mask\"]\n    val_mask = g.ndata[\"val_mask\"]\n    test_mask = g.ndata[\"test_mask\"]\n    for e in range(200):\n        # Forward\n        logits = model(g, features)\n\n        # Compute prediction\n        pred = logits.argmax(1)\n\n        # Compute loss\n        # Note that we should only compute the losses of the nodes in the training set,\n        # i.e. with train_mask 1.\n        loss = F.cross_entropy(logits[train_mask], labels[train_mask])\n\n        # Compute accuracy on training/validation/test\n        train_acc = (pred[train_mask] == labels[train_mask]).float().mean()\n        val_acc = (pred[val_mask] == labels[val_mask]).float().mean()\n        test_acc = (pred[test_mask] == labels[test_mask]).float().mean()\n\n        # Save the best validation accuracy and the corresponding test accuracy.\n        if best_val_acc < val_acc:\n            best_val_acc = val_acc\n            best_test_acc = test_acc\n\n        # Backward\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n        all_logits.append(logits.detach())\n\n        if e % 5 == 0:\n            print(\n                \"In epoch {}, loss: {:.3f}, val acc: {:.3f} (best {:.3f}), test acc: {:.3f} (best {:.3f})\".format(\n                    e, loss, val_acc, best_val_acc, test_acc, best_test_acc\n                )\n            )\n\n\nmodel = Model(g.ndata[\"feat\"].shape[1], 16, dataset.num_classes)\ntrain(g, model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## More customization\n\nIn DGL, we provide many built-in message and reduce functions under the\n``dgl.function`` package. You can find more details in `the API\ndoc <apifunction>`.\n\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "These APIs allow one to quickly implement new graph convolution modules.\nFor example, the following implements a new ``SAGEConv`` that aggregates\nneighbor representations using a weighted average. Note that ``edata``\nmember can hold edge features which can also take part in message\npassing.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "class WeightedSAGEConv(nn.Module):\n    \"\"\"Graph convolution module used by the GraphSAGE model with edge weights.\n\n    Parameters\n    ----------\n    in_feat : int\n        Input feature size.\n    out_feat : int\n        Output feature size.\n    \"\"\"\n\n    def __init__(self, in_feat, out_feat):\n        super(WeightedSAGEConv, self).__init__()\n        # A linear submodule for projecting the input and neighbor feature to the output.\n        self.linear = nn.Linear(in_feat * 2, out_feat)\n\n    def forward(self, g, h, w):\n        \"\"\"Forward computation\n\n        Parameters\n        ----------\n        g : Graph\n            The input graph.\n        h : Tensor\n            The input node feature.\n        w : Tensor\n            The edge weight.\n        \"\"\"\n        with g.local_scope():\n            g.ndata[\"h\"] = h\n            g.edata[\"w\"] = w\n            g.update_all(\n                message_func=fn.u_mul_e(\"h\", \"w\", \"m\"),\n                reduce_func=fn.mean(\"m\", \"h_N\"),\n            )\n            h_N = g.ndata[\"h_N\"]\n            h_total = torch.cat([h, h_N], dim=1)\n            return self.linear(h_total)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Because the graph in this dataset does not have edge weights, we\nmanually assign all edge weights to one in the ``forward()`` function of\nthe model. You can replace it with your own edge weights.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "class Model(nn.Module):\n    def __init__(self, in_feats, h_feats, num_classes):\n        super(Model, self).__init__()\n        self.conv1 = WeightedSAGEConv(in_feats, h_feats)\n        self.conv2 = WeightedSAGEConv(h_feats, num_classes)\n\n    def forward(self, g, in_feat):\n        h = self.conv1(g, in_feat, torch.ones(g.num_edges(), 1).to(g.device))\n        h = F.relu(h)\n        h = self.conv2(g, h, torch.ones(g.num_edges(), 1).to(g.device))\n        return h\n\n\nmodel = Model(g.ndata[\"feat\"].shape[1], 16, dataset.num_classes)\ntrain(g, model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Even more customization by user-defined function\n\nDGL allows user-defined message and reduce function for the maximal\nexpressiveness. Here is a user-defined message function that is\nequivalent to ``fn.u_mul_e('h', 'w', 'm')``.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def u_mul_e_udf(edges):\n    return {\"m\": edges.src[\"h\"] * edges.data[\"w\"]}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "``edges`` has three members: ``src``, ``data`` and ``dst``, representing\nthe source node feature, edge feature, and destination node feature for\nall edges.\n\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "You can also write your own reduce function. For example, the following\nis equivalent to the builtin ``fn.mean('m', 'h_N')`` function that averages\nthe incoming messages:\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def mean_udf(nodes):\n    return {\"h_N\": nodes.mailbox[\"m\"].mean(1)}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In short, DGL will group the nodes by their in-degrees, and for each\ngroup DGL stacks the incoming messages along the second dimension. You\ncan then perform a reduction along the second dimension to aggregate\nmessages.\n\nFor more details on customizing message and reduce function with\nuser-defined function, please refer to the `API\nreference <apiudf>`.\n\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Best practice of writing custom GNN modules\n\nDGL recommends the following practice ranked by preference:\n\n-  Use ``dgl.nn`` modules.\n-  Use ``dgl.nn.functional`` functions which contain lower-level complex\n   operations such as computing a softmax for each node over incoming\n   edges.\n-  Use ``update_all`` with builtin message and reduce functions.\n-  Use user-defined message or reduce functions.\n\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## What\u2019s next?\n\n-  `Writing Efficient Message Passing\n   Code <guide-message-passing-efficient>`.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Thumbnail credits: Representation Learning on Networks, Jure Leskovec, WWW 2018\n# sphinx_gallery_thumbnail_path = '_static/blitz_3_message_passing.png'"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.14"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}