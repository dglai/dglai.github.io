{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Distributed Link Prediction\n\nIn this tutorial, we will walk through the steps of performing distributed GNN training\nfor a link prediction task. This tutorial assumes that you have read the [Distributed Node Classification](https://docs.dgl.ai/en/latest/tutorials/dist/1_node_classification.html) and [Stochastic Training of GNN for Link Prediction](https://docs.dgl.ai/en/latest/tutorials/large/L2_large_link_prediction.html#sphx-glr-tutorials-large-l2-large-link-prediction-py). The general pipeline is shown below.\n\n.. figure:: https://data.dgl.ai/tutorial/link.png\n   :alt: Imgur\n\n\n## Partition a graph\n\nIn this tutorial, we will use [OGBL citation2 graph](https://ogb.stanford.edu/docs/linkprop/#ogbl-citation2)\nas an example to illustrate the graph partitioning. Let\u2019s first load the graph into a DGL graph and convert it \ninto a training graph, validation edges and test edges with :class:`~dgl.data.AsLinkPredDataset`.\n\n.. code-block:: python\n\n\n    import os\n    os.environ['DGLBACKEND'] = 'pytorch'\n    import dgl\n    import torch as th\n    from ogb.linkproppred import DglLinkPropPredDataset\n    data = DglLinkPropPredDataset(name='ogbl-citation2')\n    graph = data[0]\n    data = dgl.data.AsLinkPredDataset(data, [0.8, 0.1, 0.1])\n    graph_train = data[0]\n    dgl.distributed.partition_graph(graph_train, graph_name='ogbl-citation2', num_parts=4,\n                                out_path='4part_data',\n                                balance_edges=True)\n\n\n\nThen, we store the validation and test edges with the graph partitions.\n\n\n\n.. code-block:: python\n\n\n    import pickle\n    with open('4part_data/val.pkl', 'wb') as f:\n        pickle.dump(data.val_edges, f)\n    with open('4part_data/test.pkl', 'wb') as f:\n        pickle.dump(data.test_edges, f)\n\n\n\n## Distributed training script\n\nThe distributed link prediction script is very similar to distributed node classification script with just a few modifications.\n\n\n### Initialize network communication\n\nWe first initialize the network communication and Pytorch's distributed communication. \n\n```python\nimport dgl\nimport torch as th\ndgl.distributed.initialize(ip_config='ip_config.txt')\nth.distributed.init_process_group(backend='gloo')\n```\nThe configuration file `ip_config.txt` has the following format:\n\n```shell\nip_addr1 [port1]\nip_addr2 [port2]\n```\nEach row is a machine. The first column is the IP address and the second column is the port for\nconnecting to the DGL server on the machine. The port is optional and the default port is 30050.\n\n    \n\n\n### Reference to the distributed graph\n\nDGL's servers load the graph partitions automatically. After the servers load the partitions,\ntrainers connect to the servers and can start to reference to the distributed graph in the cluster as below.\n\n\n```python\ng = dgl.distributed.DistGraph('ogbl-citation2')\n```\nAs shown in the code, we refer to a distributed graph by its name. This name is basically the one passed\nto the `partition_graph` function as shown in the section above.\n\n### Get training and validation node IDs\n\nFor distributed training, each trainer can run its own set of training nodes. We can get the current graph in the trainer with its node ids and edge ids \nby invoking `node_split` and `edge_split`. We can also get the valid edges and test edges by loading the pickle files.\n\n\n\n```python\ntrain_eids = dgl.distributed.edge_split(th.ones((g.num_edges(),), dtype=th.bool), g.get_partition_book(), force_even=True)\ntrain_nids = dgl.distributed.node_split(th.ones((g.num_nodes(),), dtype=th.bool), g.get_partition_book())\nwith open('4part_data/val.pkl', 'rb') as f:\n    global_valid_eid = pickle.load(f)\nwith open('4part_data/test.pkl', 'rb') as f:\n    global_test_eid = pickle.load(f)\n```\n### Define a GNN model\n\nFor distributed training, we define a GNN model exactly in the same way as\n[mini-batch training](https://doc.dgl.ai/guide/minibatch.html#) or\n[full-graph training](https://doc.dgl.ai/guide/training-node.html#guide-training-node-classification).\nThe code below defines the GraphSage model.\n\n\n```python\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport dgl.nn as dglnn\nimport torch.optim as optim\n\nclass SAGE(nn.Module):\n    def __init__(self, in_feats, n_hidden, n_classes, n_layers):\n        super().__init__()\n        self.n_layers = n_layers\n        self.n_hidden = n_hidden\n        self.n_classes = n_classes\n        self.layers = nn.ModuleList()\n        self.layers.append(dglnn.SAGEConv(in_feats, n_hidden, 'mean'))\n        for i in range(1, n_layers - 1):\n            self.layers.append(dglnn.SAGEConv(n_hidden, n_hidden, 'mean'))\n        self.layers.append(dglnn.SAGEConv(n_hidden, n_classes, 'mean'))\n\n    def forward(self, blocks, x):\n        for l, (layer, block) in enumerate(zip(self.layers, blocks)):\n            x = layer(block, x)\n            if l != self.n_layers - 1:\n                x = F.relu(x)\n        return x\n\nnum_hidden = 256\nnum_labels = len(th.unique(g.ndata['labels'][0:g.num_nodes()]))\nnum_layers = 2\nlr = 0.001\nmodel = SAGE(g.ndata['feat'].shape[1], num_hidden, num_labels, num_layers)\nloss_fcn = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.parameters(), lr=lr)\n```\nFor distributed training, we need to convert the model into a distributed model with\nPytorch's `DistributedDataParallel`.\n\n\n```python\nmodel = th.nn.parallel.DistributedDataParallel(model)\n```\nWe also define an edge predictor :class:`~dgl.nn.pytorch.link.EdgePredictor` to predict the edge scores of pairs of node representations\n\n```python\nfrom dgl.nn import EdgePredictor\npredictor = EdgePredictor('dot')\n```\n### Distributed mini-batch sampler\n\nWe can use :class:`~dgl.dataloading.pytorch.DistEdgeDataLoader`, the distributed counterpart\nof :class:`~dgl.dataloading.pytorch.EdgeDataLoader`, to create a distributed mini-batch sampler for\nlink prediction. \n\n\n\n.. code-block:: python\n    sampler = dgl.dataloading.MultiLayerNeighborSampler([25,10])\n    dataloader = dgl.dataloading.DistEdgeDataLoader(\n        g=g,\n        eids=train_eids.numpy(),\n        graph_sampler=sampler,\n        negative_sampler=dgl.dataloading.negative_sampler.Uniform(5),\n        batch_size=args.batch_size,\n        shuffle=True,\n        drop_last=False)\n\n\n### Training loop\n\nThe training loop for distributed training is also exactly the same as the single-process training.\n\n\n```python\nimport sklearn.metrics\nimport numpy as np\n\nepoch = 0\nfor epoch in range(10):\n    for step, (input_nodes, pos_graph, neg_graph, mfgs) in enumerate(dataloader):\n        pos_graph = pos_graph\n        neg_graph = neg_graph\n        node_inputs = mfgs[0].srcdata[dgl.NID]\n        batch_inputs = g.ndata['feat'][node_inputs]\n\n        batch_pred = model(mfgs, batch_inputs)\n        pos_feature = batch_pred\n        pos_graph.ndata['h'] = batch_pred\n        pos_src, pos_dst = pos_graph.edges()\n        pos_score = predictor(pos_feature[pos_src], pos_feature[pos_dst])\n\n        neg_feature = batch_pred\n        neg_graph.ndata['h'] = batch_pred\n        neg_src, neg_dst = neg_graph.edges()\n        neg_score = predictor(neg_feature[pos_src], neg_feature[pos_dst])\n\n        score = th.cat([pos_score, neg_score])\n        label = th.cat([th.ones_like(pos_score), th.zeros_like(neg_score)])\n        loss = F.binary_cross_entropy_with_logits(score, label)\n\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n```\n### Inference\n\nIn the inference stage, we use the model after training loop to get the embedding of nodes.\n\n```python\ndef inference(model, graph, node_features, args):\n    with th.no_grad():\n        sampler = dgl.dataloading.MultiLayerNeighborSampler([25,10])\n        train_dataloader = dgl.dataloading.DistNodeDataLoader(\n            graph, th.arange(graph.num_nodes()), sampler,\n            batch_size=1024,\n            shuffle=False,\n            drop_last=False)\n\n        result = []\n        for input_nodes, output_nodes, mfgs in train_dataloader:\n            node_inputs = mfgs[0].srcdata[dgl.NID]\n            inputs = node_features[node_inputs]\n            result.append(model(mfgs, inputs))\n\n        return th.cat(result)\n\nnode_reprs = inference(model, g, g.ndata['feat'], args)\n```\nThe test edges is encoded as ((positive_edge_src, positive_edge_dst), (negative_edge_src, negative_edge_dst)). Therefore, we can get the ground truth with positive pairs and negative pairs. \n\n```python\ntest_pos_src = global_test_eid[0][0]\ntest_pos_dst = global_test_eid[0][1]\ntest_neg_src = global_test_eid[1][0]\ntest_neg_dst = global_test_eid[1][1]\ntest_labels = th.cat([th.ones_like(test_pos_src), th.zeros_like(test_neg_src)]).cpu().numpy()\n```\nThen, we use the dot product predictor to get the score of positive and negative test pairs to compute metrics such as AUC:\n\n```python\nh_pos_src = node_reprs[test_pos_src]\nh_pos_dst = node_reprs[test_pos_dst]\nh_neg_src = node_reprs[test_neg_src]\nh_neg_dst = node_reprs[test_neg_dst]\nscore_pos = predictor(h_pos_src, h_pos_dst)\nscore_neg = predictor(h_neg_src, h_neg_dst)\n\ntest_preds = th.cat([score_pos, score_neg]).cpu().numpy()\nauc = skm.roc_auc_score(test_labels, test_preds)\n```\n## Set up distributed training environment\n\nThe distributed training environment set up is similar to the distributed node classification. Please refer here for more details:\n[Set up distributed training environment](https://docs.dgl.ai/en/latest/tutorials/dist/1_node_classification.html#set-up-distributed-training-environment)\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.14"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}