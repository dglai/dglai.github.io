{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n\n# Transformer as a Graph Neural Network\n\n**Author**: Zihao Ye, Jinjing Zhou, Qipeng Guo, Quan Gan, Zheng Zhang\n\n<div class=\"alert alert-danger\"><h4>Warning</h4><p>The tutorial aims at gaining insights into the paper, with code as a mean\n    of explanation. The implementation thus is NOT optimized for running\n    efficiency. For recommended implementation, please refer to the [official\n    examples](https://github.com/dmlc/dgl/tree/master/examples).</p></div>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In this tutorial, you learn about a simplified implementation of the Transformer model.\nYou can see highlights of the most important design points. For instance, there is\nonly single-head attention. The complete code can be found\n[here](https://github.com/dmlc/dgl/tree/master/examples/pytorch/transformer)_.\n\nThe overall structure is similar to the one from the research papaer [Annotated\nTransformer](http://nlp.seas.harvard.edu/2018/04/03/attention.html)_.\n\nThe Transformer model, as a replacement of CNN/RNN architecture for\nsequence modeling, was introduced in the research paper: [Attention is All\nYou Need](https://arxiv.org/pdf/1706.03762.pdf)_. It improved the\nstate of the art for machine translation as well as natural language\ninference task\n([GPT](https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf)_).\nRecent work on pre-training Transformer with large scale corpus\n([BERT](https://arxiv.org/pdf/1810.04805.pdf)_) supports that it is\ncapable of learning high-quality semantic representation.\n\nThe interesting part of Transformer is its extensive employment of\nattention. The classic use of attention comes from machine translation\nmodel, where the output token attends to all input tokens.\n\nTransformer additionally applies *self-attention* in both decoder and\nencoder. This process forces words relate to each other to combine\ntogether, irrespective of their positions in the sequence. This is\ndifferent from RNN-based model, where words (in the source sentence) are\ncombined along the chain, which is thought to be too constrained.\n\n## Attention layer of Transformer\n\nIn the attention layer of Transformer, for each node the module learns to\nassign weights on its in-coming edges. For node pair $(i, j)$\n(from $i$ to $j$) with node\n$x_i, x_j \\in \\mathbb{R}^n$, the score of their connection is\ndefined as follows:\n\n\\begin{align}q_j = W_q\\cdot x_j \\\\\n   k_i = W_k\\cdot x_i\\\\\n   v_i = W_v\\cdot x_i\\\\\n   \\textrm{score} = q_j^T k_i\\end{align}\n\nwhere $W_q, W_k, W_v \\in \\mathbb{R}^{n\\times d_k}$ map the\nrepresentations $x$ to \u201cquery\u201d, \u201ckey\u201d, and \u201cvalue\u201d space\nrespectively.\n\nThere are other possibilities to implement the score function. The dot\nproduct measures the similarity of a given query $q_j$ and a key\n$k_i$: if $j$ needs the information stored in $i$, the\nquery vector at position $j$ ($q_j$) is supposed to be close\nto key vector at position $i$ ($k_i$).\n\nThe score is then used to compute the sum of the incoming values,\nnormalized over the weights of edges, stored in $\\textrm{wv}$.\nThen apply an affine layer to $\\textrm{wv}$ to get the output\n$o$:\n\n\\begin{align}w_{ji} = \\frac{\\exp\\{\\textrm{score}_{ji} \\}}{\\sum\\limits_{(k, i)\\in E}\\exp\\{\\textrm{score}_{ki} \\}} \\\\\n   \\textrm{wv}_i = \\sum_{(k, i)\\in E} w_{ki} v_k \\\\\n   o = W_o\\cdot \\textrm{wv} \\\\\\end{align}\n\n### Multi-head attention layer\n\nIn Transformer, attention is *multi-headed*. A head is very much like a\nchannel in a convolutional network. The multi-head attention consists of\nmultiple attention heads, in which each head refers to a single\nattention module. $\\textrm{wv}^{(i)}$ for all the heads are\nconcatenated and mapped to output $o$ with an affine layer:\n\n\\begin{align}o = W_o \\cdot \\textrm{concat}\\left(\\left[\\textrm{wv}^{(0)}, \\textrm{wv}^{(1)}, \\cdots, \\textrm{wv}^{(h)}\\right]\\right)\\end{align}\n\nThe code below wraps necessary components for multi-head attention, and\nprovides two interfaces.\n\n-  ``get`` maps state \u2018x\u2019, to query, key and value, which is required by\n   following steps(\\ ``propagate_attention``).\n-  ``get_o`` maps the updated value after attention to the output\n   $o$ for post-processing.\n\n.. code::\n\n   class MultiHeadAttention(nn.Module):\n       \"Multi-Head Attention\"\n       def __init__(self, h, dim_model):\n           \"h: number of heads; dim_model: hidden dimension\"\n           super(MultiHeadAttention, self).__init__()\n           self.d_k = dim_model // h\n           self.h = h\n           # W_q, W_k, W_v, W_o\n           self.linears = clones(nn.Linear(dim_model, dim_model), 4)\n\n       def get(self, x, fields='qkv'):\n           \"Return a dict of queries / keys / values.\"\n           batch_size = x.shape[0]\n           ret = {}\n           if 'q' in fields:\n               ret['q'] = self.linears[0](x).view(batch_size, self.h, self.d_k)\n           if 'k' in fields:\n               ret['k'] = self.linears[1](x).view(batch_size, self.h, self.d_k)\n           if 'v' in fields:\n               ret['v'] = self.linears[2](x).view(batch_size, self.h, self.d_k)\n           return ret\n\n       def get_o(self, x):\n           \"get output of the multi-head attention\"\n           batch_size = x.shape[0]\n           return self.linears[3](x.view(batch_size, -1))\n\n\n## How DGL implements Transformer with a graph neural network\n\nYou get a different perspective of Transformer by treating the\nattention as edges in a graph and adopt message passing on the edges to\ninduce the appropriate processing.\n\n### Graph structure\n\nConstruct the graph by mapping tokens of the source and target\nsentence to nodes. The complete Transformer graph is made up of three\nsubgraphs:\n\n**Source language graph**. This is a complete graph, each\ntoken $s_i$ can attend to any other token $s_j$ (including\nself-loops). |image0|\n**Target language graph**. The graph is\nhalf-complete, in that $t_i$ attends only to $t_j$ if\n$i > j$ (an output token can not depend on future words). |image1|\n**Cross-language graph**. This is a bi-partitie graph, where there is\nan edge from every source token $s_i$ to every target token\n$t_j$, meaning every target token can attend on source tokens.\n|image2|\n\nThe full picture looks like this: |image3|\n\nPre-build the graphs in dataset preparation stage.\n\n### Message passing\n\nOnce you define the graph structure, move on to defining the\ncomputation for message passing.\n\nAssuming that you have already computed all the queries $q_i$, keys\n$k_i$ and values $v_i$. For each node $i$ (no matter\nwhether it is a source token or target token), you can decompose the\nattention computation into two steps:\n\n1. **Message computation:** Compute attention score\n   $\\mathrm{score}_{ij}$ between $i$ and all nodes $j$\n   to be attended over, by taking the scaled-dot product between\n   $q_i$ and $k_j$. The message sent from $j$ to\n   $i$ will consist of the score $\\mathrm{score}_{ij}$ and\n   the value $v_j$.\n2. **Message aggregation:** Aggregate the values $v_j$ from all\n   $j$ according to the scores $\\mathrm{score}_{ij}$.\n\n#### Simple implementation\n\n##### Message computation\n\nCompute ``score`` and send source node\u2019s ``v`` to destination\u2019s mailbox\n\n.. code::\n\n   def message_func(edges):\n       return {'score': ((edges.src['k'] * edges.dst['q'])\n                         .sum(-1, keepdim=True)),\n               'v': edges.src['v']}\n\n##### Message aggregation\n\nNormalize over all in-edges and weighted sum to get output\n\n.. code::\n\n   import torch as th\n   import torch.nn.functional as F\n\n   def reduce_func(nodes, d_k=64):\n       v = nodes.mailbox['v']\n       att = F.softmax(nodes.mailbox['score'] / th.sqrt(d_k), 1)\n       return {'dx': (att * v).sum(1)}\n\n##### Execute on specific edges\n\n.. code::\n\n   import functools.partial as partial\n   def naive_propagate_attention(self, g, eids):\n       g.send_and_recv(eids, message_func, partial(reduce_func, d_k=self.d_k))\n\n#### Speeding up with built-in functions\n\nTo speed up the message passing process, use DGL\u2019s built-in\nfunctions, including:\n\n- ``fn.src_mul_egdes(src_field, edges_field, out_field)`` multiplies\n  source\u2019s attribute and edges attribute, and send the result to the\n  destination node\u2019s mailbox keyed by ``out_field``.\n- ``fn.copy_e(edges_field, out_field)`` copies edge\u2019s attribute to\n  destination node\u2019s mailbox.\n- ``fn.sum(edges_field, out_field)`` sums up\n  edge\u2019s attribute and sends aggregation to destination node\u2019s mailbox.\n\nHere, you assemble those built-in functions into ``propagate_attention``,\nwhich is also the main graph operation function in the final\nimplementation. To accelerate it, break the ``softmax`` operation into\nthe following steps. Recall that for each head there are two phases.\n\n1. Compute attention score by multiply src node\u2019s ``k`` and dst node\u2019s\n   ``q``\n\n   -  ``g.apply_edges(src_dot_dst('k', 'q', 'score'), eids)``\n\n2. Scaled Softmax over all dst nodes\u2019 in-coming edges\n\n   -  Step 1: Exponentialize score with scale normalize constant\n\n      -  ``g.apply_edges(scaled_exp('score', np.sqrt(self.d_k)))``\n\n         .. math:: \\textrm{score}_{ij}\\leftarrow\\exp{\\left(\\frac{\\textrm{score}_{ij}}{ \\sqrt{d_k}}\\right)}\n\n   -  Step 2: Get the \u201cvalues\u201d on associated nodes weighted by \u201cscores\u201d\n      on in-coming edges of each node; get the sum of \u201cscores\u201d on\n      in-coming edges of each node for normalization. Note that here\n      $\\textrm{wv}$ is not normalized.\n\n      -  ``msg: fn.u_mul_e('v', 'score', 'v'), reduce: fn.sum('v', 'wv')``\n\n         .. math:: \\textrm{wv}_j=\\sum_{i=1}^{N} \\textrm{score}_{ij} \\cdot v_i\n\n      -  ``msg: fn.copy_e('score', 'score'), reduce: fn.sum('score', 'z')``\n\n         .. math:: \\textrm{z}_j=\\sum_{i=1}^{N} \\textrm{score}_{ij}\n\nThe normalization of $\\textrm{wv}$ is left to post processing.\n\n.. code::\n\n   def src_dot_dst(src_field, dst_field, out_field):\n       def func(edges):\n           return {out_field: (edges.src[src_field] * edges.dst[dst_field]).sum(-1, keepdim=True)}\n\n       return func\n\n   def scaled_exp(field, scale_constant):\n       def func(edges):\n           # clamp for softmax numerical stability\n           return {field: th.exp((edges.data[field] / scale_constant).clamp(-5, 5))}\n\n       return func\n\n\n   def propagate_attention(self, g, eids):\n       # Compute attention score\n       g.apply_edges(src_dot_dst('k', 'q', 'score'), eids)\n       g.apply_edges(scaled_exp('score', np.sqrt(self.d_k)))\n       # Update node state\n       g.send_and_recv(eids,\n                       [fn.u_mul_e('v', 'score', 'v'), fn.copy_e('score', 'score')],\n                       [fn.sum('v', 'wv'), fn.sum('score', 'z')])\n\n### Preprocessing and postprocessing\n\nIn Transformer, data needs to be pre- and post-processed before and\nafter the ``propagate_attention`` function.\n\n**Preprocessing** The preprocessing function ``pre_func`` first\nnormalizes the node representations and then map them to a set of\nqueries, keys and values, using self-attention as an example:\n\n\\begin{align}x \\leftarrow \\textrm{LayerNorm}(x) \\\\\n   [q, k, v] \\leftarrow [W_q, W_k, W_v ]\\cdot x\\end{align}\n\n**Postprocessing** The postprocessing function ``post_funcs`` completes\nthe whole computation correspond to one layer of the transformer: 1.\nNormalize $\\textrm{wv}$ and get the output of Multi-Head Attention\nLayer $o$.\n\n\\begin{align}\\textrm{wv} \\leftarrow \\frac{\\textrm{wv}}{z} \\\\\n   o \\leftarrow W_o\\cdot \\textrm{wv} + b_o\\end{align}\n\nadd residual connection:\n\n\\begin{align}x \\leftarrow x + o\\end{align}\n\n2. Applying a two layer position-wise feed forward layer on $x$\n   then add residual connection:\n\n   .. math::\n\n\n      x \\leftarrow x + \\textrm{LayerNorm}(\\textrm{FFN}(x))\n\n   where $\\textrm{FFN}$ refers to the feed forward function.\n\n.. code::\n\n   class Encoder(nn.Module):\n       def __init__(self, layer, N):\n           super(Encoder, self).__init__()\n           self.N = N\n           self.layers = clones(layer, N)\n           self.norm = LayerNorm(layer.size)\n\n       def pre_func(self, i, fields='qkv'):\n           layer = self.layers[i]\n           def func(nodes):\n               x = nodes.data['x']\n               norm_x = layer.sublayer[0].norm(x)\n               return layer.self_attn.get(norm_x, fields=fields)\n           return func\n\n       def post_func(self, i):\n           layer = self.layers[i]\n           def func(nodes):\n               x, wv, z = nodes.data['x'], nodes.data['wv'], nodes.data['z']\n               o = layer.self_attn.get_o(wv / z)\n               x = x + layer.sublayer[0].dropout(o)\n               x = layer.sublayer[1](x, layer.feed_forward)\n               return {'x': x if i < self.N - 1 else self.norm(x)}\n           return func\n\n   class Decoder(nn.Module):\n       def __init__(self, layer, N):\n           super(Decoder, self).__init__()\n           self.N = N\n           self.layers = clones(layer, N)\n           self.norm = LayerNorm(layer.size)\n\n       def pre_func(self, i, fields='qkv', l=0):\n           layer = self.layers[i]\n           def func(nodes):\n               x = nodes.data['x']\n               if fields == 'kv':\n                   norm_x = x # In enc-dec attention, x has already been normalized.\n               else:\n                   norm_x = layer.sublayer[l].norm(x)\n               return layer.self_attn.get(norm_x, fields)\n           return func\n\n       def post_func(self, i, l=0):\n           layer = self.layers[i]\n           def func(nodes):\n               x, wv, z = nodes.data['x'], nodes.data['wv'], nodes.data['z']\n               o = layer.self_attn.get_o(wv / z)\n               x = x + layer.sublayer[l].dropout(o)\n               if l == 1:\n                   x = layer.sublayer[2](x, layer.feed_forward)\n               return {'x': x if i < self.N - 1 else self.norm(x)}\n           return func\n\nThis completes all procedures of one layer of encoder and decoder in\nTransformer.\n\n<div class=\"alert alert-info\"><h4>Note</h4><p>The sublayer connection part is little bit different from the\n   original paper. However, this implementation is the same as [The Annotated\n   Transformer](http://nlp.seas.harvard.edu/2018/04/03/attention.html)_\n   and\n   [OpenNMT](https://github.com/OpenNMT/OpenNMT-py/blob/cd29c1dbfb35f4a2701ff52a1bf4e5bdcf02802e/onmt/encoders/transformer.py)_.</p></div>\n\n## Main class of Transformer graph\n\nThe processing flow of Transformer can be seen as a 2-stage\nmessage-passing within the complete graph (adding pre- and post-\nprocessing appropriately): 1) self-attention in encoder, 2)\nself-attention in decoder followed by cross-attention between encoder\nand decoder, as shown below. |image4|\n\n.. code:: python\n\n   class Transformer(nn.Module):\n       def __init__(self, encoder, decoder, src_embed, tgt_embed, pos_enc, generator, h, d_k):\n           super(Transformer, self).__init__()\n           self.encoder, self.decoder = encoder, decoder\n           self.src_embed, self.tgt_embed = src_embed, tgt_embed\n           self.pos_enc = pos_enc\n           self.generator = generator\n           self.h, self.d_k = h, d_k\n\n       def propagate_attention(self, g, eids):\n           # Compute attention score\n           g.apply_edges(src_dot_dst('k', 'q', 'score'), eids)\n           g.apply_edges(scaled_exp('score', np.sqrt(self.d_k)))\n           # Send weighted values to target nodes\n           g.send_and_recv(eids,\n                           [fn.u_mul_e('v', 'score', 'v'), fn.copy_e('score', 'score')],\n                           [fn.sum('v', 'wv'), fn.sum('score', 'z')])\n\n       def update_graph(self, g, eids, pre_pairs, post_pairs):\n           \"Update the node states and edge states of the graph.\"\n\n           # Pre-compute queries and key-value pairs.\n           for pre_func, nids in pre_pairs:\n               g.apply_nodes(pre_func, nids)\n           self.propagate_attention(g, eids)\n           # Further calculation after attention mechanism\n           for post_func, nids in post_pairs:\n               g.apply_nodes(post_func, nids)\n\n       def forward(self, graph):\n           g = graph.g\n           nids, eids = graph.nids, graph.eids\n\n           # Word Embedding and Position Embedding\n           src_embed, src_pos = self.src_embed(graph.src[0]), self.pos_enc(graph.src[1])\n           tgt_embed, tgt_pos = self.tgt_embed(graph.tgt[0]), self.pos_enc(graph.tgt[1])\n           g.nodes[nids['enc']].data['x'] = self.pos_enc.dropout(src_embed + src_pos)\n           g.nodes[nids['dec']].data['x'] = self.pos_enc.dropout(tgt_embed + tgt_pos)\n\n           for i in range(self.encoder.N):\n               # Step 1: Encoder Self-attention\n               pre_func = self.encoder.pre_func(i, 'qkv')\n               post_func = self.encoder.post_func(i)\n               nodes, edges = nids['enc'], eids['ee']\n               self.update_graph(g, edges, [(pre_func, nodes)], [(post_func, nodes)])\n\n           for i in range(self.decoder.N):\n               # Step 2: Dncoder Self-attention\n               pre_func = self.decoder.pre_func(i, 'qkv')\n               post_func = self.decoder.post_func(i)\n               nodes, edges = nids['dec'], eids['dd']\n               self.update_graph(g, edges, [(pre_func, nodes)], [(post_func, nodes)])\n               # Step 3: Encoder-Decoder attention\n               pre_q = self.decoder.pre_func(i, 'q', 1)\n               pre_kv = self.decoder.pre_func(i, 'kv', 1)\n               post_func = self.decoder.post_func(i, 1)\n               nodes_e, nodes_d, edges = nids['enc'], nids['dec'], eids['ed']\n               self.update_graph(g, edges, [(pre_q, nodes_d), (pre_kv, nodes_e)], [(post_func, nodes_d)])\n\n           return self.generator(g.ndata['x'][nids['dec']])\n\n\n<div class=\"alert alert-info\"><h4>Note</h4><p>By calling ``update_graph`` function, you can create your own\n   Transformer on any subgraphs with nearly the same code. This\n   flexibility enables us to discover new, sparse structures (c.f. local attention\n   mentioned [here](https://arxiv.org/pdf/1508.04025.pdf)_). Note in this\n   implementation you don't use mask or padding, which makes the logic\n   more clear and saves memory. The trade-off is that the implementation is\n   slower.</p></div>\n\n## Training\n\nThis tutorial does not cover several other techniques such as Label\nSmoothing and Noam Optimizations mentioned in the original paper. For\ndetailed description about these modules, read [The\nAnnotated\nTransformer](http://nlp.seas.harvard.edu/2018/04/03/attention.html)_\nwritten by Harvard NLP team.\n\n### Task and the dataset\n\nThe Transformer is a general framework for a variety of NLP tasks. This tutorial focuses\non the sequence to sequence learning: it\u2019s a typical case to illustrate how it works.\n\nAs for the dataset, there are two example tasks: copy and sort, together\nwith two real-world translation tasks: multi30k en-de task and wmt14\nen-de task.\n\n-  **copy dataset**: copy input sequences to output. (train/valid/test:\n   9000, 1000, 1000)\n-  **sort dataset**: sort input sequences as output. (train/valid/test:\n   9000, 1000, 1000)\n-  **Multi30k en-de**, translate sentences from En to De.\n   (train/valid/test: 29000, 1000, 1000)\n-  **WMT14 en-de**, translate sentences from En to De.\n   (Train/Valid/Test: 4500966/3000/3003)\n\n<div class=\"alert alert-info\"><h4>Note</h4><p>Training with wmt14 requires multi-GPU support and is not available. Contributions are welcome!</p></div>\n\n### Graph building\n\n**Batching** This is similar to the way you handle Tree-LSTM. Build a graph pool in\nadvance, including all possible combination of input lengths and output\nlengths. Then for each sample in a batch, call ``dgl.batch`` to batch\ngraphs of their sizes together in to a single large graph.\n\nYou can wrap the process of creating graph pool and building\nBatchedGraph in ``dataset.GraphPool`` and\n``dataset.TranslationDataset``.\n\n.. code:: python\n\n   graph_pool = GraphPool()\n\n   data_iter = dataset(graph_pool, mode='train', batch_size=1, devices=devices)\n   for graph in data_iter:\n       print(graph.nids['enc']) # encoder node ids\n       print(graph.nids['dec']) # decoder node ids\n       print(graph.eids['ee']) # encoder-encoder edge ids\n       print(graph.eids['ed']) # encoder-decoder edge ids\n       print(graph.eids['dd']) # decoder-decoder edge ids\n       print(graph.src[0]) # Input word index list\n       print(graph.src[1]) # Input positions\n       print(graph.tgt[0]) # Output word index list\n       print(graph.tgt[1]) # Ouptut positions\n       break\n\nOutput:\n\n.. code::\n\n   tensor([0, 1, 2, 3, 4, 5, 6, 7, 8], device='cuda:0')\n   tensor([ 9, 10, 11, 12, 13, 14, 15, 16, 17, 18], device='cuda:0')\n   tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n           18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,\n           36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,\n           54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71,\n           72, 73, 74, 75, 76, 77, 78, 79, 80], device='cuda:0')\n   tensor([ 81,  82,  83,  84,  85,  86,  87,  88,  89,  90,  91,  92,  93,  94,\n            95,  96,  97,  98,  99, 100, 101, 102, 103, 104, 105, 106, 107, 108,\n           109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122,\n           123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136,\n           137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150,\n           151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164,\n           165, 166, 167, 168, 169, 170], device='cuda:0')\n   tensor([171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184,\n           185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198,\n           199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212,\n           213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225],\n          device='cuda:0')\n   tensor([28, 25,  7, 26,  6,  4,  5,  9, 18], device='cuda:0')\n   tensor([0, 1, 2, 3, 4, 5, 6, 7, 8], device='cuda:0')\n   tensor([ 0, 28, 25,  7, 26,  6,  4,  5,  9, 18], device='cuda:0')\n   tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9], device='cuda:0')\n\n## Put it all together\n\nTrain a one-head transformer with one layer, 128 dimension on copy\ntask. Set other parameters to the default.\n\nInference module is not included in this tutorial. It\nrequires beam search. For a full implementation, see the [GitHub\nrepo](https://github.com/dmlc/dgl/tree/master/examples/pytorch/transformer)_.\n\n.. code:: python\n\n   from tqdm.auto import tqdm\n   import torch as th\n   import numpy as np\n\n   from loss import LabelSmoothing, SimpleLossCompute\n   from modules import make_model\n   from optims import NoamOpt\n   from dgl.contrib.transformer import get_dataset, GraphPool\n\n   def run_epoch(data_iter, model, loss_compute, is_train=True):\n       for i, g in tqdm(enumerate(data_iter)):\n           with th.set_grad_enabled(is_train):\n               output = model(g)\n               loss = loss_compute(output, g.tgt_y, g.n_tokens)\n       print('average loss: {}'.format(loss_compute.avg_loss))\n       print('accuracy: {}'.format(loss_compute.accuracy))\n\n   N = 1\n   batch_size = 128\n   devices = ['cuda' if th.cuda.is_available() else 'cpu']\n\n   dataset = get_dataset(\"copy\")\n   V = dataset.vocab_size\n   criterion = LabelSmoothing(V, padding_idx=dataset.pad_id, smoothing=0.1)\n   dim_model = 128\n\n   # Create model\n   model = make_model(V, V, N=N, dim_model=128, dim_ff=128, h=1)\n\n   # Sharing weights between Encoder & Decoder\n   model.src_embed.lut.weight = model.tgt_embed.lut.weight\n   model.generator.proj.weight = model.tgt_embed.lut.weight\n\n   model, criterion = model.to(devices[0]), criterion.to(devices[0])\n   model_opt = NoamOpt(dim_model, 1, 400,\n                       th.optim.Adam(model.parameters(), lr=1e-3, betas=(0.9, 0.98), eps=1e-9))\n   loss_compute = SimpleLossCompute\n\n   att_maps = []\n   for epoch in range(4):\n       train_iter = dataset(graph_pool, mode='train', batch_size=batch_size, devices=devices)\n       valid_iter = dataset(graph_pool, mode='valid', batch_size=batch_size, devices=devices)\n       print('Epoch: {} Training...'.format(epoch))\n       model.train(True)\n       run_epoch(train_iter, model,\n                 loss_compute(criterion, model_opt), is_train=True)\n       print('Epoch: {} Evaluating...'.format(epoch))\n       model.att_weight_map = None\n       model.eval()\n       run_epoch(valid_iter, model,\n                 loss_compute(criterion, None), is_train=False)\n       att_maps.append(model.att_weight_map)\n\n## Visualization\n\nAfter training, you can visualize the attention that the Transformer generates\non copy task.\n\n.. code:: python\n\n   src_seq = dataset.get_seq_by_id(VIZ_IDX, mode='valid', field='src')\n   tgt_seq = dataset.get_seq_by_id(VIZ_IDX, mode='valid', field='tgt')[:-1]\n   # visualize head 0 of encoder-decoder attention\n   att_animation(att_maps, 'e2d', src_seq, tgt_seq, 0)\n\n|image5| from the figure you see the decoder nodes gradually learns to\nattend to corresponding nodes in input sequence, which is the expected\nbehavior.\n\n### Multi-head attention\n\nBesides the attention of a one-head attention trained on toy task. We\nalso visualize the attention scores of Encoder\u2019s Self Attention,\nDecoder\u2019s Self Attention and the Encoder-Decoder attention of an\none-Layer Transformer network trained on multi-30k dataset.\n\nFrom the visualization you see the diversity of different heads, which is what you would\nexpect. Different heads learn different relations between word pairs.\n\n-  **Encoder Self-Attention** |image6|\n\n-  **Encoder-Decoder Attention** Most words in target sequence attend on\n   their related words in source sequence, for example: when generating\n   \u201cSee\u201d (in De), several heads attend on \u201clake\u201d; when generating\n   \u201cEisfischerh\u00fctte\u201d, several heads attend on \u201cice\u201d. |image7|\n\n-  **Decoder Self-Attention** Most words attend on their previous few\n   words. |image8|\n\n## Adaptive Universal Transformer\n\nA recent research paper by Google, [Universal\nTransformer](https://arxiv.org/pdf/1807.03819.pdf)_, is an example to\nshow how ``update_graph`` adapts to more complex updating rules.\n\nThe Universal Transformer was proposed to address the problem that\nvanilla Transformer is not computationally universal by introducing\nrecurrence in Transformer:\n\n-  The basic idea of Universal Transformer is to repeatedly revise its\n   representations of all symbols in the sequence with each recurrent\n   step by applying a Transformer layer on the representations.\n-  Compared to vanilla Transformer, Universal Transformer shares weights\n   among its layers, and it does not fix the recurrence time (which\n   means the number of layers in Transformer).\n\nA further optimization employs an [adaptive computation time\n(ACT)](https://arxiv.org/pdf/1603.08983.pdf)_ mechanism to allow the\nmodel to dynamically adjust the number of times the representation of\neach position in a sequence is revised (refereed to as **step**\nhereafter). This model is also known as the Adaptive Universal\nTransformer (AUT).\n\nIn AUT, you maintain an active nodes list. In each step $t$, we\ncompute a halting probability: $h (0<h<1)$ for all nodes in this\nlist by:\n\n\\begin{align}h^t_i = \\sigma(W_h x^t_i + b_h)\\end{align}\n\nthen dynamically decide which nodes are still active. A node is halted\nat time $T$ if and only if\n$\\sum_{t=1}^{T-1} h_t < 1 - \\varepsilon \\leq \\sum_{t=1}^{T}h_t$.\nHalted nodes are removed from the list. The procedure proceeds until the\nlist is empty or a pre-defined maximum step is reached. From DGL\u2019s\nperspective, this means that the \u201cactive\u201d graph becomes sparser over\ntime.\n\nThe final state of a node $s_i$ is a weighted average of\n$x_i^t$ by $h_i^t$:\n\n\\begin{align}s_i = \\sum_{t=1}^{T} h_i^t\\cdot x_i^t\\end{align}\n\nIn DGL, implement an algorithm by calling\n``update_graph`` on nodes that are still active and edges associated\nwith this nodes. The following code shows the Universal Transformer\nclass in DGL:\n\n.. code::\n\n   class UTransformer(nn.Module):\n       \"Universal Transformer(https://arxiv.org/pdf/1807.03819.pdf) with ACT(https://arxiv.org/pdf/1603.08983.pdf).\"\n       MAX_DEPTH = 8\n       thres = 0.99\n       act_loss_weight = 0.01\n       def __init__(self, encoder, decoder, src_embed, tgt_embed, pos_enc, time_enc, generator, h, d_k):\n           super(UTransformer, self).__init__()\n           self.encoder,  self.decoder = encoder, decoder\n           self.src_embed, self.tgt_embed = src_embed, tgt_embed\n           self.pos_enc, self.time_enc = pos_enc, time_enc\n           self.halt_enc = HaltingUnit(h * d_k)\n           self.halt_dec = HaltingUnit(h * d_k)\n           self.generator = generator\n           self.h, self.d_k = h, d_k\n\n       def step_forward(self, nodes):\n           # add positional encoding and time encoding, increment step by one\n           x = nodes.data['x']\n           step = nodes.data['step']\n           pos = nodes.data['pos']\n           return {'x': self.pos_enc.dropout(x + self.pos_enc(pos.view(-1)) + self.time_enc(step.view(-1))),\n                   'step': step + 1}\n\n       def halt_and_accum(self, name, end=False):\n           \"field: 'enc' or 'dec'\"\n           halt = self.halt_enc if name == 'enc' else self.halt_dec\n           thres = self.thres\n           def func(nodes):\n               p = halt(nodes.data['x'])\n               sum_p = nodes.data['sum_p'] + p\n               active = (sum_p < thres) & (1 - end)\n               _continue = active.float()\n               r = nodes.data['r'] * (1 - _continue) + (1 - sum_p) * _continue\n               s = nodes.data['s'] + ((1 - _continue) * r + _continue * p) * nodes.data['x']\n               return {'p': p, 'sum_p': sum_p, 'r': r, 's': s, 'active': active}\n           return func\n\n       def propagate_attention(self, g, eids):\n           # Compute attention score\n           g.apply_edges(src_dot_dst('k', 'q', 'score'), eids)\n           g.apply_edges(scaled_exp('score', np.sqrt(self.d_k)), eids)\n           # Send weighted values to target nodes\n           g.send_and_recv(eids,\n                           [fn.u_mul_e('v', 'score', 'v'), fn.copy_e('score', 'score')],\n                           [fn.sum('v', 'wv'), fn.sum('score', 'z')])\n\n       def update_graph(self, g, eids, pre_pairs, post_pairs):\n           \"Update the node states and edge states of the graph.\"\n           # Pre-compute queries and key-value pairs.\n           for pre_func, nids in pre_pairs:\n               g.apply_nodes(pre_func, nids)\n           self.propagate_attention(g, eids)\n           # Further calculation after attention mechanism\n           for post_func, nids in post_pairs:\n               g.apply_nodes(post_func, nids)\n\n       def forward(self, graph):\n           g = graph.g\n           N, E = graph.n_nodes, graph.n_edges\n           nids, eids = graph.nids, graph.eids\n\n           # embed & pos\n           g.nodes[nids['enc']].data['x'] = self.src_embed(graph.src[0])\n           g.nodes[nids['dec']].data['x'] = self.tgt_embed(graph.tgt[0])\n           g.nodes[nids['enc']].data['pos'] = graph.src[1]\n           g.nodes[nids['dec']].data['pos'] = graph.tgt[1]\n\n           # init step\n           device = next(self.parameters()).device\n           g.ndata['s'] = th.zeros(N, self.h * self.d_k, dtype=th.float, device=device)    # accumulated state\n           g.ndata['p'] = th.zeros(N, 1, dtype=th.float, device=device)                    # halting prob\n           g.ndata['r'] = th.ones(N, 1, dtype=th.float, device=device)                     # remainder\n           g.ndata['sum_p'] = th.zeros(N, 1, dtype=th.float, device=device)                # sum of pondering values\n           g.ndata['step'] = th.zeros(N, 1, dtype=th.long, device=device)                  # step\n           g.ndata['active'] = th.ones(N, 1, dtype=th.uint8, device=device)                # active\n\n           for step in range(self.MAX_DEPTH):\n               pre_func = self.encoder.pre_func('qkv')\n               post_func = self.encoder.post_func()\n               nodes = g.filter_nodes(lambda v: v.data['active'].view(-1), nids['enc'])\n               if len(nodes) == 0: break\n               edges = g.filter_edges(lambda e: e.dst['active'].view(-1), eids['ee'])\n               end = step == self.MAX_DEPTH - 1\n               self.update_graph(g, edges,\n                                 [(self.step_forward, nodes), (pre_func, nodes)],\n                                 [(post_func, nodes), (self.halt_and_accum('enc', end), nodes)])\n\n           g.nodes[nids['enc']].data['x'] = self.encoder.norm(g.nodes[nids['enc']].data['s'])\n\n           for step in range(self.MAX_DEPTH):\n               pre_func = self.decoder.pre_func('qkv')\n               post_func = self.decoder.post_func()\n               nodes = g.filter_nodes(lambda v: v.data['active'].view(-1), nids['dec'])\n               if len(nodes) == 0: break\n               edges = g.filter_edges(lambda e: e.dst['active'].view(-1), eids['dd'])\n               self.update_graph(g, edges,\n                                 [(self.step_forward, nodes), (pre_func, nodes)],\n                                 [(post_func, nodes)])\n\n               pre_q = self.decoder.pre_func('q', 1)\n               pre_kv = self.decoder.pre_func('kv', 1)\n               post_func = self.decoder.post_func(1)\n               nodes_e = nids['enc']\n               edges = g.filter_edges(lambda e: e.dst['active'].view(-1), eids['ed'])\n               end = step == self.MAX_DEPTH - 1\n               self.update_graph(g, edges,\n                                 [(pre_q, nodes), (pre_kv, nodes_e)],\n                                 [(post_func, nodes), (self.halt_and_accum('dec', end), nodes)])\n\n           g.nodes[nids['dec']].data['x'] = self.decoder.norm(g.nodes[nids['dec']].data['s'])\n           act_loss = th.mean(g.ndata['r']) # ACT loss\n\n           return self.generator(g.ndata['x'][nids['dec']]), act_loss * self.act_loss_weight\n\nCall ``filter_nodes`` and ``filter_edge`` to find nodes/edges\nthat are still active:\n\n<div class=\"alert alert-info\"><h4>Note</h4><p>- :func:`~dgl.DGLGraph.filter_nodes` takes a predicate and a node\n     ID list/tensor as input, then returns a tensor of node IDs that satisfy\n     the given predicate.\n   - :func:`~dgl.DGLGraph.filter_edges` takes a predicate\n     and an edge ID list/tensor as input, then returns a tensor of edge IDs\n     that satisfy the given predicate.</p></div>\n\nFor the full implementation, see the [GitHub\nrepo](https://github.com/dmlc/dgl/tree/master/examples/pytorch/transformer/modules/act.py)_.\n\nThe figure below shows the effect of Adaptive Computational\nTime. Different positions of a sentence were revised different times.\n\n|image9|\n\nYou can also visualize the dynamics of step distribution on nodes during the\ntraining of AUT on sort task(reach 99.7% accuracy), which demonstrates\nhow AUT learns to reduce recurrence steps during training. |image10|\n\n.. |image0| image:: https://i.imgur.com/zV5LmTX.png\n.. |image1| image:: https://i.imgur.com/dETQMMx.png\n.. |image2| image:: https://i.imgur.com/hnGP229.png\n.. |image3| image:: https://i.imgur.com/Hj2rRGT.png\n.. |image4| image:: https://i.imgur.com/zlUpJ41.png\n.. |image5| image:: https://s1.ax1x.com/2018/12/06/F126xI.gif\n.. |image6| image:: https://i.imgur.com/HjYb7F2.png\n.. |image7| image:: https://i.imgur.com/383J5O5.png\n.. |image8| image:: https://i.imgur.com/c0UWB1V.png\n.. |image9| image:: https://s1.ax1x.com/2018/12/06/F1sGod.png\n.. |image10| image:: https://s1.ax1x.com/2018/12/06/F1r8Cq.gif\n\n<div class=\"alert alert-info\"><h4>Note</h4><p>The notebook itself is not executable due to many dependencies.\n    Download [7_transformer.py](https://data.dgl.ai/tutorial/7_transformer.py)_,\n    and copy the python script to directory ``examples/pytorch/transformer``\n    then run ``python 7_transformer.py`` to see how it works.</p></div>\n\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.14"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}