{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Single Machine Multi-GPU Minibatch Graph Classification\n\nIn this tutorial, you will learn how to use multiple GPUs in training a\ngraph neural network (GNN) for graph classification. This tutorial assumes\nknowledge in GNNs for graph classification and we recommend you to check\n:doc:`Training a GNN for Graph Classification <../blitz/5_graph_classification>` otherwise.\n\n(Time estimate: 8 minutes)\n\nTo use a single GPU in training a GNN, we need to put the model, graph(s), and other\ntensors (e.g. labels) on the same GPU:\n\n.. code:: python\n\n    import torch\n\n    # Use the first GPU\n    device = torch.device(\"cuda:0\")\n    model = model.to(device)\n    graph = graph.to(device)\n    labels = labels.to(device)\n\nThe node and edge features in the graphs, if any, will also be on the GPU.\nAfter that, the forward computation, backward computation and parameter\nupdate will take place on the GPU. For graph classification, this repeats\nfor each minibatch gradient descent.\n\nUsing multiple GPUs allows performing more computation per unit of time. It\nis like having a team work together, where each GPU is a team member. We need\nto distribute the computation workload across GPUs and let them synchronize\nthe efforts regularly. PyTorch provides convenient APIs for this task with\nmultiple processes, one per GPU, and we can use them in conjunction with DGL.\n\nIntuitively, we can distribute the workload along the dimension of data. This\nallows multiple GPUs to perform the forward and backward computation of\nmultiple gradient descents in parallel. To distribute a dataset across\nmultiple GPUs, we need to partition it into multiple mutually exclusive\nsubsets of a similar size, one per GPU. We need to repeat the random\npartition every epoch to guarantee randomness. We can use\n:func:`~dgl.dataloading.pytorch.GraphDataLoader`, which wraps some PyTorch \nAPIs and does the job for graph classification in data loading.\n\nOnce all GPUs have finished the backward computation for its minibatch,\nwe need to synchronize the model parameter update across them. Specifically,\nthis involves collecting gradients from all GPUs, averaging them and updating\nthe model parameters on each GPU. We can wrap a PyTorch model with\n:func:`~torch.nn.parallel.DistributedDataParallel` so that the model\nparameter update will invoke gradient synchronization first under the hood.\n\n<img src=\"https://data.dgl.ai/tutorial/mgpu_gc.png\" width=\"450px\" align=\"center\">\n\nThat\u2019s the core behind this tutorial. We will explore it more in detail with\na complete example below.\n\n<div class=\"alert alert-info\"><h4>Note</h4><p>See [this tutorial](https://pytorch.org/tutorials/intermediate/ddp_tutorial.html)_\n   from PyTorch for general multi-GPU training with ``DistributedDataParallel``.</p></div>\n\n## Distributed Process Group Initialization\n\nFor communication between multiple processes in multi-gpu training, we need\nto start the distributed backend at the beginning of each process. We use\n`world_size` to refer to the number of processes and `rank` to refer to the\nprocess ID, which should be an integer from `0` to `world_size - 1`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import os\n\nos.environ[\"DGLBACKEND\"] = \"pytorch\"\nimport torch.distributed as dist\n\n\ndef init_process_group(world_size, rank):\n    dist.init_process_group(\n        backend=\"gloo\",  # change to 'nccl' for multiple GPUs\n        init_method=\"tcp://127.0.0.1:12345\",\n        world_size=world_size,\n        rank=rank,\n    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Data Loader Preparation\n\nWe split the dataset into training, validation and test subsets. In dataset\nsplitting, we need to use a same random seed across processes to ensure a\nsame split. We follow the common practice to train with multiple GPUs and\nevaluate with a single GPU, thus only set `use_ddp` to True in the\n:func:`~dgl.dataloading.pytorch.GraphDataLoader` for the training set, where\n`ddp` stands for :func:`~torch.nn.parallel.DistributedDataParallel`.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from dgl.data import split_dataset\nfrom dgl.dataloading import GraphDataLoader\n\n\ndef get_dataloaders(dataset, seed, batch_size=32):\n    # Use a 80:10:10 train-val-test split\n    train_set, val_set, test_set = split_dataset(\n        dataset, frac_list=[0.8, 0.1, 0.1], shuffle=True, random_state=seed\n    )\n    train_loader = GraphDataLoader(\n        train_set, use_ddp=True, batch_size=batch_size, shuffle=True\n    )\n    val_loader = GraphDataLoader(val_set, batch_size=batch_size)\n    test_loader = GraphDataLoader(test_set, batch_size=batch_size)\n\n    return train_loader, val_loader, test_loader"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Model Initialization\n\nFor this tutorial, we use a simplified Graph Isomorphism Network (GIN).\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\nimport torch.nn.functional as F\n\nfrom dgl.nn.pytorch import GINConv, SumPooling\n\n\nclass GIN(nn.Module):\n    def __init__(self, input_size=1, num_classes=2):\n        super(GIN, self).__init__()\n\n        self.conv1 = GINConv(\n            nn.Linear(input_size, num_classes), aggregator_type=\"sum\"\n        )\n        self.conv2 = GINConv(\n            nn.Linear(num_classes, num_classes), aggregator_type=\"sum\"\n        )\n        self.pool = SumPooling()\n\n    def forward(self, g, feats):\n        feats = self.conv1(g, feats)\n        feats = F.relu(feats)\n        feats = self.conv2(g, feats)\n\n        return self.pool(g, feats)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "To ensure same initial model parameters across processes, we need to set the\nsame random seed before model initialization. Once we construct a model\ninstance, we wrap it with :func:`~torch.nn.parallel.DistributedDataParallel`.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import torch\nfrom torch.nn.parallel import DistributedDataParallel\n\n\ndef init_model(seed, device):\n    torch.manual_seed(seed)\n    model = GIN().to(device)\n    if device.type == \"cpu\":\n        model = DistributedDataParallel(model)\n    else:\n        model = DistributedDataParallel(\n            model, device_ids=[device], output_device=device\n        )\n\n    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Main Function for Each Process\n\nDefine the model evaluation function as in the single-GPU setting.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def evaluate(model, dataloader, device):\n    model.eval()\n\n    total = 0\n    total_correct = 0\n\n    for bg, labels in dataloader:\n        bg = bg.to(device)\n        labels = labels.to(device)\n        # Get input node features\n        feats = bg.ndata.pop(\"attr\")\n        with torch.no_grad():\n            pred = model(bg, feats)\n        _, pred = torch.max(pred, 1)\n        total += len(labels)\n        total_correct += (pred == labels).sum().cpu().item()\n\n    return 1.0 * total_correct / total"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Define the run function for each process.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from torch.optim import Adam\n\n\ndef run(rank, world_size, dataset, seed=0):\n    init_process_group(world_size, rank)\n    if torch.cuda.is_available():\n        device = torch.device(\"cuda:{:d}\".format(rank))\n        torch.cuda.set_device(device)\n    else:\n        device = torch.device(\"cpu\")\n\n    model = init_model(seed, device)\n    criterion = nn.CrossEntropyLoss()\n    optimizer = Adam(model.parameters(), lr=0.01)\n\n    train_loader, val_loader, test_loader = get_dataloaders(dataset, seed)\n    for epoch in range(5):\n        model.train()\n        # The line below ensures all processes use a different\n        # random ordering in data loading for each epoch.\n        train_loader.set_epoch(epoch)\n\n        total_loss = 0\n        for bg, labels in train_loader:\n            bg = bg.to(device)\n            labels = labels.to(device)\n            feats = bg.ndata.pop(\"attr\")\n            pred = model(bg, feats)\n\n            loss = criterion(pred, labels)\n            total_loss += loss.cpu().item()\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n        loss = total_loss\n        print(\"Loss: {:.4f}\".format(loss))\n\n        val_acc = evaluate(model, val_loader, device)\n        print(\"Val acc: {:.4f}\".format(val_acc))\n\n    test_acc = evaluate(model, test_loader, device)\n    print(\"Test acc: {:.4f}\".format(test_acc))\n    dist.destroy_process_group()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Finally we load the dataset and launch the processes.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import torch.multiprocessing as mp\nfrom dgl.data import GINDataset\n\n\ndef main():\n    if not torch.cuda.is_available():\n        print(\"No GPU found!\")\n        return\n\n    num_gpus = torch.cuda.device_count()\n    dataset = GINDataset(name=\"IMDBBINARY\", self_loop=False)\n    mp.spawn(run, args=(num_gpus, dataset), nprocs=num_gpus)\n\n\nif __name__ == \"__main__\":\n    main()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.14"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}