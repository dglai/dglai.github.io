<!DOCTYPE html>
<html class="writer-html5" lang="en" data-content_root="../">
<head>
  <meta charset="utf-8" /><meta name="viewport" content="width=device-width, initial-scale=1" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>DataLoader &mdash; DGL 2.4 documentation</title>
      <link rel="stylesheet" type="text/css" href="../_static/pygments.css?v=80d5e7a1" />
      <link rel="stylesheet" type="text/css" href="../_static/css/theme.css?v=19f00094" />
      <link rel="stylesheet" type="text/css" href="../_static/graphviz.css?v=fd3f3429" />
      <link rel="stylesheet" type="text/css" href="../_static/copybutton.css?v=76b2166b" />
      <link rel="stylesheet" type="text/css" href="../_static/sg_gallery.css?v=61a4c737" />
      <link rel="stylesheet" type="text/css" href="../_static/sg_gallery-binder.css?v=f4aeca0c" />
      <link rel="stylesheet" type="text/css" href="../_static/sg_gallery-dataframe.css?v=2082cf3c" />
      <link rel="stylesheet" type="text/css" href="../_static/sg_gallery-rendered-html.css?v=1277b6f3" />
      <link rel="stylesheet" type="text/css" href="../_static/css/custom.css?v=0bf289b5" />

  
  <!--[if lt IE 9]>
    <script src="../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script src="../_static/jquery.js?v=5d32c60e"></script>
        <script src="../_static/_sphinx_javascript_frameworks_compat.js?v=2cd50e6c"></script>
        <script src="../_static/documentation_options.js?v=9caaf7ed"></script>
        <script src="../_static/doctools.js?v=9a2dae69"></script>
        <script src="../_static/sphinx_highlight.js?v=dc90522c"></script>
        <script src="../_static/clipboard.min.js?v=a7894cd8"></script>
        <script src="../_static/copybutton.js?v=ccdb6887"></script>
        <script crossorigin="anonymous" integrity="sha256-Ae2Vz/4ePdIu6ZyI/5ZGsYnb+m0JlOmKPjt6XZ9JJkA=" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
        <script src="https://cdn.jsdelivr.net/npm/@jupyter-widgets/html-manager@^1.0.1/dist/embed-amd.js"></script>
    <script src="../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="GraphDataLoader" href="dgl.dataloading.GraphDataLoader.html" />
    <link rel="prev" title="dgl.dataloading" href="../api/python/dgl.dataloading.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="../index.html" class="icon icon-home">
            DGL
          </a>
              <div class="version">
                2.4
              </div>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">Get Started</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../install/index.html">Install and Setup</a></li>
<li class="toctree-l1"><a class="reference internal" href="../tutorials/blitz/index.html">A Blitz Introduction to DGL</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Advanced Materials</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../stochastic_training/index.html">ğŸ†• Stochastic Training of GNNs with GraphBolt</a></li>
<li class="toctree-l1"><a class="reference internal" href="../guide/index.html">User Guide</a></li>
<li class="toctree-l1"><a class="reference internal" href="../guide_cn/index.html">ç”¨æˆ·æŒ‡å—ã€åŒ…å«è¿‡æ—¶ä¿¡æ¯ã€‘</a></li>
<li class="toctree-l1"><a class="reference internal" href="../guide_ko/index.html">ì‚¬ìš©ì ê°€ì´ë“œ[ì‹œëŒ€ì— ë’¤ì³ì§„]</a></li>
<li class="toctree-l1"><a class="reference internal" href="../graphtransformer/index.html">ğŸ†• Tutorial: Graph Transformer</a></li>
<li class="toctree-l1"><a class="reference internal" href="../notebooks/sparse/index.html">Tutorials: dgl.sparse</a></li>
<li class="toctree-l1"><a class="reference internal" href="../tutorials/cpu/index.html">Training on CPUs</a></li>
<li class="toctree-l1"><a class="reference internal" href="../tutorials/multi/index.html">Training on Multiple GPUs</a></li>
<li class="toctree-l1"><a class="reference internal" href="../tutorials/dist/index.html">Distributed training</a></li>
<li class="toctree-l1"><a class="reference internal" href="../tutorials/models/index.html">Paper Study with DGL</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">API Reference</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../api/python/dgl.html">dgl</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api/python/dgl.data.html">dgl.data</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="../api/python/dgl.dataloading.html">dgl.dataloading</a><ul class="current">
<li class="toctree-l2 current"><a class="reference internal" href="../api/python/dgl.dataloading.html#dataloaders">DataLoaders</a><ul class="current">
<li class="toctree-l3 current"><a class="current reference internal" href="#">DataLoader</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#dgl.dataloading.DataLoader"><code class="docutils literal notranslate"><span class="pre">DataLoader</span></code></a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="dgl.dataloading.GraphDataLoader.html">GraphDataLoader</a></li>
<li class="toctree-l3"><a class="reference internal" href="dgl.dataloading.DistNodeDataLoader.html">DistNodeDataLoader</a></li>
<li class="toctree-l3"><a class="reference internal" href="dgl.dataloading.DistEdgeDataLoader.html">DistEdgeDataLoader</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../api/python/dgl.dataloading.html#samplers">Samplers</a></li>
<li class="toctree-l2"><a class="reference internal" href="../api/python/dgl.dataloading.html#sampler-transformations">Sampler Transformations</a></li>
<li class="toctree-l2"><a class="reference internal" href="../api/python/dgl.dataloading.html#negative-samplers-for-link-prediction">Negative Samplers for Link Prediction</a></li>
<li class="toctree-l2"><a class="reference internal" href="../api/python/dgl.dataloading.html#utility-class-and-functions-for-feature-prefetching">Utility Class and Functions for Feature Prefetching</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../api/python/dgl.DGLGraph.html">dgl.DGLGraph</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api/python/dgl.distributed.html">dgl.distributed</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api/python/dgl.function.html">dgl.function</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api/python/dgl.geometry.html">dgl.geometry</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api/python/dgl.graphbolt.html">ğŸ†• dgl.graphbolt</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api/python/nn-pytorch.html">dgl.nn (PyTorch)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api/python/nn.functional.html">dgl.nn.functional</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api/python/dgl.ops.html">dgl.ops</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api/python/dgl.optim.html">dgl.optim</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api/python/dgl.sampling.html">dgl.sampling</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api/python/dgl.sparse_v0.html">dgl.sparse</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api/python/dgl.multiprocessing.html">dgl.multiprocessing</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api/python/transforms.html">dgl.transforms</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api/python/udf.html">User-defined Functions</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Notes</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../contribute.html">Contribute to DGL</a></li>
<li class="toctree-l1"><a class="reference internal" href="../developer/ffi.html">DGL Foreign Function Interface (FFI)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../performance.html">Performance Benchmarks</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Misc</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../faq.html">Frequently Asked Questions (FAQ)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../env_var.html">Environment Variables</a></li>
<li class="toctree-l1"><a class="reference internal" href="../resources.html">Resources</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">DGL</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../index.html" class="icon icon-home" aria-label="Home"></a></li>
          <li class="breadcrumb-item"><a href="../api/python/dgl.dataloading.html">dgl.dataloading</a></li>
      <li class="breadcrumb-item active">DataLoader</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../_sources/generated/dgl.dataloading.DataLoader.rst.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="dataloader">
<h1>DataLoader<a class="headerlink" href="#dataloader" title="Link to this heading">ïƒ</a></h1>
<dl class="py class">
<dt class="sig sig-object py" id="dgl.dataloading.DataLoader">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">dgl.dataloading.</span></span><span class="sig-name descname"><span class="pre">DataLoader</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">graph</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">indices</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">graph_sampler</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">device</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">use_ddp</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">ddp_seed</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">batch_size</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">drop_last</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">shuffle</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">use_prefetch_thread</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">use_alternate_streams</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">pin_prefetcher</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">use_uva</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">gpu_cache</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/dgl/dataloading/dataloader.html#DataLoader"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#dgl.dataloading.DataLoader" title="Link to this definition">ïƒ</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">DataLoader</span></code></p>
<p>Sampled graph data loader. Wrap a <a class="reference internal" href="../api/python/dgl.DGLGraph.html#dgl.DGLGraph" title="dgl.DGLGraph"><code class="xref py py-class docutils literal notranslate"><span class="pre">DGLGraph</span></code></a> and a
<a class="reference internal" href="dgl.dataloading.Sampler.html#dgl.dataloading.Sampler" title="dgl.dataloading.Sampler"><code class="xref py py-class docutils literal notranslate"><span class="pre">Sampler</span></code></a> into an iterable over mini-batches of samples.</p>
<p>DGLâ€™s <code class="docutils literal notranslate"><span class="pre">DataLoader</span></code> extends PyTorchâ€™s <code class="docutils literal notranslate"><span class="pre">DataLoader</span></code> by handling creation
and transmission of graph samples. It supports iterating over a set of nodes,
edges or any kinds of indices to get samples in the form of <code class="docutils literal notranslate"><span class="pre">DGLGraph</span></code>, message
flow graphs (MFGS), or any other structures necessary to train a graph neural network.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>graph</strong> (<a class="reference internal" href="../api/python/dgl.DGLGraph.html#dgl.DGLGraph" title="dgl.DGLGraph"><em>DGLGraph</em></a>) â€“ The graph.</p></li>
<li><p><strong>indices</strong> (<em>Tensor</em><em> or </em><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#dict" title="(in Python v3.12)"><em>dict</em></a><em>[</em><em>ntype</em><em>, </em><em>Tensor</em><em>]</em>) â€“ <p>The set of indices.  It can either be a tensor of integer indices or a dictionary
of types and indices.</p>
<p>The actual meaning of the indices is defined by the <code class="xref py py-meth docutils literal notranslate"><span class="pre">sample()</span></code> method of
<code class="xref py py-attr docutils literal notranslate"><span class="pre">graph_sampler</span></code>.</p>
</p></li>
<li><p><strong>graph_sampler</strong> (<a class="reference internal" href="dgl.dataloading.Sampler.html#dgl.dataloading.Sampler" title="dgl.dataloading.Sampler"><em>dgl.dataloading.Sampler</em></a>) â€“ The subgraph sampler.</p></li>
<li><p><strong>device</strong> (<em>device context</em><em>, </em><em>optional</em>) â€“ <p>The device of the generated MFGs in each iteration, which should be a
PyTorch device object (e.g., <code class="docutils literal notranslate"><span class="pre">torch.device</span></code>).</p>
<p>By default this value is None. If <code class="xref py py-attr docutils literal notranslate"><span class="pre">use_uva</span></code> is True, MFGs and graphs will
generated in torch.cuda.current_device(), otherwise generated in the same device
of <code class="xref py py-attr docutils literal notranslate"><span class="pre">g</span></code>.</p>
</p></li>
<li><p><strong>use_ddp</strong> (<em>boolean</em><em>, </em><em>optional</em>) â€“ <p>If True, tells the DataLoader to split the training set for each
participating process appropriately using
<code class="xref py py-class docutils literal notranslate"><span class="pre">torch.utils.data.distributed.DistributedSampler</span></code>.</p>
<p>Overrides the <code class="xref py py-attr docutils literal notranslate"><span class="pre">sampler</span></code> argument of <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.utils.data.DataLoader</span></code>.</p>
</p></li>
<li><p><strong>ddp_seed</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.12)"><em>int</em></a><em>, </em><em>optional</em>) â€“ <p>The seed for shuffling the dataset in
<code class="xref py py-class docutils literal notranslate"><span class="pre">torch.utils.data.distributed.DistributedSampler</span></code>.</p>
<p>Only effective when <code class="xref py py-attr docutils literal notranslate"><span class="pre">use_ddp</span></code> is True.</p>
</p></li>
<li><p><strong>use_uva</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.12)"><em>bool</em></a><em>, </em><em>optional</em>) â€“ <p>Whether to use Unified Virtual Addressing (UVA) to directly sample the graph
and slice the features from CPU into GPU.  Setting it to True will pin the
graph and feature tensors into pinned memory.</p>
<p>If True, requires that <code class="xref py py-attr docutils literal notranslate"><span class="pre">indices</span></code> must have the same device as the
<code class="xref py py-attr docutils literal notranslate"><span class="pre">device</span></code> argument.</p>
<p>Default: False.</p>
</p></li>
<li><p><strong>use_prefetch_thread</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.12)"><em>bool</em></a><em>, </em><em>optional</em>) â€“ <p>(Advanced option)
Spawns a new Python thread to perform feature slicing
asynchronously.  Can make things faster at the cost of GPU memory.</p>
<p>Default: True if the graph is on CPU and <code class="xref py py-attr docutils literal notranslate"><span class="pre">device</span></code> is CUDA.  False otherwise.</p>
</p></li>
<li><p><strong>use_alternate_streams</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.12)"><em>bool</em></a><em>, </em><em>optional</em>) â€“ <p>(Advanced option)
Whether to slice and transfers the features to GPU on a non-default stream.</p>
<p>Default: True if the graph is on CPU, <code class="xref py py-attr docutils literal notranslate"><span class="pre">device</span></code> is CUDA, and <code class="xref py py-attr docutils literal notranslate"><span class="pre">use_uva</span></code>
is False.  False otherwise.</p>
</p></li>
<li><p><strong>pin_prefetcher</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.12)"><em>bool</em></a><em>, </em><em>optional</em>) â€“ <p>(Advanced option)
Whether to pin the feature tensors into pinned memory.</p>
<p>Default: True if the graph is on CPU and <code class="xref py py-attr docutils literal notranslate"><span class="pre">device</span></code> is CUDA.  False otherwise.</p>
</p></li>
<li><p><strong>gpu_cache</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#dict" title="(in Python v3.12)"><em>dict</em></a><em>[</em><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#dict" title="(in Python v3.12)"><em>dict</em></a><em>]</em><em>, </em><em>optional</em>) â€“ <p>Which node and edge features to cache using HugeCTR gpu_cache. Example:
{â€œnodeâ€: {â€œfeaturesâ€: 500000}, â€œedgeâ€: {â€œtypesâ€: 4000000}} would
indicate that we want to cache 500k of the node â€œfeaturesâ€ and 4M of the
edge â€œtypesâ€ in GPU caches.</p>
<p>Is supported only on NVIDIA GPUs with compute capability 70 or above.
The dictionary holds the keys of features along with the corresponding
cache sizes. Please see
<a class="reference external" href="https://github.com/NVIDIA-Merlin/HugeCTR/blob/main/gpu_cache/ReadMe.md">https://github.com/NVIDIA-Merlin/HugeCTR/blob/main/gpu_cache/ReadMe.md</a>
for further reference.</p>
</p></li>
<li><p><strong>kwargs</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#dict" title="(in Python v3.12)"><em>dict</em></a>) â€“ <p>Key-word arguments to be passed to the parent PyTorch
<code class="xref py py-class docutils literal notranslate"><span class="pre">torch.utils.data.DataLoader</span></code> class. Common arguments are:</p>
<blockquote>
<div><ul>
<li><p><code class="docutils literal notranslate"><span class="pre">batch_size</span></code> (int): The number of indices in each batch.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">drop_last</span></code> (bool): Whether to drop the last incomplete batch.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">shuffle</span></code> (bool): Whether to randomly shuffle the indices at each epoch.</p></li>
</ul>
</div></blockquote>
</p></li>
</ul>
</dd>
</dl>
<p class="rubric">Examples</p>
<p>To train a 3-layer GNN for node classification on a set of nodes <code class="docutils literal notranslate"><span class="pre">train_nid</span></code> on
a homogeneous graph where each node takes messages from 15 neighbors on the
first layer, 10 neighbors on the second, and 5 neighbors on the third (assume
the backend is PyTorch):</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">sampler</span> <span class="o">=</span> <span class="n">dgl</span><span class="o">.</span><span class="n">dataloading</span><span class="o">.</span><span class="n">MultiLayerNeighborSampler</span><span class="p">([</span><span class="mi">15</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">5</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dataloader</span> <span class="o">=</span> <span class="n">dgl</span><span class="o">.</span><span class="n">dataloading</span><span class="o">.</span><span class="n">DataLoader</span><span class="p">(</span>
<span class="gp">... </span>    <span class="n">g</span><span class="p">,</span> <span class="n">train_nid</span><span class="p">,</span> <span class="n">sampler</span><span class="p">,</span>
<span class="gp">... </span>    <span class="n">batch_size</span><span class="o">=</span><span class="mi">1024</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">drop_last</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">num_workers</span><span class="o">=</span><span class="mi">4</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">for</span> <span class="n">input_nodes</span><span class="p">,</span> <span class="n">output_nodes</span><span class="p">,</span> <span class="n">blocks</span> <span class="ow">in</span> <span class="n">dataloader</span><span class="p">:</span>
<span class="gp">... </span>    <span class="n">train_on</span><span class="p">(</span><span class="n">input_nodes</span><span class="p">,</span> <span class="n">output_nodes</span><span class="p">,</span> <span class="n">blocks</span><span class="p">)</span>
</pre></div>
</div>
<p><strong>Using with Distributed Data Parallel</strong></p>
<p>If you are using PyTorchâ€™s distributed training (e.g. when using
<code class="xref py py-mod docutils literal notranslate"><span class="pre">torch.nn.parallel.DistributedDataParallel</span></code>), you can train the model by turning
on the <cite>use_ddp</cite> option:</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">sampler</span> <span class="o">=</span> <span class="n">dgl</span><span class="o">.</span><span class="n">dataloading</span><span class="o">.</span><span class="n">MultiLayerNeighborSampler</span><span class="p">([</span><span class="mi">15</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">5</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dataloader</span> <span class="o">=</span> <span class="n">dgl</span><span class="o">.</span><span class="n">dataloading</span><span class="o">.</span><span class="n">DataLoader</span><span class="p">(</span>
<span class="gp">... </span>    <span class="n">g</span><span class="p">,</span> <span class="n">train_nid</span><span class="p">,</span> <span class="n">sampler</span><span class="p">,</span> <span class="n">use_ddp</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
<span class="gp">... </span>    <span class="n">batch_size</span><span class="o">=</span><span class="mi">1024</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">drop_last</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">num_workers</span><span class="o">=</span><span class="mi">4</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">start_epoch</span><span class="p">,</span> <span class="n">n_epochs</span><span class="p">):</span>
<span class="gp">... </span>    <span class="k">for</span> <span class="n">input_nodes</span><span class="p">,</span> <span class="n">output_nodes</span><span class="p">,</span> <span class="n">blocks</span> <span class="ow">in</span> <span class="n">dataloader</span><span class="p">:</span>
<span class="gp">... </span>        <span class="n">train_on</span><span class="p">(</span><span class="n">input_nodes</span><span class="p">,</span> <span class="n">output_nodes</span><span class="p">,</span> <span class="n">blocks</span><span class="p">)</span>
</pre></div>
</div>
<p class="rubric">Notes</p>
<p>Please refer to
<span class="xref std std-doc">Minibatch Training Tutorials</span>
and <a class="reference internal" href="../guide/minibatch.html#guide-minibatch"><span class="std std-ref">User Guide Section 6</span></a> for usage.</p>
<p><strong>Tips for selecting the proper device</strong></p>
<ul class="simple">
<li><p>If the input graph <code class="xref py py-attr docutils literal notranslate"><span class="pre">g</span></code> is on GPU, the output device <code class="xref py py-attr docutils literal notranslate"><span class="pre">device</span></code> must be the same GPU
and <code class="xref py py-attr docutils literal notranslate"><span class="pre">num_workers</span></code> must be zero. In this case, the sampling and subgraph construction
will take place on the GPU. This is the recommended setting when using a single-GPU and
the whole graph fits in GPU memory.</p></li>
<li><p>If the input graph <code class="xref py py-attr docutils literal notranslate"><span class="pre">g</span></code> is on CPU while the output device <code class="xref py py-attr docutils literal notranslate"><span class="pre">device</span></code> is GPU, then
depending on the value of <code class="xref py py-attr docutils literal notranslate"><span class="pre">use_uva</span></code>:</p>
<ul>
<li><p>If <code class="xref py py-attr docutils literal notranslate"><span class="pre">use_uva</span></code> is set to True, the sampling and subgraph construction will happen
on GPU even if the GPU itself cannot hold the entire graph. This is the recommended
setting unless there are operations not supporting UVA. <code class="xref py py-attr docutils literal notranslate"><span class="pre">num_workers</span></code> must be 0
in this case.</p></li>
<li><p>Otherwise, both the sampling and subgraph construction will take place on the CPU.</p></li>
</ul>
</li>
</ul>
</dd></dl>

</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="../api/python/dgl.dataloading.html" class="btn btn-neutral float-left" title="dgl.dataloading" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="dgl.dataloading.GraphDataLoader.html" class="btn btn-neutral float-right" title="GraphDataLoader" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2018, DGL Team.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>