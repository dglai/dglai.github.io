<!DOCTYPE html>
<html class="writer-html5" lang="en" data-content_root="../">
<head>
  <meta charset="utf-8" /><meta name="viewport" content="width=device-width, initial-scale=1" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>DistributedItemSampler &mdash; DGL 2.4 documentation</title>
      <link rel="stylesheet" type="text/css" href="../_static/pygments.css?v=80d5e7a1" />
      <link rel="stylesheet" type="text/css" href="../_static/css/theme.css?v=19f00094" />
      <link rel="stylesheet" type="text/css" href="../_static/graphviz.css?v=fd3f3429" />
      <link rel="stylesheet" type="text/css" href="../_static/copybutton.css?v=76b2166b" />
      <link rel="stylesheet" type="text/css" href="../_static/sg_gallery.css?v=61a4c737" />
      <link rel="stylesheet" type="text/css" href="../_static/sg_gallery-binder.css?v=f4aeca0c" />
      <link rel="stylesheet" type="text/css" href="../_static/sg_gallery-dataframe.css?v=2082cf3c" />
      <link rel="stylesheet" type="text/css" href="../_static/sg_gallery-rendered-html.css?v=1277b6f3" />
      <link rel="stylesheet" type="text/css" href="../_static/css/custom.css?v=0bf289b5" />

  
  <!--[if lt IE 9]>
    <script src="../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script src="../_static/jquery.js?v=5d32c60e"></script>
        <script src="../_static/_sphinx_javascript_frameworks_compat.js?v=2cd50e6c"></script>
        <script src="../_static/documentation_options.js?v=9caaf7ed"></script>
        <script src="../_static/doctools.js?v=9a2dae69"></script>
        <script src="../_static/sphinx_highlight.js?v=dc90522c"></script>
        <script src="../_static/clipboard.min.js?v=a7894cd8"></script>
        <script src="../_static/copybutton.js?v=ccdb6887"></script>
        <script crossorigin="anonymous" integrity="sha256-Ae2Vz/4ePdIu6ZyI/5ZGsYnb+m0JlOmKPjt6XZ9JJkA=" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
        <script src="https://cdn.jsdelivr.net/npm/@jupyter-widgets/html-manager@^1.0.1/dist/embed-amd.js"></script>
    <script src="../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="MiniBatch" href="dgl.graphbolt.MiniBatch.html" />
    <link rel="prev" title="ItemSampler" href="dgl.graphbolt.ItemSampler.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="../index.html" class="icon icon-home">
            DGL
          </a>
              <div class="version">
                2.4
              </div>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">Get Started</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../install/index.html">Install and Setup</a></li>
<li class="toctree-l1"><a class="reference internal" href="../tutorials/blitz/index.html">A Blitz Introduction to DGL</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Advanced Materials</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../stochastic_training/index.html">🆕 Stochastic Training of GNNs with GraphBolt</a></li>
<li class="toctree-l1"><a class="reference internal" href="../guide/index.html">User Guide</a></li>
<li class="toctree-l1"><a class="reference internal" href="../guide_cn/index.html">用户指南【包含过时信息】</a></li>
<li class="toctree-l1"><a class="reference internal" href="../guide_ko/index.html">사용자 가이드[시대에 뒤쳐진]</a></li>
<li class="toctree-l1"><a class="reference internal" href="../graphtransformer/index.html">🆕 Tutorial: Graph Transformer</a></li>
<li class="toctree-l1"><a class="reference internal" href="../notebooks/sparse/index.html">Tutorials: dgl.sparse</a></li>
<li class="toctree-l1"><a class="reference internal" href="../tutorials/cpu/index.html">Training on CPUs</a></li>
<li class="toctree-l1"><a class="reference internal" href="../tutorials/multi/index.html">Training on Multiple GPUs</a></li>
<li class="toctree-l1"><a class="reference internal" href="../tutorials/dist/index.html">Distributed training</a></li>
<li class="toctree-l1"><a class="reference internal" href="../tutorials/models/index.html">Paper Study with DGL</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">API Reference</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../api/python/dgl.html">dgl</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api/python/dgl.data.html">dgl.data</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api/python/dgl.dataloading.html">dgl.dataloading</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api/python/dgl.DGLGraph.html">dgl.DGLGraph</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api/python/dgl.distributed.html">dgl.distributed</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api/python/dgl.function.html">dgl.function</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api/python/dgl.geometry.html">dgl.geometry</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="../api/python/dgl.graphbolt.html">🆕 dgl.graphbolt</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="../api/python/dgl.graphbolt.html#dataset">Dataset</a></li>
<li class="toctree-l2"><a class="reference internal" href="../api/python/dgl.graphbolt.html#graph">Graph</a></li>
<li class="toctree-l2"><a class="reference internal" href="../api/python/dgl.graphbolt.html#feature-and-featurestore">Feature and FeatureStore</a></li>
<li class="toctree-l2"><a class="reference internal" href="../api/python/dgl.graphbolt.html#dataloader">DataLoader</a></li>
<li class="toctree-l2"><a class="reference internal" href="../api/python/dgl.graphbolt.html#itemset">ItemSet</a></li>
<li class="toctree-l2 current"><a class="reference internal" href="../api/python/dgl.graphbolt.html#itemsampler">ItemSampler</a><ul class="current">
<li class="toctree-l3"><a class="reference internal" href="dgl.graphbolt.ItemSampler.html">ItemSampler</a></li>
<li class="toctree-l3 current"><a class="current reference internal" href="#">DistributedItemSampler</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#dgl.graphbolt.DistributedItemSampler"><code class="docutils literal notranslate"><span class="pre">DistributedItemSampler</span></code></a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../api/python/dgl.graphbolt.html#minibatch">MiniBatch</a></li>
<li class="toctree-l2"><a class="reference internal" href="../api/python/dgl.graphbolt.html#negativesampler">NegativeSampler</a></li>
<li class="toctree-l2"><a class="reference internal" href="../api/python/dgl.graphbolt.html#subgraphsampler">SubgraphSampler</a></li>
<li class="toctree-l2"><a class="reference internal" href="../api/python/dgl.graphbolt.html#featurefetcher">FeatureFetcher</a></li>
<li class="toctree-l2"><a class="reference internal" href="../api/python/dgl.graphbolt.html#copyto">CopyTo</a></li>
<li class="toctree-l2"><a class="reference internal" href="../api/python/dgl.graphbolt.html#utilities">Utilities</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../api/python/nn-pytorch.html">dgl.nn (PyTorch)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api/python/nn.functional.html">dgl.nn.functional</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api/python/dgl.ops.html">dgl.ops</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api/python/dgl.optim.html">dgl.optim</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api/python/dgl.sampling.html">dgl.sampling</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api/python/dgl.sparse_v0.html">dgl.sparse</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api/python/dgl.multiprocessing.html">dgl.multiprocessing</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api/python/transforms.html">dgl.transforms</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api/python/udf.html">User-defined Functions</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Notes</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../contribute.html">Contribute to DGL</a></li>
<li class="toctree-l1"><a class="reference internal" href="../developer/ffi.html">DGL Foreign Function Interface (FFI)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../performance.html">Performance Benchmarks</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Misc</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../faq.html">Frequently Asked Questions (FAQ)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../env_var.html">Environment Variables</a></li>
<li class="toctree-l1"><a class="reference internal" href="../resources.html">Resources</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">DGL</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../index.html" class="icon icon-home" aria-label="Home"></a></li>
          <li class="breadcrumb-item"><a href="../api/python/dgl.graphbolt.html">🆕 dgl.graphbolt</a></li>
      <li class="breadcrumb-item active">DistributedItemSampler</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../_sources/generated/dgl.graphbolt.DistributedItemSampler.rst.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="distributeditemsampler">
<h1>DistributedItemSampler<a class="headerlink" href="#distributeditemsampler" title="Link to this heading"></a></h1>
<dl class="py class">
<dt class="sig sig-object py" id="dgl.graphbolt.DistributedItemSampler">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">dgl.graphbolt.</span></span><span class="sig-name descname"><span class="pre">DistributedItemSampler</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">item_set:</span> <span class="pre">~dgl.graphbolt.itemset.ItemSet</span> <span class="pre">|</span> <span class="pre">~dgl.graphbolt.itemset.HeteroItemSet</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">batch_size:</span> <span class="pre">int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">minibatcher:</span> <span class="pre">~typing.Callable</span> <span class="pre">|</span> <span class="pre">None</span> <span class="pre">=</span> <span class="pre">&lt;function</span> <span class="pre">minibatcher_default&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">drop_last:</span> <span class="pre">bool</span> <span class="pre">|</span> <span class="pre">None</span> <span class="pre">=</span> <span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">shuffle:</span> <span class="pre">bool</span> <span class="pre">|</span> <span class="pre">None</span> <span class="pre">=</span> <span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">drop_uneven_inputs:</span> <span class="pre">bool</span> <span class="pre">|</span> <span class="pre">None</span> <span class="pre">=</span> <span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">seed:</span> <span class="pre">int</span> <span class="pre">|</span> <span class="pre">None</span> <span class="pre">=</span> <span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/dgl/graphbolt/item_sampler.html#DistributedItemSampler"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#dgl.graphbolt.DistributedItemSampler" title="Link to this definition"></a></dt>
<dd><p>Bases: <a class="reference internal" href="dgl.graphbolt.ItemSampler.html#dgl.graphbolt.ItemSampler" title="dgl.graphbolt.item_sampler.ItemSampler"><code class="xref py py-class docutils literal notranslate"><span class="pre">ItemSampler</span></code></a></p>
<p>A sampler to iterate over input items and create subsets distributedly.</p>
<p>This sampler creates a distributed subset of items from the given data set,
which can be used for training with PyTorch’s Distributed Data Parallel
(DDP). The items can be node IDs, node pairs with or without labels, node
pairs with negative sources/destinations, DGLGraphs, or heterogeneous
counterparts. The original item set is split such that each replica
(process) receives an exclusive subset.</p>
<p>Note: The items will be first split onto each replica, then get shuffled
(if needed) and batched. Therefore, each replica will always get a same set
of items.</p>
<p>Note: This class <cite>DistributedItemSampler</cite> is not decorated with
<cite>torchdata.datapipes.functional_datapipe</cite> on purpose. This indicates it
does not support function-like call. But any iterable datapipes from
<cite>torchdata</cite> can be further appended.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>item_set</strong> (<em>Union</em><em>[</em><a class="reference internal" href="dgl.graphbolt.ItemSet.html#dgl.graphbolt.ItemSet" title="dgl.graphbolt.ItemSet"><em>ItemSet</em></a><em>, </em><a class="reference internal" href="dgl.graphbolt.HeteroItemSet.html#dgl.graphbolt.HeteroItemSet" title="dgl.graphbolt.HeteroItemSet"><em>HeteroItemSet</em></a><em>]</em>) – Data to be sampled.</p></li>
<li><p><strong>batch_size</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.12)"><em>int</em></a>) – The size of each batch.</p></li>
<li><p><strong>minibatcher</strong> (<em>Optional</em><em>[</em><em>Callable</em><em>]</em>) – A callable that takes in a list of items and returns a <cite>MiniBatch</cite>.</p></li>
<li><p><strong>drop_last</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.12)"><em>bool</em></a>) – Option to drop the last batch if it’s not full.</p></li>
<li><p><strong>shuffle</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.12)"><em>bool</em></a>) – Option to shuffle before sample.</p></li>
<li><p><strong>num_replicas</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.12)"><em>int</em></a>) – The number of model replicas that will be created during Distributed
Data Parallel (DDP) training. It should be the same as the real world
size, otherwise it could cause errors. By default, it is retrieved from
the current distributed group.</p></li>
<li><p><strong>drop_uneven_inputs</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.12)"><em>bool</em></a>) – Option to make sure the numbers of batches for each replica are the
same. If some of the replicas have more batches than the others, the
redundant batches of those replicas will be dropped. If the drop_last
parameter is also set to True, the last batch will be dropped before the
redundant batches are dropped.
Note: When using Distributed Data Parallel (DDP) training, the program
may hang or error if the a replica has fewer inputs. It is recommended
to use the Join Context Manager provided by PyTorch to solve this
problem. Please refer to
<a class="reference external" href="https://pytorch.org/tutorials/advanced/generic_join.html">https://pytorch.org/tutorials/advanced/generic_join.html</a>. However, this
option can be used if the Join Context Manager is not helpful for any
reason.</p></li>
<li><p><strong>seed</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.12)"><em>int</em></a>) – The seed for reproducible stochastic shuffling. If None, a random seed
will be generated.</p></li>
</ul>
</dd>
</dl>
<p class="rubric">Examples</p>
<p>0. Preparation: DistributedItemSampler needs multi-processing environment to
work. You need to spawn subprocesses and initialize processing group before
executing following examples. Due to randomness, the output is not always
the same as listed below.</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">torch</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">dgl</span> <span class="kn">import</span> <span class="n">graphbolt</span> <span class="k">as</span> <span class="n">gb</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">item_set</span> <span class="o">=</span> <span class="n">gb</span><span class="o">.</span><span class="n">ItemSet</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">15</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">num_replicas</span> <span class="o">=</span> <span class="mi">4</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">batch_size</span> <span class="o">=</span> <span class="mi">2</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">mp</span><span class="o">.</span><span class="n">spawn</span><span class="p">(</span><span class="o">...</span><span class="p">)</span>
</pre></div>
</div>
<ol class="arabic simple">
<li><p>shuffle = False, drop_last = False, drop_uneven_inputs = False.</p></li>
</ol>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">item_sampler</span> <span class="o">=</span> <span class="n">gb</span><span class="o">.</span><span class="n">DistributedItemSampler</span><span class="p">(</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="n">item_set</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">drop_last</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="n">drop_uneven_inputs</span><span class="o">=</span><span class="kc">False</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">data_loader</span> <span class="o">=</span> <span class="n">gb</span><span class="o">.</span><span class="n">DataLoader</span><span class="p">(</span><span class="n">item_sampler</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Replica#</span><span class="si">{</span><span class="n">proc_id</span><span class="si">}</span><span class="s2">: </span><span class="si">{</span><span class="nb">list</span><span class="p">(</span><span class="n">data_loader</span><span class="p">)</span><span class="si">}</span><span class="s2">)</span>
<span class="go">Replica#0: [tensor([0, 1]), tensor([2, 3])]</span>
<span class="go">Replica#1: [tensor([4, 5]), tensor([6, 7])]</span>
<span class="go">Replica#2: [tensor([8, 9]), tensor([10, 11])]</span>
<span class="go">Replica#3: [tensor([12, 13]), tensor([14])]</span>
</pre></div>
</div>
<ol class="arabic simple" start="2">
<li><p>shuffle = False, drop_last = True, drop_uneven_inputs = False.</p></li>
</ol>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">item_sampler</span> <span class="o">=</span> <span class="n">gb</span><span class="o">.</span><span class="n">DistributedItemSampler</span><span class="p">(</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="n">item_set</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">drop_last</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="n">drop_uneven_inputs</span><span class="o">=</span><span class="kc">False</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">data_loader</span> <span class="o">=</span> <span class="n">gb</span><span class="o">.</span><span class="n">DataLoader</span><span class="p">(</span><span class="n">item_sampler</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Replica#</span><span class="si">{</span><span class="n">proc_id</span><span class="si">}</span><span class="s2">: </span><span class="si">{</span><span class="nb">list</span><span class="p">(</span><span class="n">data_loader</span><span class="p">)</span><span class="si">}</span><span class="s2">)</span>
<span class="go">Replica#0: [tensor([0, 1]), tensor([2, 3])]</span>
<span class="go">Replica#1: [tensor([4, 5]), tensor([6, 7])]</span>
<span class="go">Replica#2: [tensor([8, 9]), tensor([10, 11])]</span>
<span class="go">Replica#3: [tensor([12, 13])]</span>
</pre></div>
</div>
<ol class="arabic simple" start="3">
<li><p>shuffle = False, drop_last = False, drop_uneven_inputs = True.</p></li>
</ol>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">item_sampler</span> <span class="o">=</span> <span class="n">gb</span><span class="o">.</span><span class="n">DistributedItemSampler</span><span class="p">(</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="n">item_set</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">drop_last</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="n">drop_uneven_inputs</span><span class="o">=</span><span class="kc">True</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">data_loader</span> <span class="o">=</span> <span class="n">gb</span><span class="o">.</span><span class="n">DataLoader</span><span class="p">(</span><span class="n">item_sampler</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Replica#</span><span class="si">{</span><span class="n">proc_id</span><span class="si">}</span><span class="s2">: </span><span class="si">{</span><span class="nb">list</span><span class="p">(</span><span class="n">data_loader</span><span class="p">)</span><span class="si">}</span><span class="s2">)</span>
<span class="go">Replica#0: [tensor([0, 1]), tensor([2, 3])]</span>
<span class="go">Replica#1: [tensor([4, 5]), tensor([6, 7])]</span>
<span class="go">Replica#2: [tensor([8, 9]), tensor([10, 11])]</span>
<span class="go">Replica#3: [tensor([12, 13]), tensor([14])]</span>
</pre></div>
</div>
<ol class="arabic simple" start="4">
<li><p>shuffle = False, drop_last = True, drop_uneven_inputs = True.</p></li>
</ol>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">item_sampler</span> <span class="o">=</span> <span class="n">gb</span><span class="o">.</span><span class="n">DistributedItemSampler</span><span class="p">(</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="n">item_set</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">drop_last</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="n">drop_uneven_inputs</span><span class="o">=</span><span class="kc">True</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">data_loader</span> <span class="o">=</span> <span class="n">gb</span><span class="o">.</span><span class="n">DataLoader</span><span class="p">(</span><span class="n">item_sampler</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Replica#</span><span class="si">{</span><span class="n">proc_id</span><span class="si">}</span><span class="s2">: </span><span class="si">{</span><span class="nb">list</span><span class="p">(</span><span class="n">data_loader</span><span class="p">)</span><span class="si">}</span><span class="s2">)</span>
<span class="go">Replica#0: [tensor([0, 1])]</span>
<span class="go">Replica#1: [tensor([4, 5])]</span>
<span class="go">Replica#2: [tensor([8, 9])]</span>
<span class="go">Replica#3: [tensor([12, 13])]</span>
</pre></div>
</div>
<ol class="arabic simple" start="5">
<li><p>shuffle = True, drop_last = True, drop_uneven_inputs = False.</p></li>
</ol>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">item_sampler</span> <span class="o">=</span> <span class="n">gb</span><span class="o">.</span><span class="n">DistributedItemSampler</span><span class="p">(</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="n">item_set</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">drop_last</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="n">drop_uneven_inputs</span><span class="o">=</span><span class="kc">False</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">data_loader</span> <span class="o">=</span> <span class="n">gb</span><span class="o">.</span><span class="n">DataLoader</span><span class="p">(</span><span class="n">item_sampler</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Replica#</span><span class="si">{</span><span class="n">proc_id</span><span class="si">}</span><span class="s2">: </span><span class="si">{</span><span class="nb">list</span><span class="p">(</span><span class="n">data_loader</span><span class="p">)</span><span class="si">}</span><span class="s2">)</span>
<span class="go">(One possible output:)</span>
<span class="go">Replica#0: [tensor([3, 2]), tensor([0, 1])]</span>
<span class="go">Replica#1: [tensor([6, 5]), tensor([7, 4])]</span>
<span class="go">Replica#2: [tensor([8, 10])]</span>
<span class="go">Replica#3: [tensor([14, 12])]</span>
</pre></div>
</div>
<ol class="arabic simple" start="6">
<li><p>shuffle = True, drop_last = True, drop_uneven_inputs = True.</p></li>
</ol>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">item_sampler</span> <span class="o">=</span> <span class="n">gb</span><span class="o">.</span><span class="n">DistributedItemSampler</span><span class="p">(</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="n">item_set</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">drop_last</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="n">drop_uneven_inputs</span><span class="o">=</span><span class="kc">True</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">data_loader</span> <span class="o">=</span> <span class="n">gb</span><span class="o">.</span><span class="n">DataLoader</span><span class="p">(</span><span class="n">item_sampler</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Replica#</span><span class="si">{</span><span class="n">proc_id</span><span class="si">}</span><span class="s2">: </span><span class="si">{</span><span class="nb">list</span><span class="p">(</span><span class="n">data_loader</span><span class="p">)</span><span class="si">}</span><span class="s2">)</span>
<span class="go">(One possible output:)</span>
<span class="go">Replica#0: [tensor([1, 3])]</span>
<span class="go">Replica#1: [tensor([7, 5])]</span>
<span class="go">Replica#2: [tensor([11, 9])]</span>
<span class="go">Replica#3: [tensor([13, 14])]</span>
</pre></div>
</div>
</dd></dl>

</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="dgl.graphbolt.ItemSampler.html" class="btn btn-neutral float-left" title="ItemSampler" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="dgl.graphbolt.MiniBatch.html" class="btn btn-neutral float-right" title="MiniBatch" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2018, DGL Team.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>