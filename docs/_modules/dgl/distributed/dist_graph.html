<!DOCTYPE html>
<html class="writer-html5" lang="en" data-content_root="../../../">
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>dgl.distributed.dist_graph &mdash; DGL 2.4 documentation</title>
      <link rel="stylesheet" type="text/css" href="../../../_static/pygments.css?v=80d5e7a1" />
      <link rel="stylesheet" type="text/css" href="../../../_static/css/theme.css?v=19f00094" />
      <link rel="stylesheet" type="text/css" href="../../../_static/graphviz.css?v=fd3f3429" />
      <link rel="stylesheet" type="text/css" href="../../../_static/copybutton.css?v=76b2166b" />
      <link rel="stylesheet" type="text/css" href="../../../_static/sg_gallery.css?v=61a4c737" />
      <link rel="stylesheet" type="text/css" href="../../../_static/sg_gallery-binder.css?v=f4aeca0c" />
      <link rel="stylesheet" type="text/css" href="../../../_static/sg_gallery-dataframe.css?v=2082cf3c" />
      <link rel="stylesheet" type="text/css" href="../../../_static/sg_gallery-rendered-html.css?v=1277b6f3" />
      <link rel="stylesheet" type="text/css" href="../../../_static/css/custom.css?v=0bf289b5" />

  
  <!--[if lt IE 9]>
    <script src="../../../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script src="../../../_static/jquery.js?v=5d32c60e"></script>
        <script src="../../../_static/_sphinx_javascript_frameworks_compat.js?v=2cd50e6c"></script>
        <script src="../../../_static/documentation_options.js?v=9caaf7ed"></script>
        <script src="../../../_static/doctools.js?v=9a2dae69"></script>
        <script src="../../../_static/sphinx_highlight.js?v=dc90522c"></script>
        <script src="../../../_static/clipboard.min.js?v=a7894cd8"></script>
        <script src="../../../_static/copybutton.js?v=ccdb6887"></script>
        <script crossorigin="anonymous" integrity="sha256-Ae2Vz/4ePdIu6ZyI/5ZGsYnb+m0JlOmKPjt6XZ9JJkA=" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
        <script src="https://cdn.jsdelivr.net/npm/@jupyter-widgets/html-manager@^1.0.1/dist/embed-amd.js"></script>
    <script src="../../../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../../../genindex.html" />
    <link rel="search" title="Search" href="../../../search.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="../../../index.html" class="icon icon-home">
            DGL
          </a>
              <div class="version">
                2.4
              </div>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">Get Started</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../install/index.html">Install and Setup</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../tutorials/blitz/index.html">A Blitz Introduction to DGL</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Advanced Materials</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../stochastic_training/index.html">🆕 Stochastic Training of GNNs with GraphBolt</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../guide/index.html">User Guide</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../guide_cn/index.html">用户指南【包含过时信息】</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../guide_ko/index.html">사용자 가이드[시대에 뒤쳐진]</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../graphtransformer/index.html">🆕 Tutorial: Graph Transformer</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../notebooks/sparse/index.html">Tutorials: dgl.sparse</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../tutorials/cpu/index.html">Training on CPUs</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../tutorials/multi/index.html">Training on Multiple GPUs</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../tutorials/dist/index.html">Distributed training</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../tutorials/models/index.html">Paper Study with DGL</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">API Reference</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../api/python/dgl.html">dgl</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../api/python/dgl.data.html">dgl.data</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../api/python/dgl.dataloading.html">dgl.dataloading</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../api/python/dgl.DGLGraph.html">dgl.DGLGraph</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../api/python/dgl.distributed.html">dgl.distributed</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../api/python/dgl.function.html">dgl.function</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../api/python/dgl.geometry.html">dgl.geometry</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../api/python/dgl.graphbolt.html">🆕 dgl.graphbolt</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../api/python/nn-pytorch.html">dgl.nn (PyTorch)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../api/python/nn.functional.html">dgl.nn.functional</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../api/python/dgl.ops.html">dgl.ops</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../api/python/dgl.optim.html">dgl.optim</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../api/python/dgl.sampling.html">dgl.sampling</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../api/python/dgl.sparse_v0.html">dgl.sparse</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../api/python/dgl.multiprocessing.html">dgl.multiprocessing</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../api/python/transforms.html">dgl.transforms</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../api/python/udf.html">User-defined Functions</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Notes</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../contribute.html">Contribute to DGL</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../developer/ffi.html">DGL Foreign Function Interface (FFI)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../performance.html">Performance Benchmarks</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Misc</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../faq.html">Frequently Asked Questions (FAQ)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../env_var.html">Environment Variables</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../resources.html">Resources</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../../index.html">DGL</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../../../index.html" class="icon icon-home" aria-label="Home"></a></li>
          <li class="breadcrumb-item"><a href="../../index.html">Module code</a></li>
      <li class="breadcrumb-item active">dgl.distributed.dist_graph</li>
      <li class="wy-breadcrumbs-aside">
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <h1>Source code for dgl.distributed.dist_graph</h1><div class="highlight"><pre>
<span></span><span class="sd">&quot;&quot;&quot;Define distributed graph.&quot;&quot;&quot;</span>

<span class="kn">import</span> <span class="nn">gc</span>

<span class="kn">import</span> <span class="nn">os</span>
<span class="kn">from</span> <span class="nn">collections</span> <span class="kn">import</span> <span class="n">namedtuple</span>
<span class="kn">from</span> <span class="nn">collections.abc</span> <span class="kn">import</span> <span class="n">Mapping</span><span class="p">,</span> <span class="n">MutableMapping</span>

<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">torch</span>

<span class="kn">from</span> <span class="nn">..</span> <span class="kn">import</span> <span class="n">backend</span> <span class="k">as</span> <span class="n">F</span><span class="p">,</span> <span class="n">graphbolt</span> <span class="k">as</span> <span class="n">gb</span><span class="p">,</span> <span class="n">heterograph_index</span>
<span class="kn">from</span> <span class="nn">.._ffi.ndarray</span> <span class="kn">import</span> <span class="n">empty_shared_mem</span>
<span class="kn">from</span> <span class="nn">..base</span> <span class="kn">import</span> <span class="n">ALL</span><span class="p">,</span> <span class="n">DGLError</span><span class="p">,</span> <span class="n">EID</span><span class="p">,</span> <span class="n">ETYPE</span><span class="p">,</span> <span class="n">is_all</span><span class="p">,</span> <span class="n">NID</span>
<span class="kn">from</span> <span class="nn">..convert</span> <span class="kn">import</span> <span class="n">graph</span> <span class="k">as</span> <span class="n">dgl_graph</span><span class="p">,</span> <span class="n">heterograph</span> <span class="k">as</span> <span class="n">dgl_heterograph</span>
<span class="kn">from</span> <span class="nn">..frame</span> <span class="kn">import</span> <span class="n">infer_scheme</span>

<span class="kn">from</span> <span class="nn">..heterograph</span> <span class="kn">import</span> <span class="n">DGLGraph</span>
<span class="kn">from</span> <span class="nn">..ndarray</span> <span class="kn">import</span> <span class="n">exist_shared_mem_array</span>
<span class="kn">from</span> <span class="nn">..transforms</span> <span class="kn">import</span> <span class="n">compact_graphs</span>
<span class="kn">from</span> <span class="nn">.</span> <span class="kn">import</span> <span class="n">graph_services</span><span class="p">,</span> <span class="n">role</span><span class="p">,</span> <span class="n">rpc</span>
<span class="kn">from</span> <span class="nn">.dist_tensor</span> <span class="kn">import</span> <span class="n">DistTensor</span>
<span class="kn">from</span> <span class="nn">.graph_partition_book</span> <span class="kn">import</span> <span class="p">(</span>
    <span class="n">_etype_str_to_tuple</span><span class="p">,</span>
    <span class="n">EdgePartitionPolicy</span><span class="p">,</span>
    <span class="n">get_shared_mem_partition_book</span><span class="p">,</span>
    <span class="n">HeteroDataName</span><span class="p">,</span>
    <span class="n">NodePartitionPolicy</span><span class="p">,</span>
    <span class="n">parse_hetero_data_name</span><span class="p">,</span>
    <span class="n">PartitionPolicy</span><span class="p">,</span>
<span class="p">)</span>
<span class="kn">from</span> <span class="nn">.graph_services</span> <span class="kn">import</span> <span class="p">(</span>
    <span class="n">find_edges</span> <span class="k">as</span> <span class="n">dist_find_edges</span><span class="p">,</span>
    <span class="n">in_degrees</span> <span class="k">as</span> <span class="n">dist_in_degrees</span><span class="p">,</span>
    <span class="n">out_degrees</span> <span class="k">as</span> <span class="n">dist_out_degrees</span><span class="p">,</span>
<span class="p">)</span>
<span class="kn">from</span> <span class="nn">.kvstore</span> <span class="kn">import</span> <span class="n">get_kvstore</span><span class="p">,</span> <span class="n">KVServer</span>
<span class="kn">from</span> <span class="nn">.partition</span> <span class="kn">import</span> <span class="p">(</span>
    <span class="n">load_partition</span><span class="p">,</span>
    <span class="n">load_partition_book</span><span class="p">,</span>
    <span class="n">load_partition_feats</span><span class="p">,</span>
    <span class="n">RESERVED_FIELD_DTYPE</span><span class="p">,</span>
<span class="p">)</span>
<span class="kn">from</span> <span class="nn">.rpc_server</span> <span class="kn">import</span> <span class="n">start_server</span>
<span class="kn">from</span> <span class="nn">.server_state</span> <span class="kn">import</span> <span class="n">ServerState</span>
<span class="kn">from</span> <span class="nn">.shared_mem_utils</span> <span class="kn">import</span> <span class="p">(</span>
    <span class="n">_get_edata_path</span><span class="p">,</span>
    <span class="n">_get_ndata_path</span><span class="p">,</span>
    <span class="n">_to_shared_mem</span><span class="p">,</span>
    <span class="n">DTYPE_DICT</span><span class="p">,</span>
<span class="p">)</span>

<span class="n">INIT_GRAPH</span> <span class="o">=</span> <span class="mi">800001</span>
<span class="n">QUERY_IF_USE_GRAPHBOLT</span> <span class="o">=</span> <span class="mi">800002</span>
<span class="n">ADD_EDGE_ATTRIBUTE_FROM_KV</span> <span class="o">=</span> <span class="mi">800003</span>
<span class="n">ADD_EDGE_ATTRIBUTE_FROM_SHARED_MEM</span> <span class="o">=</span> <span class="mi">800004</span>


<span class="k">class</span> <span class="nc">InitGraphRequest</span><span class="p">(</span><span class="n">rpc</span><span class="o">.</span><span class="n">Request</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Init graph on the backup servers.</span>

<span class="sd">    When the backup server starts, they don&#39;t load the graph structure.</span>
<span class="sd">    This request tells the backup servers that they can map to the graph structure</span>
<span class="sd">    with shared memory.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">graph_name</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_graph_name</span> <span class="o">=</span> <span class="n">graph_name</span>

    <span class="k">def</span> <span class="nf">__getstate__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_graph_name</span>

    <span class="k">def</span> <span class="nf">__setstate__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">state</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_graph_name</span> <span class="o">=</span> <span class="n">state</span>

    <span class="k">def</span> <span class="nf">process_request</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">server_state</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">server_state</span><span class="o">.</span><span class="n">graph</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">server_state</span><span class="o">.</span><span class="n">graph</span> <span class="o">=</span> <span class="n">_get_graph_from_shared_mem</span><span class="p">(</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">_graph_name</span><span class="p">,</span> <span class="n">server_state</span><span class="o">.</span><span class="n">use_graphbolt</span>
            <span class="p">)</span>
        <span class="k">return</span> <span class="n">InitGraphResponse</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_graph_name</span><span class="p">)</span>


<span class="k">class</span> <span class="nc">InitGraphResponse</span><span class="p">(</span><span class="n">rpc</span><span class="o">.</span><span class="n">Response</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Ack the init graph request&quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">graph_name</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_graph_name</span> <span class="o">=</span> <span class="n">graph_name</span>

    <span class="k">def</span> <span class="nf">__getstate__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_graph_name</span>

    <span class="k">def</span> <span class="nf">__setstate__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">state</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_graph_name</span> <span class="o">=</span> <span class="n">state</span>


<span class="k">class</span> <span class="nc">QueryIfUseGraphBoltRequest</span><span class="p">(</span><span class="n">rpc</span><span class="o">.</span><span class="n">Request</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Query if use GraphBolt.&quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="nf">__getstate__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="kc">None</span>

    <span class="k">def</span> <span class="nf">__setstate__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">state</span><span class="p">):</span>
        <span class="k">pass</span>

    <span class="k">def</span> <span class="nf">process_request</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">server_state</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">QueryIfUseGraphBoltResponse</span><span class="p">(</span><span class="n">server_state</span><span class="o">.</span><span class="n">use_graphbolt</span><span class="p">)</span>


<span class="k">class</span> <span class="nc">QueryIfUseGraphBoltResponse</span><span class="p">(</span><span class="n">rpc</span><span class="o">.</span><span class="n">Response</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Ack the query request about if use GraphBolt.&quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">use_graphbolt</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_use_graphbolt</span> <span class="o">=</span> <span class="n">use_graphbolt</span>

    <span class="k">def</span> <span class="nf">__getstate__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_use_graphbolt</span>

    <span class="k">def</span> <span class="nf">__setstate__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">state</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_use_graphbolt</span> <span class="o">=</span> <span class="n">state</span>


<span class="k">def</span> <span class="nf">_copy_data_to_shared_mem</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">name</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Copy data to shared memory.&quot;&quot;&quot;</span>
    <span class="c1"># [TODO] Copy data to shared memory.</span>
    <span class="k">assert</span> <span class="n">data</span><span class="o">.</span><span class="n">dtype</span> <span class="o">==</span> <span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="s2">&quot;Only float32 is supported.&quot;</span>
    <span class="n">data_type</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">reverse_data_type_dict</span><span class="p">[</span><span class="n">F</span><span class="o">.</span><span class="n">dtype</span><span class="p">(</span><span class="n">data</span><span class="p">)]</span>
    <span class="n">shared_data</span> <span class="o">=</span> <span class="n">empty_shared_mem</span><span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="kc">True</span><span class="p">,</span> <span class="n">data</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">data_type</span><span class="p">)</span>
    <span class="n">dlpack</span> <span class="o">=</span> <span class="n">shared_data</span><span class="o">.</span><span class="n">to_dlpack</span><span class="p">()</span>
    <span class="n">ret</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">zerocopy_from_dlpack</span><span class="p">(</span><span class="n">dlpack</span><span class="p">)</span>
    <span class="n">rpc</span><span class="o">.</span><span class="n">copy_data_to_shared_memory</span><span class="p">(</span><span class="n">ret</span><span class="p">,</span> <span class="n">data</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">ret</span>


<span class="k">def</span> <span class="nf">_copy_data_from_shared_mem</span><span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="n">shape</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Copy data from shared memory.&quot;&quot;&quot;</span>
    <span class="n">data_type</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">reverse_data_type_dict</span><span class="p">[</span><span class="n">F</span><span class="o">.</span><span class="n">float32</span><span class="p">]</span>
    <span class="n">data</span> <span class="o">=</span> <span class="n">empty_shared_mem</span><span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="kc">False</span><span class="p">,</span> <span class="n">shape</span><span class="p">,</span> <span class="n">data_type</span><span class="p">)</span>
    <span class="n">dlpack</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">to_dlpack</span><span class="p">()</span>
    <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">zerocopy_from_dlpack</span><span class="p">(</span><span class="n">dlpack</span><span class="p">)</span>


<span class="k">class</span> <span class="nc">AddEdgeAttributeFromKVRequest</span><span class="p">(</span><span class="n">rpc</span><span class="o">.</span><span class="n">Request</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Add edge attribute from kvstore to local GraphBolt partition.&quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">name</span><span class="p">,</span> <span class="n">kv_names</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_name</span> <span class="o">=</span> <span class="n">name</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_kv_names</span> <span class="o">=</span> <span class="n">kv_names</span>

    <span class="k">def</span> <span class="nf">__getstate__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_name</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_kv_names</span>

    <span class="k">def</span> <span class="nf">__setstate__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">state</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_name</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_kv_names</span> <span class="o">=</span> <span class="n">state</span>

    <span class="k">def</span> <span class="nf">process_request</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">server_state</span><span class="p">):</span>
        <span class="c1"># For now, this is only used to add prob/mask data to the graph.</span>
        <span class="n">name</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_name</span>
        <span class="n">g</span> <span class="o">=</span> <span class="n">server_state</span><span class="o">.</span><span class="n">graph</span>
        <span class="k">if</span> <span class="n">name</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">g</span><span class="o">.</span><span class="n">edge_attributes</span><span class="p">:</span>
            <span class="c1"># Fetch target data from kvstore.</span>
            <span class="n">kv_store</span> <span class="o">=</span> <span class="n">server_state</span><span class="o">.</span><span class="n">kv_store</span>
            <span class="n">data</span> <span class="o">=</span> <span class="p">[</span>
                <span class="n">kv_store</span><span class="o">.</span><span class="n">data_store</span><span class="p">[</span><span class="n">kv_name</span><span class="p">]</span> <span class="k">if</span> <span class="n">kv_name</span> <span class="k">else</span> <span class="kc">None</span>
                <span class="k">for</span> <span class="n">kv_name</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_kv_names</span>
            <span class="p">]</span>
            <span class="c1"># Due to data type limitation in GraphBolt&#39;s sampling, we only support float32.</span>
            <span class="n">data_type</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">float32</span>
            <span class="n">gpb</span> <span class="o">=</span> <span class="n">server_state</span><span class="o">.</span><span class="n">partition_book</span>
            <span class="c1"># Initialize the edge attribute.</span>
            <span class="n">num_edges</span> <span class="o">=</span> <span class="n">g</span><span class="o">.</span><span class="n">total_num_edges</span>
            <span class="n">attr_data</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">num_edges</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">data_type</span><span class="p">)</span>
            <span class="c1"># Map data from kvstore to the local partition for inner edges only.</span>
            <span class="n">num_inner_edges</span> <span class="o">=</span> <span class="n">gpb</span><span class="o">.</span><span class="n">metadata</span><span class="p">()[</span><span class="n">gpb</span><span class="o">.</span><span class="n">partid</span><span class="p">][</span><span class="s2">&quot;num_edges&quot;</span><span class="p">]</span>
            <span class="n">homo_eids</span> <span class="o">=</span> <span class="n">g</span><span class="o">.</span><span class="n">edge_attributes</span><span class="p">[</span><span class="n">EID</span><span class="p">][:</span><span class="n">num_inner_edges</span><span class="p">]</span>
            <span class="n">etype_ids</span><span class="p">,</span> <span class="n">typed_eids</span> <span class="o">=</span> <span class="n">gpb</span><span class="o">.</span><span class="n">map_to_per_etype</span><span class="p">(</span><span class="n">homo_eids</span><span class="p">)</span>
            <span class="k">for</span> <span class="n">etype_id</span><span class="p">,</span> <span class="n">c_etype</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">gpb</span><span class="o">.</span><span class="n">canonical_etypes</span><span class="p">):</span>
                <span class="n">curr_indices</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nonzero</span><span class="p">(</span><span class="n">etype_ids</span> <span class="o">==</span> <span class="n">etype_id</span><span class="p">)</span><span class="o">.</span><span class="n">squeeze</span><span class="p">()</span>
                <span class="n">curr_typed_eids</span> <span class="o">=</span> <span class="n">typed_eids</span><span class="p">[</span><span class="n">curr_indices</span><span class="p">]</span>
                <span class="n">curr_local_eids</span> <span class="o">=</span> <span class="n">gpb</span><span class="o">.</span><span class="n">eid2localeid</span><span class="p">(</span>
                    <span class="n">curr_typed_eids</span><span class="p">,</span> <span class="n">gpb</span><span class="o">.</span><span class="n">partid</span><span class="p">,</span> <span class="n">etype</span><span class="o">=</span><span class="n">c_etype</span>
                <span class="p">)</span>
                <span class="k">if</span> <span class="n">data</span><span class="p">[</span><span class="n">etype_id</span><span class="p">]</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                    <span class="k">continue</span>
                <span class="n">attr_data</span><span class="p">[</span><span class="n">curr_indices</span><span class="p">]</span> <span class="o">=</span> <span class="n">data</span><span class="p">[</span><span class="n">etype_id</span><span class="p">][</span><span class="n">curr_local_eids</span><span class="p">]</span><span class="o">.</span><span class="n">to</span><span class="p">(</span>
                    <span class="n">data_type</span>
                <span class="p">)</span>
            <span class="c1"># Copy data to shared memory.</span>
            <span class="n">attr_data</span> <span class="o">=</span> <span class="n">_copy_data_to_shared_mem</span><span class="p">(</span><span class="n">attr_data</span><span class="p">,</span> <span class="s2">&quot;__edge__&quot;</span> <span class="o">+</span> <span class="n">name</span><span class="p">)</span>
            <span class="n">g</span><span class="o">.</span><span class="n">add_edge_attribute</span><span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="n">attr_data</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">AddEdgeAttributeFromKVResponse</span><span class="p">(</span><span class="n">name</span><span class="p">)</span>


<span class="k">class</span> <span class="nc">AddEdgeAttributeFromKVResponse</span><span class="p">(</span><span class="n">rpc</span><span class="o">.</span><span class="n">Response</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Ack the request of adding edge attribute.&quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">name</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_name</span> <span class="o">=</span> <span class="n">name</span>

    <span class="k">def</span> <span class="nf">__getstate__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_name</span>

    <span class="k">def</span> <span class="nf">__setstate__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">state</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_name</span> <span class="o">=</span> <span class="n">state</span>


<span class="k">class</span> <span class="nc">AddEdgeAttributeFromSharedMemRequest</span><span class="p">(</span><span class="n">rpc</span><span class="o">.</span><span class="n">Request</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Add edge attribute from shared memory to local GraphBolt partition.&quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">name</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_name</span> <span class="o">=</span> <span class="n">name</span>

    <span class="k">def</span> <span class="nf">__getstate__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_name</span>

    <span class="k">def</span> <span class="nf">__setstate__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">state</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_name</span> <span class="o">=</span> <span class="n">state</span>

    <span class="k">def</span> <span class="nf">process_request</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">server_state</span><span class="p">):</span>
        <span class="n">name</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_name</span>
        <span class="n">g</span> <span class="o">=</span> <span class="n">server_state</span><span class="o">.</span><span class="n">graph</span>
        <span class="k">if</span> <span class="n">name</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">g</span><span class="o">.</span><span class="n">edge_attributes</span><span class="p">:</span>
            <span class="n">data</span> <span class="o">=</span> <span class="n">_copy_data_from_shared_mem</span><span class="p">(</span>
                <span class="s2">&quot;__edge__&quot;</span> <span class="o">+</span> <span class="n">name</span><span class="p">,</span> <span class="p">(</span><span class="n">g</span><span class="o">.</span><span class="n">total_num_edges</span><span class="p">,)</span>
            <span class="p">)</span>
            <span class="n">g</span><span class="o">.</span><span class="n">add_edge_attribute</span><span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="n">data</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">AddEdgeAttributeFromSharedMemResponse</span><span class="p">(</span><span class="n">name</span><span class="p">)</span>


<span class="k">class</span> <span class="nc">AddEdgeAttributeFromSharedMemResponse</span><span class="p">(</span><span class="n">rpc</span><span class="o">.</span><span class="n">Response</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Ack the request of adding edge attribute from shared memory.&quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">name</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_name</span> <span class="o">=</span> <span class="n">name</span>

    <span class="k">def</span> <span class="nf">__getstate__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_name</span>

    <span class="k">def</span> <span class="nf">__setstate__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">state</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_name</span> <span class="o">=</span> <span class="n">state</span>


<span class="k">def</span> <span class="nf">_copy_graph_to_shared_mem</span><span class="p">(</span><span class="n">g</span><span class="p">,</span> <span class="n">graph_name</span><span class="p">,</span> <span class="n">graph_format</span><span class="p">,</span> <span class="n">use_graphbolt</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">use_graphbolt</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">g</span><span class="o">.</span><span class="n">copy_to_shared_memory</span><span class="p">(</span><span class="n">graph_name</span><span class="p">)</span>
    <span class="n">new_g</span> <span class="o">=</span> <span class="n">g</span><span class="o">.</span><span class="n">shared_memory</span><span class="p">(</span><span class="n">graph_name</span><span class="p">,</span> <span class="n">formats</span><span class="o">=</span><span class="n">graph_format</span><span class="p">)</span>
    <span class="c1"># We should share the node/edge data to the client explicitly instead of putting them</span>
    <span class="c1"># in the KVStore because some of the node/edge data may be duplicated.</span>
    <span class="n">new_g</span><span class="o">.</span><span class="n">ndata</span><span class="p">[</span><span class="s2">&quot;inner_node&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">_to_shared_mem</span><span class="p">(</span>
        <span class="n">g</span><span class="o">.</span><span class="n">ndata</span><span class="p">[</span><span class="s2">&quot;inner_node&quot;</span><span class="p">],</span> <span class="n">_get_ndata_path</span><span class="p">(</span><span class="n">graph_name</span><span class="p">,</span> <span class="s2">&quot;inner_node&quot;</span><span class="p">)</span>
    <span class="p">)</span>
    <span class="n">new_g</span><span class="o">.</span><span class="n">ndata</span><span class="p">[</span><span class="n">NID</span><span class="p">]</span> <span class="o">=</span> <span class="n">_to_shared_mem</span><span class="p">(</span>
        <span class="n">g</span><span class="o">.</span><span class="n">ndata</span><span class="p">[</span><span class="n">NID</span><span class="p">],</span> <span class="n">_get_ndata_path</span><span class="p">(</span><span class="n">graph_name</span><span class="p">,</span> <span class="n">NID</span><span class="p">)</span>
    <span class="p">)</span>

    <span class="n">new_g</span><span class="o">.</span><span class="n">edata</span><span class="p">[</span><span class="s2">&quot;inner_edge&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">_to_shared_mem</span><span class="p">(</span>
        <span class="n">g</span><span class="o">.</span><span class="n">edata</span><span class="p">[</span><span class="s2">&quot;inner_edge&quot;</span><span class="p">],</span> <span class="n">_get_edata_path</span><span class="p">(</span><span class="n">graph_name</span><span class="p">,</span> <span class="s2">&quot;inner_edge&quot;</span><span class="p">)</span>
    <span class="p">)</span>
    <span class="n">new_g</span><span class="o">.</span><span class="n">edata</span><span class="p">[</span><span class="n">EID</span><span class="p">]</span> <span class="o">=</span> <span class="n">_to_shared_mem</span><span class="p">(</span>
        <span class="n">g</span><span class="o">.</span><span class="n">edata</span><span class="p">[</span><span class="n">EID</span><span class="p">],</span> <span class="n">_get_edata_path</span><span class="p">(</span><span class="n">graph_name</span><span class="p">,</span> <span class="n">EID</span><span class="p">)</span>
    <span class="p">)</span>
    <span class="c1"># for heterogeneous graph, we need to put ETYPE into KVStore</span>
    <span class="c1"># for homogeneous graph, ETYPE does not exist</span>
    <span class="k">if</span> <span class="n">ETYPE</span> <span class="ow">in</span> <span class="n">g</span><span class="o">.</span><span class="n">edata</span><span class="p">:</span>
        <span class="n">new_g</span><span class="o">.</span><span class="n">edata</span><span class="p">[</span><span class="n">ETYPE</span><span class="p">]</span> <span class="o">=</span> <span class="n">_to_shared_mem</span><span class="p">(</span>
            <span class="n">g</span><span class="o">.</span><span class="n">edata</span><span class="p">[</span><span class="n">ETYPE</span><span class="p">],</span>
            <span class="n">_get_edata_path</span><span class="p">(</span><span class="n">graph_name</span><span class="p">,</span> <span class="n">ETYPE</span><span class="p">),</span>
        <span class="p">)</span>
    <span class="k">return</span> <span class="n">new_g</span>


<span class="k">def</span> <span class="nf">_get_shared_mem_ndata</span><span class="p">(</span><span class="n">g</span><span class="p">,</span> <span class="n">graph_name</span><span class="p">,</span> <span class="n">name</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Get shared-memory node data from DistGraph server.</span>

<span class="sd">    This is called by the DistGraph client to access the node data in the DistGraph server</span>
<span class="sd">    with shared memory.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">shape</span> <span class="o">=</span> <span class="p">(</span><span class="n">g</span><span class="o">.</span><span class="n">num_nodes</span><span class="p">(),)</span>
    <span class="n">dtype</span> <span class="o">=</span> <span class="n">RESERVED_FIELD_DTYPE</span><span class="p">[</span><span class="n">name</span><span class="p">]</span>
    <span class="n">dtype</span> <span class="o">=</span> <span class="n">DTYPE_DICT</span><span class="p">[</span><span class="n">dtype</span><span class="p">]</span>
    <span class="n">data</span> <span class="o">=</span> <span class="n">empty_shared_mem</span><span class="p">(</span>
        <span class="n">_get_ndata_path</span><span class="p">(</span><span class="n">graph_name</span><span class="p">,</span> <span class="n">name</span><span class="p">),</span> <span class="kc">False</span><span class="p">,</span> <span class="n">shape</span><span class="p">,</span> <span class="n">dtype</span>
    <span class="p">)</span>
    <span class="n">dlpack</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">to_dlpack</span><span class="p">()</span>
    <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">zerocopy_from_dlpack</span><span class="p">(</span><span class="n">dlpack</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">_get_shared_mem_edata</span><span class="p">(</span><span class="n">g</span><span class="p">,</span> <span class="n">graph_name</span><span class="p">,</span> <span class="n">name</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Get shared-memory edge data from DistGraph server.</span>

<span class="sd">    This is called by the DistGraph client to access the edge data in the DistGraph server</span>
<span class="sd">    with shared memory.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">shape</span> <span class="o">=</span> <span class="p">(</span><span class="n">g</span><span class="o">.</span><span class="n">num_edges</span><span class="p">(),)</span>
    <span class="n">dtype</span> <span class="o">=</span> <span class="n">RESERVED_FIELD_DTYPE</span><span class="p">[</span><span class="n">name</span><span class="p">]</span>
    <span class="n">dtype</span> <span class="o">=</span> <span class="n">DTYPE_DICT</span><span class="p">[</span><span class="n">dtype</span><span class="p">]</span>
    <span class="n">data</span> <span class="o">=</span> <span class="n">empty_shared_mem</span><span class="p">(</span>
        <span class="n">_get_edata_path</span><span class="p">(</span><span class="n">graph_name</span><span class="p">,</span> <span class="n">name</span><span class="p">),</span> <span class="kc">False</span><span class="p">,</span> <span class="n">shape</span><span class="p">,</span> <span class="n">dtype</span>
    <span class="p">)</span>
    <span class="n">dlpack</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">to_dlpack</span><span class="p">()</span>
    <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">zerocopy_from_dlpack</span><span class="p">(</span><span class="n">dlpack</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">_exist_shared_mem_array</span><span class="p">(</span><span class="n">graph_name</span><span class="p">,</span> <span class="n">name</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">exist_shared_mem_array</span><span class="p">(</span><span class="n">_get_edata_path</span><span class="p">(</span><span class="n">graph_name</span><span class="p">,</span> <span class="n">name</span><span class="p">))</span>


<span class="k">def</span> <span class="nf">_get_graph_from_shared_mem</span><span class="p">(</span><span class="n">graph_name</span><span class="p">,</span> <span class="n">use_graphbolt</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Get the graph from the DistGraph server.</span>

<span class="sd">    The DistGraph server puts the graph structure of the local partition in the shared memory.</span>
<span class="sd">    The client can access the graph structure and some metadata on nodes and edges directly</span>
<span class="sd">    through shared memory to reduce the overhead of data access.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">use_graphbolt</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">gb</span><span class="o">.</span><span class="n">load_from_shared_memory</span><span class="p">(</span><span class="n">graph_name</span><span class="p">)</span>
    <span class="n">g</span><span class="p">,</span> <span class="n">ntypes</span><span class="p">,</span> <span class="n">etypes</span> <span class="o">=</span> <span class="n">heterograph_index</span><span class="o">.</span><span class="n">create_heterograph_from_shared_memory</span><span class="p">(</span>
        <span class="n">graph_name</span>
    <span class="p">)</span>
    <span class="k">if</span> <span class="n">g</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">return</span> <span class="kc">None</span>
    <span class="n">g</span> <span class="o">=</span> <span class="n">DGLGraph</span><span class="p">(</span><span class="n">g</span><span class="p">,</span> <span class="n">ntypes</span><span class="p">,</span> <span class="n">etypes</span><span class="p">)</span>

    <span class="n">g</span><span class="o">.</span><span class="n">ndata</span><span class="p">[</span><span class="s2">&quot;inner_node&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">_get_shared_mem_ndata</span><span class="p">(</span><span class="n">g</span><span class="p">,</span> <span class="n">graph_name</span><span class="p">,</span> <span class="s2">&quot;inner_node&quot;</span><span class="p">)</span>
    <span class="n">g</span><span class="o">.</span><span class="n">ndata</span><span class="p">[</span><span class="n">NID</span><span class="p">]</span> <span class="o">=</span> <span class="n">_get_shared_mem_ndata</span><span class="p">(</span><span class="n">g</span><span class="p">,</span> <span class="n">graph_name</span><span class="p">,</span> <span class="n">NID</span><span class="p">)</span>

    <span class="n">g</span><span class="o">.</span><span class="n">edata</span><span class="p">[</span><span class="s2">&quot;inner_edge&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">_get_shared_mem_edata</span><span class="p">(</span><span class="n">g</span><span class="p">,</span> <span class="n">graph_name</span><span class="p">,</span> <span class="s2">&quot;inner_edge&quot;</span><span class="p">)</span>
    <span class="n">g</span><span class="o">.</span><span class="n">edata</span><span class="p">[</span><span class="n">EID</span><span class="p">]</span> <span class="o">=</span> <span class="n">_get_shared_mem_edata</span><span class="p">(</span><span class="n">g</span><span class="p">,</span> <span class="n">graph_name</span><span class="p">,</span> <span class="n">EID</span><span class="p">)</span>

    <span class="c1"># heterogeneous graph has ETYPE</span>
    <span class="k">if</span> <span class="n">_exist_shared_mem_array</span><span class="p">(</span><span class="n">graph_name</span><span class="p">,</span> <span class="n">ETYPE</span><span class="p">):</span>
        <span class="n">g</span><span class="o">.</span><span class="n">edata</span><span class="p">[</span><span class="n">ETYPE</span><span class="p">]</span> <span class="o">=</span> <span class="n">_get_shared_mem_edata</span><span class="p">(</span><span class="n">g</span><span class="p">,</span> <span class="n">graph_name</span><span class="p">,</span> <span class="n">ETYPE</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">g</span>


<span class="n">NodeSpace</span> <span class="o">=</span> <span class="n">namedtuple</span><span class="p">(</span><span class="s2">&quot;NodeSpace&quot;</span><span class="p">,</span> <span class="p">[</span><span class="s2">&quot;data&quot;</span><span class="p">])</span>
<span class="n">EdgeSpace</span> <span class="o">=</span> <span class="n">namedtuple</span><span class="p">(</span><span class="s2">&quot;EdgeSpace&quot;</span><span class="p">,</span> <span class="p">[</span><span class="s2">&quot;data&quot;</span><span class="p">])</span>


<span class="k">class</span> <span class="nc">HeteroNodeView</span><span class="p">(</span><span class="nb">object</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;A NodeView class to act as G.nodes for a DistGraph.&quot;&quot;&quot;</span>

    <span class="vm">__slots__</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;_graph&quot;</span><span class="p">]</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">graph</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_graph</span> <span class="o">=</span> <span class="n">graph</span>

    <span class="k">def</span> <span class="fm">__getitem__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">key</span><span class="p">):</span>
        <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">key</span><span class="p">,</span> <span class="nb">str</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">NodeSpace</span><span class="p">(</span><span class="n">data</span><span class="o">=</span><span class="n">NodeDataView</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_graph</span><span class="p">,</span> <span class="n">key</span><span class="p">))</span>


<span class="k">class</span> <span class="nc">HeteroEdgeView</span><span class="p">(</span><span class="nb">object</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;An EdgeView class to act as G.edges for a DistGraph.&quot;&quot;&quot;</span>

    <span class="vm">__slots__</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;_graph&quot;</span><span class="p">]</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">graph</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_graph</span> <span class="o">=</span> <span class="n">graph</span>

    <span class="k">def</span> <span class="fm">__getitem__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">key</span><span class="p">):</span>
        <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">key</span><span class="p">,</span> <span class="nb">str</span><span class="p">)</span> <span class="ow">or</span> <span class="p">(</span>
            <span class="nb">isinstance</span><span class="p">(</span><span class="n">key</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">)</span> <span class="ow">and</span> <span class="nb">len</span><span class="p">(</span><span class="n">key</span><span class="p">)</span> <span class="o">==</span> <span class="mi">3</span>
        <span class="p">),</span> <span class="sa">f</span><span class="s2">&quot;Expect edge type in string or triplet of string, but got </span><span class="si">{</span><span class="n">key</span><span class="si">}</span><span class="s2">.&quot;</span>
        <span class="k">return</span> <span class="n">EdgeSpace</span><span class="p">(</span><span class="n">data</span><span class="o">=</span><span class="n">EdgeDataView</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_graph</span><span class="p">,</span> <span class="n">key</span><span class="p">))</span>


<span class="k">class</span> <span class="nc">NodeDataView</span><span class="p">(</span><span class="n">MutableMapping</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;The data view class when dist_graph.ndata[...].data is called.&quot;&quot;&quot;</span>

    <span class="vm">__slots__</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;_graph&quot;</span><span class="p">,</span> <span class="s2">&quot;_data&quot;</span><span class="p">]</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">g</span><span class="p">,</span> <span class="n">ntype</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_graph</span> <span class="o">=</span> <span class="n">g</span>
        <span class="k">if</span> <span class="n">ntype</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">or</span> <span class="nb">len</span><span class="p">(</span><span class="n">g</span><span class="o">.</span><span class="n">ntypes</span><span class="p">)</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_data</span> <span class="o">=</span> <span class="n">g</span><span class="o">.</span><span class="n">_ndata_store</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">ntype</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">g</span><span class="o">.</span><span class="n">ntypes</span><span class="p">:</span>
                <span class="k">raise</span> <span class="n">DGLError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Node type </span><span class="si">{</span><span class="n">ntype</span><span class="si">}</span><span class="s2"> does not exist.&quot;</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_data</span> <span class="o">=</span> <span class="n">g</span><span class="o">.</span><span class="n">_ndata_store</span><span class="p">[</span><span class="n">ntype</span><span class="p">]</span>

    <span class="k">def</span> <span class="nf">_get_names</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="nb">list</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_data</span><span class="o">.</span><span class="n">keys</span><span class="p">())</span>

    <span class="k">def</span> <span class="fm">__getitem__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">key</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_data</span><span class="p">[</span><span class="n">key</span><span class="p">]</span>

    <span class="k">def</span> <span class="fm">__setitem__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">key</span><span class="p">,</span> <span class="n">val</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_data</span><span class="p">[</span><span class="n">key</span><span class="p">]</span> <span class="o">=</span> <span class="n">val</span>

    <span class="k">def</span> <span class="fm">__delitem__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">key</span><span class="p">):</span>
        <span class="k">del</span> <span class="bp">self</span><span class="o">.</span><span class="n">_data</span><span class="p">[</span><span class="n">key</span><span class="p">]</span>

    <span class="k">def</span> <span class="fm">__len__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="c1"># The number of node data may change. Let&#39;s count it every time we need them.</span>
        <span class="c1"># It&#39;s not called frequently. It should be fine.</span>
        <span class="k">return</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_data</span><span class="p">)</span>

    <span class="k">def</span> <span class="fm">__iter__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="nb">iter</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_data</span><span class="p">)</span>

    <span class="k">def</span> <span class="fm">__repr__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="n">reprs</span> <span class="o">=</span> <span class="p">{}</span>
        <span class="k">for</span> <span class="n">name</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_data</span><span class="p">:</span>
            <span class="n">dtype</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">dtype</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_data</span><span class="p">[</span><span class="n">name</span><span class="p">])</span>
            <span class="n">shape</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_data</span><span class="p">[</span><span class="n">name</span><span class="p">])</span>
            <span class="n">reprs</span><span class="p">[</span><span class="n">name</span><span class="p">]</span> <span class="o">=</span> <span class="s2">&quot;DistTensor(shape=</span><span class="si">{}</span><span class="s2">, dtype=</span><span class="si">{}</span><span class="s2">)&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span>
                <span class="nb">str</span><span class="p">(</span><span class="n">shape</span><span class="p">),</span> <span class="nb">str</span><span class="p">(</span><span class="n">dtype</span><span class="p">)</span>
            <span class="p">)</span>
        <span class="k">return</span> <span class="nb">repr</span><span class="p">(</span><span class="n">reprs</span><span class="p">)</span>


<span class="k">class</span> <span class="nc">EdgeDataView</span><span class="p">(</span><span class="n">MutableMapping</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;The data view class when G.edges[...].data is called.&quot;&quot;&quot;</span>

    <span class="vm">__slots__</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;_graph&quot;</span><span class="p">,</span> <span class="s2">&quot;_data&quot;</span><span class="p">]</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">g</span><span class="p">,</span> <span class="n">etype</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_graph</span> <span class="o">=</span> <span class="n">g</span>
        <span class="k">if</span> <span class="n">etype</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">or</span> <span class="nb">len</span><span class="p">(</span><span class="n">g</span><span class="o">.</span><span class="n">canonical_etypes</span><span class="p">)</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_data</span> <span class="o">=</span> <span class="n">g</span><span class="o">.</span><span class="n">_edata_store</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">c_etype</span> <span class="o">=</span> <span class="n">g</span><span class="o">.</span><span class="n">to_canonical_etype</span><span class="p">(</span><span class="n">etype</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_data</span> <span class="o">=</span> <span class="n">g</span><span class="o">.</span><span class="n">_edata_store</span><span class="p">[</span><span class="n">c_etype</span><span class="p">]</span>

    <span class="k">def</span> <span class="nf">_get_names</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="nb">list</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_data</span><span class="o">.</span><span class="n">keys</span><span class="p">())</span>

    <span class="k">def</span> <span class="fm">__getitem__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">key</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_data</span><span class="p">[</span><span class="n">key</span><span class="p">]</span>

    <span class="k">def</span> <span class="fm">__setitem__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">key</span><span class="p">,</span> <span class="n">val</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_data</span><span class="p">[</span><span class="n">key</span><span class="p">]</span> <span class="o">=</span> <span class="n">val</span>

    <span class="k">def</span> <span class="fm">__delitem__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">key</span><span class="p">):</span>
        <span class="k">del</span> <span class="bp">self</span><span class="o">.</span><span class="n">_data</span><span class="p">[</span><span class="n">key</span><span class="p">]</span>

    <span class="k">def</span> <span class="fm">__len__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="c1"># The number of edge data may change. Let&#39;s count it every time we need them.</span>
        <span class="c1"># It&#39;s not called frequently. It should be fine.</span>
        <span class="k">return</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_data</span><span class="p">)</span>

    <span class="k">def</span> <span class="fm">__iter__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="nb">iter</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_data</span><span class="p">)</span>

    <span class="k">def</span> <span class="fm">__repr__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="n">reprs</span> <span class="o">=</span> <span class="p">{}</span>
        <span class="k">for</span> <span class="n">name</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_data</span><span class="p">:</span>
            <span class="n">dtype</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">dtype</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_data</span><span class="p">[</span><span class="n">name</span><span class="p">])</span>
            <span class="n">shape</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_data</span><span class="p">[</span><span class="n">name</span><span class="p">])</span>
            <span class="n">reprs</span><span class="p">[</span><span class="n">name</span><span class="p">]</span> <span class="o">=</span> <span class="s2">&quot;DistTensor(shape=</span><span class="si">{}</span><span class="s2">, dtype=</span><span class="si">{}</span><span class="s2">)&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span>
                <span class="nb">str</span><span class="p">(</span><span class="n">shape</span><span class="p">),</span> <span class="nb">str</span><span class="p">(</span><span class="n">dtype</span><span class="p">)</span>
            <span class="p">)</span>
        <span class="k">return</span> <span class="nb">repr</span><span class="p">(</span><span class="n">reprs</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">_format_partition</span><span class="p">(</span><span class="n">graph</span><span class="p">,</span> <span class="n">graph_format</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Format the partition to the specified format.&quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">graph</span><span class="p">,</span> <span class="n">gb</span><span class="o">.</span><span class="n">FusedCSCSamplingGraph</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">graph</span>
    <span class="c1"># formatting dtype</span>
    <span class="c1"># TODO(Rui) Formatting forcely is not a perfect solution.</span>
    <span class="c1">#   We&#39;d better store all dtypes when mapping to shared memory</span>
    <span class="c1">#   and map back with original dtypes.</span>
    <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">dtype</span> <span class="ow">in</span> <span class="n">RESERVED_FIELD_DTYPE</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
        <span class="k">if</span> <span class="n">k</span> <span class="ow">in</span> <span class="n">graph</span><span class="o">.</span><span class="n">ndata</span><span class="p">:</span>
            <span class="n">graph</span><span class="o">.</span><span class="n">ndata</span><span class="p">[</span><span class="n">k</span><span class="p">]</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">graph</span><span class="o">.</span><span class="n">ndata</span><span class="p">[</span><span class="n">k</span><span class="p">],</span> <span class="n">dtype</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">k</span> <span class="ow">in</span> <span class="n">graph</span><span class="o">.</span><span class="n">edata</span><span class="p">:</span>
            <span class="n">graph</span><span class="o">.</span><span class="n">edata</span><span class="p">[</span><span class="n">k</span><span class="p">]</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">graph</span><span class="o">.</span><span class="n">edata</span><span class="p">[</span><span class="n">k</span><span class="p">],</span> <span class="n">dtype</span><span class="p">)</span>
    <span class="c1"># Create the graph formats specified the users.</span>
    <span class="nb">print</span><span class="p">(</span>
        <span class="s2">&quot;Start to create specified graph formats which may take &quot;</span>
        <span class="s2">&quot;non-trivial time.&quot;</span>
    <span class="p">)</span>
    <span class="n">graph</span> <span class="o">=</span> <span class="n">graph</span><span class="o">.</span><span class="n">formats</span><span class="p">(</span><span class="n">graph_format</span><span class="p">)</span>
    <span class="n">graph</span><span class="o">.</span><span class="n">create_formats_</span><span class="p">()</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Finished creating specified graph formats: </span><span class="si">{</span><span class="n">graph_format</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">graph</span>


<span class="k">class</span> <span class="nc">DistGraphServer</span><span class="p">(</span><span class="n">KVServer</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;The DistGraph server.</span>

<span class="sd">    This DistGraph server loads the graph data and sets up a service so that trainers and</span>
<span class="sd">    samplers can read data of a graph partition (graph structure, node data and edge data)</span>
<span class="sd">    from remote machines. A server is responsible for one graph partition.</span>

<span class="sd">    Currently, each machine runs only one main server with a set of backup servers to handle</span>
<span class="sd">    clients&#39; requests. The main server and the backup servers all handle the requests for the same</span>
<span class="sd">    graph partition. They all share the partition data (graph structure and node/edge data) with</span>
<span class="sd">    shared memory.</span>

<span class="sd">    By default, the partition data is shared with the DistGraph clients that run on</span>
<span class="sd">    the same machine. However, a user can disable shared memory option. This is useful for the case</span>
<span class="sd">    that a user wants to run the server and the client on different machines.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    server_id : int</span>
<span class="sd">        The server ID (start from 0).</span>
<span class="sd">    ip_config : str</span>
<span class="sd">        Path of IP configuration file.</span>
<span class="sd">    num_servers : int</span>
<span class="sd">        Server count on each machine.</span>
<span class="sd">    num_clients : int</span>
<span class="sd">        Total number of client nodes.</span>
<span class="sd">    part_config : string</span>
<span class="sd">        The path of the config file generated by the partition tool.</span>
<span class="sd">    disable_shared_mem : bool</span>
<span class="sd">        Disable shared memory.</span>
<span class="sd">    graph_format : str or list of str</span>
<span class="sd">        The graph formats.</span>
<span class="sd">    use_graphbolt : bool</span>
<span class="sd">        Whether to load GraphBolt partition. Default: False.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">server_id</span><span class="p">,</span>
        <span class="n">ip_config</span><span class="p">,</span>
        <span class="n">num_servers</span><span class="p">,</span>
        <span class="n">num_clients</span><span class="p">,</span>
        <span class="n">part_config</span><span class="p">,</span>
        <span class="n">disable_shared_mem</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
        <span class="n">graph_format</span><span class="o">=</span><span class="p">(</span><span class="s2">&quot;csc&quot;</span><span class="p">,</span> <span class="s2">&quot;coo&quot;</span><span class="p">),</span>
        <span class="n">use_graphbolt</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">DistGraphServer</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span>
            <span class="n">server_id</span><span class="o">=</span><span class="n">server_id</span><span class="p">,</span>
            <span class="n">ip_config</span><span class="o">=</span><span class="n">ip_config</span><span class="p">,</span>
            <span class="n">num_servers</span><span class="o">=</span><span class="n">num_servers</span><span class="p">,</span>
            <span class="n">num_clients</span><span class="o">=</span><span class="n">num_clients</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">ip_config</span> <span class="o">=</span> <span class="n">ip_config</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">num_servers</span> <span class="o">=</span> <span class="n">num_servers</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">use_graphbolt</span> <span class="o">=</span> <span class="n">use_graphbolt</span>
        <span class="c1"># Load graph partition data.</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">is_backup_server</span><span class="p">():</span>
            <span class="c1"># The backup server doesn&#39;t load the graph partition. It&#39;ll initialized afterwards.</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">gpb</span><span class="p">,</span> <span class="n">graph_name</span><span class="p">,</span> <span class="n">ntypes</span><span class="p">,</span> <span class="n">etypes</span> <span class="o">=</span> <span class="n">load_partition_book</span><span class="p">(</span>
                <span class="n">part_config</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">part_id</span>
            <span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">client_g</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="c1"># Loading of node/edge_feats are deferred to lower the peak memory consumption.</span>
            <span class="p">(</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">client_g</span><span class="p">,</span>
                <span class="n">_</span><span class="p">,</span>
                <span class="n">_</span><span class="p">,</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">gpb</span><span class="p">,</span>
                <span class="n">graph_name</span><span class="p">,</span>
                <span class="n">ntypes</span><span class="p">,</span>
                <span class="n">etypes</span><span class="p">,</span>
            <span class="p">)</span> <span class="o">=</span> <span class="n">load_partition</span><span class="p">(</span>
                <span class="n">part_config</span><span class="p">,</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">part_id</span><span class="p">,</span>
                <span class="n">load_feats</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                <span class="n">use_graphbolt</span><span class="o">=</span><span class="n">use_graphbolt</span><span class="p">,</span>
            <span class="p">)</span>
            <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;load &quot;</span> <span class="o">+</span> <span class="n">graph_name</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">client_g</span> <span class="o">=</span> <span class="n">_format_partition</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">client_g</span><span class="p">,</span> <span class="n">graph_format</span><span class="p">)</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="n">disable_shared_mem</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">client_g</span> <span class="o">=</span> <span class="n">_copy_graph_to_shared_mem</span><span class="p">(</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">client_g</span><span class="p">,</span> <span class="n">graph_name</span><span class="p">,</span> <span class="n">graph_format</span><span class="p">,</span> <span class="n">use_graphbolt</span>
                <span class="p">)</span>

        <span class="k">if</span> <span class="ow">not</span> <span class="n">disable_shared_mem</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">gpb</span><span class="o">.</span><span class="n">shared_memory</span><span class="p">(</span><span class="n">graph_name</span><span class="p">)</span>
        <span class="k">assert</span> <span class="bp">self</span><span class="o">.</span><span class="n">gpb</span><span class="o">.</span><span class="n">partid</span> <span class="o">==</span> <span class="bp">self</span><span class="o">.</span><span class="n">part_id</span>
        <span class="k">for</span> <span class="n">ntype</span> <span class="ow">in</span> <span class="n">ntypes</span><span class="p">:</span>
            <span class="n">node_name</span> <span class="o">=</span> <span class="n">HeteroDataName</span><span class="p">(</span><span class="kc">True</span><span class="p">,</span> <span class="n">ntype</span><span class="p">,</span> <span class="s2">&quot;&quot;</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">add_part_policy</span><span class="p">(</span>
                <span class="n">PartitionPolicy</span><span class="p">(</span><span class="n">node_name</span><span class="o">.</span><span class="n">policy_str</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">gpb</span><span class="p">)</span>
            <span class="p">)</span>
        <span class="k">for</span> <span class="n">etype</span> <span class="ow">in</span> <span class="n">etypes</span><span class="p">:</span>
            <span class="n">edge_name</span> <span class="o">=</span> <span class="n">HeteroDataName</span><span class="p">(</span><span class="kc">False</span><span class="p">,</span> <span class="n">etype</span><span class="p">,</span> <span class="s2">&quot;&quot;</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">add_part_policy</span><span class="p">(</span>
                <span class="n">PartitionPolicy</span><span class="p">(</span><span class="n">edge_name</span><span class="o">.</span><span class="n">policy_str</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">gpb</span><span class="p">)</span>
            <span class="p">)</span>

        <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">is_backup_server</span><span class="p">():</span>
            <span class="n">node_feats</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">load_partition_feats</span><span class="p">(</span>
                <span class="n">part_config</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">part_id</span><span class="p">,</span> <span class="n">load_nodes</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">load_edges</span><span class="o">=</span><span class="kc">False</span>
            <span class="p">)</span>
            <span class="k">for</span> <span class="n">name</span> <span class="ow">in</span> <span class="n">node_feats</span><span class="p">:</span>
                <span class="c1"># The feature name has the following format: node_type + &quot;/&quot; + feature_name to avoid</span>
                <span class="c1"># feature name collision for different node types.</span>
                <span class="n">ntype</span><span class="p">,</span> <span class="n">feat_name</span> <span class="o">=</span> <span class="n">name</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s2">&quot;/&quot;</span><span class="p">)</span>
                <span class="n">data_name</span> <span class="o">=</span> <span class="n">HeteroDataName</span><span class="p">(</span><span class="kc">True</span><span class="p">,</span> <span class="n">ntype</span><span class="p">,</span> <span class="n">feat_name</span><span class="p">)</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">init_data</span><span class="p">(</span>
                    <span class="n">name</span><span class="o">=</span><span class="nb">str</span><span class="p">(</span><span class="n">data_name</span><span class="p">),</span>
                    <span class="n">policy_str</span><span class="o">=</span><span class="n">data_name</span><span class="o">.</span><span class="n">policy_str</span><span class="p">,</span>
                    <span class="n">data_tensor</span><span class="o">=</span><span class="n">node_feats</span><span class="p">[</span><span class="n">name</span><span class="p">],</span>
                <span class="p">)</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">orig_data</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="nb">str</span><span class="p">(</span><span class="n">data_name</span><span class="p">))</span>
            <span class="c1"># Let&#39;s free once node features are copied to shared memory</span>
            <span class="k">del</span> <span class="n">node_feats</span>
            <span class="n">gc</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
            <span class="n">_</span><span class="p">,</span> <span class="n">edge_feats</span> <span class="o">=</span> <span class="n">load_partition_feats</span><span class="p">(</span>
                <span class="n">part_config</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">part_id</span><span class="p">,</span> <span class="n">load_nodes</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">load_edges</span><span class="o">=</span><span class="kc">True</span>
            <span class="p">)</span>
            <span class="k">for</span> <span class="n">name</span> <span class="ow">in</span> <span class="n">edge_feats</span><span class="p">:</span>
                <span class="c1"># The feature name has the following format: edge_type + &quot;/&quot; + feature_name to avoid</span>
                <span class="c1"># feature name collision for different edge types.</span>
                <span class="n">etype</span><span class="p">,</span> <span class="n">feat_name</span> <span class="o">=</span> <span class="n">name</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s2">&quot;/&quot;</span><span class="p">)</span>
                <span class="n">etype</span> <span class="o">=</span> <span class="n">_etype_str_to_tuple</span><span class="p">(</span><span class="n">etype</span><span class="p">)</span>
                <span class="n">data_name</span> <span class="o">=</span> <span class="n">HeteroDataName</span><span class="p">(</span><span class="kc">False</span><span class="p">,</span> <span class="n">etype</span><span class="p">,</span> <span class="n">feat_name</span><span class="p">)</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">init_data</span><span class="p">(</span>
                    <span class="n">name</span><span class="o">=</span><span class="nb">str</span><span class="p">(</span><span class="n">data_name</span><span class="p">),</span>
                    <span class="n">policy_str</span><span class="o">=</span><span class="n">data_name</span><span class="o">.</span><span class="n">policy_str</span><span class="p">,</span>
                    <span class="n">data_tensor</span><span class="o">=</span><span class="n">edge_feats</span><span class="p">[</span><span class="n">name</span><span class="p">],</span>
                <span class="p">)</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">orig_data</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="nb">str</span><span class="p">(</span><span class="n">data_name</span><span class="p">))</span>
            <span class="c1"># Let&#39;s free once edge features are copied to shared memory</span>
            <span class="k">del</span> <span class="n">edge_feats</span>
            <span class="n">gc</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">start</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Start graph store server.&quot;&quot;&quot;</span>
        <span class="c1"># start server</span>
        <span class="n">server_state</span> <span class="o">=</span> <span class="n">ServerState</span><span class="p">(</span>
            <span class="n">kv_store</span><span class="o">=</span><span class="bp">self</span><span class="p">,</span>
            <span class="n">local_g</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">client_g</span><span class="p">,</span>
            <span class="n">partition_book</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">gpb</span><span class="p">,</span>
            <span class="n">use_graphbolt</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">use_graphbolt</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span>
            <span class="s2">&quot;start graph service on server </span><span class="si">{}</span><span class="s2"> for part </span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">server_id</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">part_id</span>
            <span class="p">)</span>
        <span class="p">)</span>
        <span class="n">start_server</span><span class="p">(</span>
            <span class="n">server_id</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">server_id</span><span class="p">,</span>
            <span class="n">ip_config</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">ip_config</span><span class="p">,</span>
            <span class="n">num_servers</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">num_servers</span><span class="p">,</span>
            <span class="n">num_clients</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">num_clients</span><span class="p">,</span>
            <span class="n">server_state</span><span class="o">=</span><span class="n">server_state</span><span class="p">,</span>
        <span class="p">)</span>


<div class="viewcode-block" id="DistGraph">
<a class="viewcode-back" href="../../../api/python/dgl.distributed.html#dgl.distributed.DistGraph">[docs]</a>
<span class="k">class</span> <span class="nc">DistGraph</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;The class for accessing a distributed graph.</span>

<span class="sd">    This class provides a subset of DGLGraph APIs for accessing partitioned graph data in</span>
<span class="sd">    distributed GNN training and inference. Thus, its main use case is to work with</span>
<span class="sd">    distributed sampling APIs to generate mini-batches and perform forward and</span>
<span class="sd">    backward computation on the mini-batches.</span>

<span class="sd">    The class can run in two modes: the standalone mode and the distributed mode.</span>

<span class="sd">    * When a user runs the training script normally, ``DistGraph`` will be in the standalone mode.</span>
<span class="sd">      In this mode, the input data must be constructed by</span>
<span class="sd">      :py:meth:`~dgl.distributed.partition.partition_graph` with only one partition. This mode is</span>
<span class="sd">      used for testing and debugging purpose. In this mode, users have to provide ``part_config``</span>
<span class="sd">      so that ``DistGraph`` can load the input graph.</span>
<span class="sd">    * When a user runs the training script with the distributed launch script, ``DistGraph`` will</span>
<span class="sd">      be set into the distributed mode. This is used for actual distributed training. All data of</span>
<span class="sd">      partitions are loaded by the ``DistGraph`` servers, which are created by DGL&#39;s launch script.</span>
<span class="sd">      ``DistGraph`` connects with the servers to access the partitioned graph data.</span>

<span class="sd">    Currently, the ``DistGraph`` servers and clients run on the same set of machines</span>
<span class="sd">    in the distributed mode. ``DistGraph`` uses shared-memory to access the partition data</span>
<span class="sd">    in the local machine. This gives the best performance for distributed training</span>

<span class="sd">    Users may want to run ``DistGraph`` servers and clients on separate sets of machines.</span>
<span class="sd">    In this case, a user may want to disable shared memory by passing</span>
<span class="sd">    ``disable_shared_mem=False`` when creating ``DistGraphServer``. When shared memory is disabled,</span>
<span class="sd">    a user has to pass a partition book.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    graph_name : str</span>
<span class="sd">        The name of the graph. This name has to be the same as the one used for</span>
<span class="sd">        partitioning a graph in :py:meth:`dgl.distributed.partition.partition_graph`.</span>
<span class="sd">    gpb : GraphPartitionBook, optional</span>
<span class="sd">        The partition book object. Normally, users do not need to provide the partition book.</span>
<span class="sd">        This argument is necessary only when users want to run server process and trainer</span>
<span class="sd">        processes on different machines.</span>
<span class="sd">    part_config : str, optional</span>
<span class="sd">        The path of partition configuration file generated by</span>
<span class="sd">        :py:meth:`dgl.distributed.partition.partition_graph`. It&#39;s used in the standalone mode.</span>

<span class="sd">    Examples</span>
<span class="sd">    --------</span>
<span class="sd">    The example shows the creation of ``DistGraph`` in the standalone mode.</span>

<span class="sd">    &gt;&gt;&gt; dgl.distributed.partition_graph(g, &#39;graph_name&#39;, 1, num_hops=1, part_method=&#39;metis&#39;,</span>
<span class="sd">    ...                                 out_path=&#39;output/&#39;)</span>
<span class="sd">    &gt;&gt;&gt; g = dgl.distributed.DistGraph(&#39;graph_name&#39;, part_config=&#39;output/graph_name.json&#39;)</span>

<span class="sd">    The example shows the creation of ``DistGraph`` in the distributed mode.</span>

<span class="sd">    &gt;&gt;&gt; g = dgl.distributed.DistGraph(&#39;graph-name&#39;)</span>

<span class="sd">    The code below shows the mini-batch training using ``DistGraph``.</span>

<span class="sd">    &gt;&gt;&gt; def sample(seeds):</span>
<span class="sd">    ...     seeds = th.LongTensor(np.asarray(seeds))</span>
<span class="sd">    ...     frontier = dgl.distributed.sample_neighbors(g, seeds, 10)</span>
<span class="sd">    ...     return dgl.to_block(frontier, seeds)</span>
<span class="sd">    &gt;&gt;&gt; dataloader = dgl.distributed.DistDataLoader(dataset=nodes, batch_size=1000,</span>
<span class="sd">    ...                                             collate_fn=sample, shuffle=True)</span>
<span class="sd">    &gt;&gt;&gt; for block in dataloader:</span>
<span class="sd">    ...     feat = g.ndata[&#39;features&#39;][block.srcdata[dgl.NID]]</span>
<span class="sd">    ...     labels = g.ndata[&#39;labels&#39;][block.dstdata[dgl.NID]]</span>
<span class="sd">    ...     pred = model(block, feat)</span>

<span class="sd">    Note</span>
<span class="sd">    ----</span>
<span class="sd">    DGL&#39;s distributed training by default runs server processes and trainer processes on the same</span>
<span class="sd">    set of machines. If users need to run them on different sets of machines, it requires</span>
<span class="sd">    manually setting up servers and trainers. The setup is not fully tested yet.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">graph_name</span><span class="p">,</span> <span class="n">gpb</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">part_config</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">graph_name</span> <span class="o">=</span> <span class="n">graph_name</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_added_edge_attributes</span> <span class="o">=</span> <span class="p">[]</span>  <span class="c1"># For prob/mask sampling on GB.</span>
        <span class="k">if</span> <span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;DGL_DIST_MODE&quot;</span><span class="p">,</span> <span class="s2">&quot;standalone&quot;</span><span class="p">)</span> <span class="o">==</span> <span class="s2">&quot;standalone&quot;</span><span class="p">:</span>
            <span class="c1"># &quot;GraphBolt is not supported in standalone mode.&quot;</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_use_graphbolt</span> <span class="o">=</span> <span class="kc">False</span>
            <span class="k">assert</span> <span class="p">(</span>
                <span class="n">part_config</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
            <span class="p">),</span> <span class="s2">&quot;When running in the standalone model, the partition config file is required&quot;</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_client</span> <span class="o">=</span> <span class="n">get_kvstore</span><span class="p">()</span>
            <span class="k">assert</span> <span class="p">(</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">_client</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
            <span class="p">),</span> <span class="s2">&quot;Distributed module is not initialized. Please call dgl.distributed.initialize.&quot;</span>
            <span class="c1"># Load graph partition data.</span>
            <span class="n">g</span><span class="p">,</span> <span class="n">node_feats</span><span class="p">,</span> <span class="n">edge_feats</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_gpb</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">load_partition</span><span class="p">(</span>
                <span class="n">part_config</span><span class="p">,</span> <span class="mi">0</span>
            <span class="p">)</span>
            <span class="k">assert</span> <span class="p">(</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">_gpb</span><span class="o">.</span><span class="n">num_partitions</span><span class="p">()</span> <span class="o">==</span> <span class="mi">1</span>
            <span class="p">),</span> <span class="s2">&quot;The standalone mode can only work with the graph data with one partition&quot;</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_gpb</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">_gpb</span> <span class="o">=</span> <span class="n">gpb</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_g</span> <span class="o">=</span> <span class="n">g</span>
            <span class="k">for</span> <span class="n">name</span> <span class="ow">in</span> <span class="n">node_feats</span><span class="p">:</span>
                <span class="c1"># The feature name has the following format: node_type + &quot;/&quot; + feature_name.</span>
                <span class="n">ntype</span><span class="p">,</span> <span class="n">feat_name</span> <span class="o">=</span> <span class="n">name</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s2">&quot;/&quot;</span><span class="p">)</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">_client</span><span class="o">.</span><span class="n">add_data</span><span class="p">(</span>
                    <span class="nb">str</span><span class="p">(</span><span class="n">HeteroDataName</span><span class="p">(</span><span class="kc">True</span><span class="p">,</span> <span class="n">ntype</span><span class="p">,</span> <span class="n">feat_name</span><span class="p">)),</span>
                    <span class="n">node_feats</span><span class="p">[</span><span class="n">name</span><span class="p">],</span>
                    <span class="n">NodePartitionPolicy</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_gpb</span><span class="p">,</span> <span class="n">ntype</span><span class="o">=</span><span class="n">ntype</span><span class="p">),</span>
                <span class="p">)</span>
            <span class="k">for</span> <span class="n">name</span> <span class="ow">in</span> <span class="n">edge_feats</span><span class="p">:</span>
                <span class="c1"># The feature name has the following format: edge_type + &quot;/&quot; + feature_name.</span>
                <span class="n">etype</span><span class="p">,</span> <span class="n">feat_name</span> <span class="o">=</span> <span class="n">name</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s2">&quot;/&quot;</span><span class="p">)</span>
                <span class="n">etype</span> <span class="o">=</span> <span class="n">_etype_str_to_tuple</span><span class="p">(</span><span class="n">etype</span><span class="p">)</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">_client</span><span class="o">.</span><span class="n">add_data</span><span class="p">(</span>
                    <span class="nb">str</span><span class="p">(</span><span class="n">HeteroDataName</span><span class="p">(</span><span class="kc">False</span><span class="p">,</span> <span class="n">etype</span><span class="p">,</span> <span class="n">feat_name</span><span class="p">)),</span>
                    <span class="n">edge_feats</span><span class="p">[</span><span class="n">name</span><span class="p">],</span>
                    <span class="n">EdgePartitionPolicy</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_gpb</span><span class="p">,</span> <span class="n">etype</span><span class="o">=</span><span class="n">etype</span><span class="p">),</span>
                <span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_client</span><span class="o">.</span><span class="n">map_shared_data</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_gpb</span><span class="p">)</span>
            <span class="n">rpc</span><span class="o">.</span><span class="n">set_num_client</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="c1"># Query the main server about whether GraphBolt is used.</span>
            <span class="n">rpc</span><span class="o">.</span><span class="n">send_request</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">QueryIfUseGraphBoltRequest</span><span class="p">())</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_use_graphbolt</span> <span class="o">=</span> <span class="n">rpc</span><span class="o">.</span><span class="n">recv_response</span><span class="p">()</span><span class="o">.</span><span class="n">_use_graphbolt</span>

            <span class="bp">self</span><span class="o">.</span><span class="n">_init</span><span class="p">(</span><span class="n">gpb</span><span class="p">)</span>
            <span class="c1"># Tell the backup servers to load the graph structure from shared memory.</span>
            <span class="k">for</span> <span class="n">server_id</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_client</span><span class="o">.</span><span class="n">num_servers</span><span class="p">):</span>
                <span class="n">rpc</span><span class="o">.</span><span class="n">send_request</span><span class="p">(</span><span class="n">server_id</span><span class="p">,</span> <span class="n">InitGraphRequest</span><span class="p">(</span><span class="n">graph_name</span><span class="p">))</span>
            <span class="k">for</span> <span class="n">server_id</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_client</span><span class="o">.</span><span class="n">num_servers</span><span class="p">):</span>
                <span class="n">rpc</span><span class="o">.</span><span class="n">recv_response</span><span class="p">()</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_client</span><span class="o">.</span><span class="n">barrier</span><span class="p">()</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">_init_ndata_store</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_init_edata_store</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_init_metadata</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">_init</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">gpb</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_client</span> <span class="o">=</span> <span class="n">get_kvstore</span><span class="p">()</span>
        <span class="k">assert</span> <span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_client</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
        <span class="p">),</span> <span class="s2">&quot;Distributed module is not initialized. Please call dgl.distributed.initialize.&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_g</span> <span class="o">=</span> <span class="n">_get_graph_from_shared_mem</span><span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">graph_name</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_use_graphbolt</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_gpb</span> <span class="o">=</span> <span class="n">get_shared_mem_partition_book</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">graph_name</span><span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_gpb</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_gpb</span> <span class="o">=</span> <span class="n">gpb</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_client</span><span class="o">.</span><span class="n">map_shared_data</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_gpb</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">_init_ndata_store</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize node data store.&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_ndata_store</span> <span class="o">=</span> <span class="p">{}</span>
        <span class="k">for</span> <span class="n">ntype</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">ntypes</span><span class="p">:</span>
            <span class="n">names</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_get_ndata_names</span><span class="p">(</span><span class="n">ntype</span><span class="p">)</span>
            <span class="n">data</span> <span class="o">=</span> <span class="p">{}</span>
            <span class="k">for</span> <span class="n">name</span> <span class="ow">in</span> <span class="n">names</span><span class="p">:</span>
                <span class="k">assert</span> <span class="n">name</span><span class="o">.</span><span class="n">is_node</span><span class="p">()</span>
                <span class="n">policy</span> <span class="o">=</span> <span class="n">PartitionPolicy</span><span class="p">(</span>
                    <span class="n">name</span><span class="o">.</span><span class="n">policy_str</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_partition_book</span><span class="p">()</span>
                <span class="p">)</span>
                <span class="n">dtype</span><span class="p">,</span> <span class="n">shape</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_client</span><span class="o">.</span><span class="n">get_data_meta</span><span class="p">(</span><span class="nb">str</span><span class="p">(</span><span class="n">name</span><span class="p">))</span>
                <span class="c1"># We create a wrapper on the existing tensor in the kvstore.</span>
                <span class="n">data</span><span class="p">[</span><span class="n">name</span><span class="o">.</span><span class="n">get_name</span><span class="p">()]</span> <span class="o">=</span> <span class="n">DistTensor</span><span class="p">(</span>
                    <span class="n">shape</span><span class="p">,</span>
                    <span class="n">dtype</span><span class="p">,</span>
                    <span class="n">name</span><span class="o">.</span><span class="n">get_name</span><span class="p">(),</span>
                    <span class="n">part_policy</span><span class="o">=</span><span class="n">policy</span><span class="p">,</span>
                    <span class="n">attach</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                <span class="p">)</span>
            <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">ntypes</span><span class="p">)</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">_ndata_store</span> <span class="o">=</span> <span class="n">data</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">_ndata_store</span><span class="p">[</span><span class="n">ntype</span><span class="p">]</span> <span class="o">=</span> <span class="n">data</span>

    <span class="k">def</span> <span class="nf">_init_edata_store</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize edge data store.&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_edata_store</span> <span class="o">=</span> <span class="p">{}</span>
        <span class="k">for</span> <span class="n">etype</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">canonical_etypes</span><span class="p">:</span>
            <span class="n">names</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_get_edata_names</span><span class="p">(</span><span class="n">etype</span><span class="p">)</span>
            <span class="n">data</span> <span class="o">=</span> <span class="p">{}</span>
            <span class="k">for</span> <span class="n">name</span> <span class="ow">in</span> <span class="n">names</span><span class="p">:</span>
                <span class="k">assert</span> <span class="n">name</span><span class="o">.</span><span class="n">is_edge</span><span class="p">()</span>
                <span class="n">policy</span> <span class="o">=</span> <span class="n">PartitionPolicy</span><span class="p">(</span>
                    <span class="n">name</span><span class="o">.</span><span class="n">policy_str</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_partition_book</span><span class="p">()</span>
                <span class="p">)</span>
                <span class="n">dtype</span><span class="p">,</span> <span class="n">shape</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_client</span><span class="o">.</span><span class="n">get_data_meta</span><span class="p">(</span><span class="nb">str</span><span class="p">(</span><span class="n">name</span><span class="p">))</span>
                <span class="c1"># We create a wrapper on the existing tensor in the kvstore.</span>
                <span class="n">data</span><span class="p">[</span><span class="n">name</span><span class="o">.</span><span class="n">get_name</span><span class="p">()]</span> <span class="o">=</span> <span class="n">DistTensor</span><span class="p">(</span>
                    <span class="n">shape</span><span class="p">,</span>
                    <span class="n">dtype</span><span class="p">,</span>
                    <span class="n">name</span><span class="o">.</span><span class="n">get_name</span><span class="p">(),</span>
                    <span class="n">part_policy</span><span class="o">=</span><span class="n">policy</span><span class="p">,</span>
                    <span class="n">attach</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                <span class="p">)</span>
            <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">canonical_etypes</span><span class="p">)</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">_edata_store</span> <span class="o">=</span> <span class="n">data</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">_edata_store</span><span class="p">[</span><span class="n">etype</span><span class="p">]</span> <span class="o">=</span> <span class="n">data</span>

    <span class="k">def</span> <span class="nf">_init_metadata</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_num_nodes</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_num_edges</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="k">for</span> <span class="n">part_md</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_gpb</span><span class="o">.</span><span class="n">metadata</span><span class="p">():</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_num_nodes</span> <span class="o">+=</span> <span class="nb">int</span><span class="p">(</span><span class="n">part_md</span><span class="p">[</span><span class="s2">&quot;num_nodes&quot;</span><span class="p">])</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_num_edges</span> <span class="o">+=</span> <span class="nb">int</span><span class="p">(</span><span class="n">part_md</span><span class="p">[</span><span class="s2">&quot;num_edges&quot;</span><span class="p">])</span>

        <span class="c1"># When we store node/edge types in a list, they are stored in the order of type IDs.</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_ntype_map</span> <span class="o">=</span> <span class="p">{</span><span class="n">ntype</span><span class="p">:</span> <span class="n">i</span> <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">ntype</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">ntypes</span><span class="p">)}</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_etype_map</span> <span class="o">=</span> <span class="p">{</span>
            <span class="n">etype</span><span class="p">:</span> <span class="n">i</span> <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">etype</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">canonical_etypes</span><span class="p">)</span>
        <span class="p">}</span>

    <span class="k">def</span> <span class="nf">__getstate__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">graph_name</span><span class="p">,</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_gpb</span><span class="p">,</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_use_graphbolt</span><span class="p">,</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_added_edge_attributes</span><span class="p">,</span>
        <span class="p">)</span>

    <span class="k">def</span> <span class="nf">__setstate__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">state</span><span class="p">):</span>
        <span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">graph_name</span><span class="p">,</span>
            <span class="n">gpb</span><span class="p">,</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_use_graphbolt</span><span class="p">,</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_added_edge_attributes</span><span class="p">,</span>
        <span class="p">)</span> <span class="o">=</span> <span class="n">state</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_init</span><span class="p">(</span><span class="n">gpb</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">_init_ndata_store</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_init_edata_store</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_init_metadata</span><span class="p">()</span>

        <span class="c1"># For prob/mask sampling on GB only.</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_use_graphbolt</span> <span class="ow">and</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_added_edge_attributes</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
            <span class="c1"># Add edge attribute from main server&#39;s shared memory.</span>
            <span class="k">for</span> <span class="n">name</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_added_edge_attributes</span><span class="p">:</span>
                <span class="n">data</span> <span class="o">=</span> <span class="n">_copy_data_from_shared_mem</span><span class="p">(</span>
                    <span class="s2">&quot;__edge__&quot;</span> <span class="o">+</span> <span class="n">name</span><span class="p">,</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">local_partition</span><span class="o">.</span><span class="n">total_num_edges</span><span class="p">,)</span>
                <span class="p">)</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">local_partition</span><span class="o">.</span><span class="n">add_edge_attribute</span><span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="n">data</span><span class="p">)</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">local_partition</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Return the local partition on the client</span>

<span class="sd">        DistGraph provides a global view of the distributed graph. Internally,</span>
<span class="sd">        it may contains a partition of the graph if it is co-located with</span>
<span class="sd">        the server. When servers and clients run on separate sets of machines,</span>
<span class="sd">        this returns None.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        DGLGraph</span>
<span class="sd">            The local partition</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_g</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">nodes</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Return a node view&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="n">HeteroNodeView</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">edges</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Return an edge view&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="n">HeteroEdgeView</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">ndata</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Return the data view of all the nodes.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        NodeDataView</span>
<span class="sd">            The data view in the distributed graph storage.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">assert</span> <span class="p">(</span>
            <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">ntypes</span><span class="p">)</span> <span class="o">==</span> <span class="mi">1</span>
        <span class="p">),</span> <span class="s2">&quot;ndata only works for a graph with one node type.&quot;</span>
        <span class="k">return</span> <span class="n">NodeDataView</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">edata</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Return the data view of all the edges.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        EdgeDataView</span>
<span class="sd">            The data view in the distributed graph storage.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">assert</span> <span class="p">(</span>
            <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">etypes</span><span class="p">)</span> <span class="o">==</span> <span class="mi">1</span>
        <span class="p">),</span> <span class="s2">&quot;edata only works for a graph with one edge type.&quot;</span>
        <span class="k">return</span> <span class="n">EdgeDataView</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">idtype</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;The dtype of graph index</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        backend dtype object</span>
<span class="sd">            th.int32/th.int64 or tf.int32/tf.int64 etc.</span>

<span class="sd">        See Also</span>
<span class="sd">        --------</span>
<span class="sd">        long</span>
<span class="sd">        int</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># TODO(da?): describe when self._g is None and idtype shouldn&#39;t be called.</span>
        <span class="c1"># For GraphBolt partition, we use the global node ID&#39;s dtype.</span>
        <span class="k">return</span> <span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">get_partition_book</span><span class="p">()</span><span class="o">.</span><span class="n">global_nid_dtype</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_use_graphbolt</span>
            <span class="k">else</span> <span class="n">F</span><span class="o">.</span><span class="n">int64</span>
        <span class="p">)</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">device</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Get the device context of this graph.</span>

<span class="sd">        Examples</span>
<span class="sd">        --------</span>
<span class="sd">        The following example uses PyTorch backend.</span>

<span class="sd">        &gt;&gt;&gt; g = dgl.heterograph({</span>
<span class="sd">        ...     (&#39;user&#39;, &#39;plays&#39;, &#39;game&#39;): ([0, 1, 1, 2], [0, 0, 2, 1])</span>
<span class="sd">        ... })</span>
<span class="sd">        &gt;&gt;&gt; print(g.device)</span>
<span class="sd">        device(type=&#39;cpu&#39;)</span>
<span class="sd">        &gt;&gt;&gt; g = g.to(&#39;cuda:0&#39;)</span>
<span class="sd">        &gt;&gt;&gt; print(g.device)</span>
<span class="sd">        device(type=&#39;cuda&#39;, index=0)</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        Device context object</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># TODO(da?): describe when self._g is None and device shouldn&#39;t be called.</span>
        <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">is_pinned</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Check if the graph structure is pinned to the page-locked memory.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        bool</span>
<span class="sd">            True if the graph structure is pinned.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># (Xin Yao): Currently we don&#39;t support pinning a DistGraph.</span>
        <span class="k">return</span> <span class="kc">False</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">ntypes</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Return the list of node types of this graph.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        list of str</span>

<span class="sd">        Examples</span>
<span class="sd">        --------</span>

<span class="sd">        &gt;&gt;&gt; g = DistGraph(&quot;test&quot;)</span>
<span class="sd">        &gt;&gt;&gt; g.ntypes</span>
<span class="sd">        [&#39;_U&#39;]</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_gpb</span><span class="o">.</span><span class="n">ntypes</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">etypes</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Return the list of edge types of this graph.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        list of str</span>

<span class="sd">        Examples</span>
<span class="sd">        --------</span>

<span class="sd">        &gt;&gt;&gt; g = DistGraph(&quot;test&quot;)</span>
<span class="sd">        &gt;&gt;&gt; g.etypes</span>
<span class="sd">        [&#39;_E&#39;]</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_gpb</span><span class="o">.</span><span class="n">etypes</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">canonical_etypes</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Return all the canonical edge types in the graph.</span>

<span class="sd">        A canonical edge type is a string triplet ``(str, str, str)``</span>
<span class="sd">        for source node type, edge type and destination node type.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        list[(str, str, str)]</span>
<span class="sd">            All the canonical edge type triplets in a list.</span>

<span class="sd">        Notes</span>
<span class="sd">        -----</span>
<span class="sd">        DGL internally assigns an integer ID for each edge type. The returned</span>
<span class="sd">        edge type names are sorted according to their IDs.</span>

<span class="sd">        See Also</span>
<span class="sd">        --------</span>
<span class="sd">        etypes</span>

<span class="sd">        Examples</span>
<span class="sd">        --------</span>
<span class="sd">        The following example uses PyTorch backend.</span>

<span class="sd">        &gt;&gt;&gt; import dgl</span>
<span class="sd">        &gt;&gt;&gt; import torch</span>

<span class="sd">        &gt;&gt;&gt; g = DistGraph(&quot;test&quot;)</span>
<span class="sd">        &gt;&gt;&gt; g.canonical_etypes</span>
<span class="sd">        [(&#39;user&#39;, &#39;follows&#39;, &#39;user&#39;),</span>
<span class="sd">         (&#39;user&#39;, &#39;follows&#39;, &#39;game&#39;),</span>
<span class="sd">         (&#39;user&#39;, &#39;plays&#39;, &#39;game&#39;)]</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_gpb</span><span class="o">.</span><span class="n">canonical_etypes</span>

    <span class="k">def</span> <span class="nf">to_canonical_etype</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">etype</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Convert an edge type to the corresponding canonical edge type in the graph.</span>

<span class="sd">        A canonical edge type is a string triplet ``(str, str, str)``</span>
<span class="sd">        for source node type, edge type and destination node type.</span>

<span class="sd">        The function expects the given edge type name can uniquely identify a canonical edge</span>
<span class="sd">        type. DGL will raise error if this is not the case.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        etype : str or (str, str, str)</span>
<span class="sd">            If :attr:`etype` is an edge type (str), it returns the corresponding canonical edge</span>
<span class="sd">            type in the graph. If :attr:`etype` is already a canonical edge type,</span>
<span class="sd">            it directly returns the input unchanged.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        (str, str, str)</span>
<span class="sd">            The canonical edge type corresponding to the edge type.</span>

<span class="sd">        Examples</span>
<span class="sd">        --------</span>
<span class="sd">        The following example uses PyTorch backend.</span>

<span class="sd">        &gt;&gt;&gt; import dgl</span>
<span class="sd">        &gt;&gt;&gt; import torch</span>

<span class="sd">        &gt;&gt;&gt; g = DistGraph(&quot;test&quot;)</span>
<span class="sd">        &gt;&gt;&gt; g.canonical_etypes</span>
<span class="sd">        [(&#39;user&#39;, &#39;follows&#39;, &#39;user&#39;),</span>
<span class="sd">         (&#39;user&#39;, &#39;follows&#39;, &#39;game&#39;),</span>
<span class="sd">         (&#39;user&#39;, &#39;plays&#39;, &#39;game&#39;)]</span>

<span class="sd">        &gt;&gt;&gt; g.to_canonical_etype(&#39;plays&#39;)</span>
<span class="sd">        (&#39;user&#39;, &#39;plays&#39;, &#39;game&#39;)</span>
<span class="sd">        &gt;&gt;&gt; g.to_canonical_etype((&#39;user&#39;, &#39;plays&#39;, &#39;game&#39;))</span>
<span class="sd">        (&#39;user&#39;, &#39;plays&#39;, &#39;game&#39;)</span>

<span class="sd">        See Also</span>
<span class="sd">        --------</span>
<span class="sd">        canonical_etypes</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_gpb</span><span class="o">.</span><span class="n">to_canonical_etype</span><span class="p">(</span><span class="n">etype</span><span class="p">)</span>

<div class="viewcode-block" id="DistGraph.get_ntype_id">
<a class="viewcode-back" href="../../../api/python/dgl.distributed.html#dgl.distributed.DistGraph.get_ntype_id">[docs]</a>
    <span class="k">def</span> <span class="nf">get_ntype_id</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">ntype</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Return the ID of the given node type.</span>

<span class="sd">        ntype can also be None. If so, there should be only one node type in the</span>
<span class="sd">        graph.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        ntype : str</span>
<span class="sd">            Node type</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        int</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">ntype</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_ntype_map</span><span class="p">)</span> <span class="o">!=</span> <span class="mi">1</span><span class="p">:</span>
                <span class="k">raise</span> <span class="n">DGLError</span><span class="p">(</span>
                    <span class="s2">&quot;Node type name must be specified if there are more than one &quot;</span>
                    <span class="s2">&quot;node types.&quot;</span>
                <span class="p">)</span>
            <span class="k">return</span> <span class="mi">0</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_ntype_map</span><span class="p">[</span><span class="n">ntype</span><span class="p">]</span></div>


<div class="viewcode-block" id="DistGraph.get_etype_id">
<a class="viewcode-back" href="../../../api/python/dgl.distributed.html#dgl.distributed.DistGraph.get_etype_id">[docs]</a>
    <span class="k">def</span> <span class="nf">get_etype_id</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">etype</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Return the id of the given edge type.</span>

<span class="sd">        etype can also be None. If so, there should be only one edge type in the</span>
<span class="sd">        graph.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        etype : str or tuple of str</span>
<span class="sd">            Edge type</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        int</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">etype</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_etype_map</span><span class="p">)</span> <span class="o">!=</span> <span class="mi">1</span><span class="p">:</span>
                <span class="k">raise</span> <span class="n">DGLError</span><span class="p">(</span>
                    <span class="s2">&quot;Edge type name must be specified if there are more than one &quot;</span>
                    <span class="s2">&quot;edge types.&quot;</span>
                <span class="p">)</span>
            <span class="k">return</span> <span class="mi">0</span>
        <span class="n">etype</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">to_canonical_etype</span><span class="p">(</span><span class="n">etype</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_etype_map</span><span class="p">[</span><span class="n">etype</span><span class="p">]</span></div>


<div class="viewcode-block" id="DistGraph.number_of_nodes">
<a class="viewcode-back" href="../../../api/python/dgl.distributed.html#dgl.distributed.DistGraph.number_of_nodes">[docs]</a>
    <span class="k">def</span> <span class="nf">number_of_nodes</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">ntype</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Alias of :func:`num_nodes`&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_nodes</span><span class="p">(</span><span class="n">ntype</span><span class="p">)</span></div>


<div class="viewcode-block" id="DistGraph.number_of_edges">
<a class="viewcode-back" href="../../../api/python/dgl.distributed.html#dgl.distributed.DistGraph.number_of_edges">[docs]</a>
    <span class="k">def</span> <span class="nf">number_of_edges</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">etype</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Alias of :func:`num_edges`&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_edges</span><span class="p">(</span><span class="n">etype</span><span class="p">)</span></div>


<div class="viewcode-block" id="DistGraph.num_nodes">
<a class="viewcode-back" href="../../../api/python/dgl.distributed.html#dgl.distributed.DistGraph.num_nodes">[docs]</a>
    <span class="k">def</span> <span class="nf">num_nodes</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">ntype</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Return the total number of nodes in the distributed graph.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        ntype : str, optional</span>
<span class="sd">            The node type name. If given, it returns the number of nodes of the</span>
<span class="sd">            type. If not given (default), it returns the total number of nodes of all types.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        int</span>
<span class="sd">            The number of nodes</span>

<span class="sd">        Examples</span>
<span class="sd">        --------</span>
<span class="sd">        &gt;&gt;&gt; g = dgl.distributed.DistGraph(&#39;ogb-product&#39;)</span>
<span class="sd">        &gt;&gt;&gt; print(g.num_nodes())</span>
<span class="sd">        2449029</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">ntype</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">ntypes</span><span class="p">)</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
                <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_gpb</span><span class="o">.</span><span class="n">_num_nodes</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">ntypes</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="k">return</span> <span class="nb">sum</span><span class="p">(</span>
                    <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">_gpb</span><span class="o">.</span><span class="n">_num_nodes</span><span class="p">(</span><span class="n">ntype</span><span class="p">)</span> <span class="k">for</span> <span class="n">ntype</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">ntypes</span><span class="p">]</span>
                <span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_gpb</span><span class="o">.</span><span class="n">_num_nodes</span><span class="p">(</span><span class="n">ntype</span><span class="p">)</span></div>


<div class="viewcode-block" id="DistGraph.num_edges">
<a class="viewcode-back" href="../../../api/python/dgl.distributed.html#dgl.distributed.DistGraph.num_edges">[docs]</a>
    <span class="k">def</span> <span class="nf">num_edges</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">etype</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Return the total number of edges in the distributed graph.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        etype : str or (str, str, str), optional</span>
<span class="sd">            The type name of the edges. The allowed type name formats are:</span>

<span class="sd">            * ``(str, str, str)`` for source node type, edge type and destination node type.</span>
<span class="sd">            * or one ``str`` edge type name if the name can uniquely identify a</span>
<span class="sd">              triplet format in the graph.</span>

<span class="sd">            If not provided, return the total number of edges regardless of the types</span>
<span class="sd">            in the graph.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        int</span>
<span class="sd">            The number of edges</span>

<span class="sd">        Examples</span>
<span class="sd">        --------</span>
<span class="sd">        &gt;&gt;&gt; g = dgl.distributed.DistGraph(&#39;ogb-product&#39;)</span>
<span class="sd">        &gt;&gt;&gt; print(g.num_edges())</span>
<span class="sd">        123718280</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">etype</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">return</span> <span class="nb">sum</span><span class="p">(</span>
                <span class="p">[</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">_gpb</span><span class="o">.</span><span class="n">_num_edges</span><span class="p">(</span><span class="n">c_etype</span><span class="p">)</span>
                    <span class="k">for</span> <span class="n">c_etype</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">canonical_etypes</span>
                <span class="p">]</span>
            <span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_gpb</span><span class="o">.</span><span class="n">_num_edges</span><span class="p">(</span><span class="n">etype</span><span class="p">)</span></div>


<div class="viewcode-block" id="DistGraph.out_degrees">
<a class="viewcode-back" href="../../../api/python/dgl.distributed.html#dgl.distributed.DistGraph.out_degrees">[docs]</a>
    <span class="k">def</span> <span class="nf">out_degrees</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">u</span><span class="o">=</span><span class="n">ALL</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Return the out-degree(s) of the given nodes.</span>

<span class="sd">        It computes the out-degree(s).</span>
<span class="sd">        It does not support heterogeneous graphs yet.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        u : node IDs</span>
<span class="sd">            The node IDs. The allowed formats are:</span>

<span class="sd">            * ``int``: A single node.</span>
<span class="sd">            * Int Tensor: Each element is a node ID. The tensor must have the same device type</span>
<span class="sd">              and ID data type as the graph&#39;s.</span>
<span class="sd">            * iterable[int]: Each element is a node ID.</span>

<span class="sd">            If not given, return the in-degrees of all the nodes.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        int or Tensor</span>
<span class="sd">            The out-degree(s) of the node(s) in a Tensor. The i-th element is the out-degree</span>
<span class="sd">            of the i-th input node. If :attr:`v` is an ``int``, return an ``int`` too.</span>

<span class="sd">        Examples</span>
<span class="sd">        --------</span>
<span class="sd">        The following example uses PyTorch backend.</span>

<span class="sd">        &gt;&gt;&gt; import dgl</span>
<span class="sd">        &gt;&gt;&gt; import torch</span>

<span class="sd">        Query for all nodes.</span>

<span class="sd">        &gt;&gt;&gt; g.out_degrees()</span>
<span class="sd">        tensor([2, 2, 0, 0])</span>

<span class="sd">        Query for nodes 1 and 2.</span>

<span class="sd">        &gt;&gt;&gt; g.out_degrees(torch.tensor([1, 2]))</span>
<span class="sd">        tensor([2, 0])</span>

<span class="sd">        See Also</span>
<span class="sd">        --------</span>
<span class="sd">        in_degrees</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">is_all</span><span class="p">(</span><span class="n">u</span><span class="p">):</span>
            <span class="n">u</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_nodes</span><span class="p">())</span>
        <span class="k">return</span> <span class="n">dist_out_degrees</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">u</span><span class="p">)</span></div>


<div class="viewcode-block" id="DistGraph.in_degrees">
<a class="viewcode-back" href="../../../api/python/dgl.distributed.html#dgl.distributed.DistGraph.in_degrees">[docs]</a>
    <span class="k">def</span> <span class="nf">in_degrees</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">v</span><span class="o">=</span><span class="n">ALL</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Return the in-degree(s) of the given nodes.</span>

<span class="sd">        It computes the in-degree(s).</span>
<span class="sd">        It does not support heterogeneous graphs yet.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        v : node IDs</span>
<span class="sd">            The node IDs. The allowed formats are:</span>

<span class="sd">            * ``int``: A single node.</span>
<span class="sd">            * Int Tensor: Each element is a node ID. The tensor must have the same device type</span>
<span class="sd">              and ID data type as the graph&#39;s.</span>
<span class="sd">            * iterable[int]: Each element is a node ID.</span>

<span class="sd">            If not given, return the in-degrees of all the nodes.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        int or Tensor</span>
<span class="sd">            The in-degree(s) of the node(s) in a Tensor. The i-th element is the in-degree</span>
<span class="sd">            of the i-th input node. If :attr:`v` is an ``int``, return an ``int`` too.</span>

<span class="sd">        Examples</span>
<span class="sd">        --------</span>
<span class="sd">        The following example uses PyTorch backend.</span>

<span class="sd">        &gt;&gt;&gt; import dgl</span>
<span class="sd">        &gt;&gt;&gt; import torch</span>

<span class="sd">        Query for all nodes.</span>

<span class="sd">        &gt;&gt;&gt; g.in_degrees()</span>
<span class="sd">        tensor([0, 2, 1, 1])</span>

<span class="sd">        Query for nodes 1 and 2.</span>

<span class="sd">        &gt;&gt;&gt; g.in_degrees(torch.tensor([1, 2]))</span>
<span class="sd">        tensor([2, 1])</span>

<span class="sd">        See Also</span>
<span class="sd">        --------</span>
<span class="sd">        out_degrees</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">is_all</span><span class="p">(</span><span class="n">v</span><span class="p">):</span>
            <span class="n">v</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_nodes</span><span class="p">())</span>
        <span class="k">return</span> <span class="n">dist_in_degrees</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">v</span><span class="p">)</span></div>


<div class="viewcode-block" id="DistGraph.node_attr_schemes">
<a class="viewcode-back" href="../../../api/python/dgl.distributed.html#dgl.distributed.DistGraph.node_attr_schemes">[docs]</a>
    <span class="k">def</span> <span class="nf">node_attr_schemes</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Return the node feature schemes.</span>

<span class="sd">        Each feature scheme is a named tuple that stores the shape and data type</span>
<span class="sd">        of the node feature.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        dict of str to schemes</span>
<span class="sd">            The schemes of node feature columns.</span>

<span class="sd">        Examples</span>
<span class="sd">        --------</span>
<span class="sd">        The following uses PyTorch backend.</span>

<span class="sd">        &gt;&gt;&gt; g.node_attr_schemes()</span>
<span class="sd">        {&#39;h&#39;: Scheme(shape=(4,), dtype=torch.float32)}</span>

<span class="sd">        See Also</span>
<span class="sd">        --------</span>
<span class="sd">        edge_attr_schemes</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">schemes</span> <span class="o">=</span> <span class="p">{}</span>
        <span class="k">for</span> <span class="n">key</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">ndata</span><span class="p">:</span>
            <span class="n">schemes</span><span class="p">[</span><span class="n">key</span><span class="p">]</span> <span class="o">=</span> <span class="n">infer_scheme</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">ndata</span><span class="p">[</span><span class="n">key</span><span class="p">])</span>
        <span class="k">return</span> <span class="n">schemes</span></div>


<div class="viewcode-block" id="DistGraph.edge_attr_schemes">
<a class="viewcode-back" href="../../../api/python/dgl.distributed.html#dgl.distributed.DistGraph.edge_attr_schemes">[docs]</a>
    <span class="k">def</span> <span class="nf">edge_attr_schemes</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Return the edge feature schemes.</span>

<span class="sd">        Each feature scheme is a named tuple that stores the shape and data type</span>
<span class="sd">        of the edge feature.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        dict of str to schemes</span>
<span class="sd">            The schemes of edge feature columns.</span>

<span class="sd">        Examples</span>
<span class="sd">        --------</span>
<span class="sd">        The following uses PyTorch backend.</span>

<span class="sd">        &gt;&gt;&gt; g.edge_attr_schemes()</span>
<span class="sd">        {&#39;h&#39;: Scheme(shape=(4,), dtype=torch.float32)}</span>

<span class="sd">        See Also</span>
<span class="sd">        --------</span>
<span class="sd">        node_attr_schemes</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">schemes</span> <span class="o">=</span> <span class="p">{}</span>
        <span class="k">for</span> <span class="n">key</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">edata</span><span class="p">:</span>
            <span class="n">schemes</span><span class="p">[</span><span class="n">key</span><span class="p">]</span> <span class="o">=</span> <span class="n">infer_scheme</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">edata</span><span class="p">[</span><span class="n">key</span><span class="p">])</span>
        <span class="k">return</span> <span class="n">schemes</span></div>


<div class="viewcode-block" id="DistGraph.rank">
<a class="viewcode-back" href="../../../api/python/dgl.distributed.html#dgl.distributed.DistGraph.rank">[docs]</a>
    <span class="k">def</span> <span class="nf">rank</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;The rank of the current DistGraph.</span>

<span class="sd">        This returns a unique number to identify the DistGraph object among all of</span>
<span class="sd">        the client processes.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        int</span>
<span class="sd">            The rank of the current DistGraph.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="n">role</span><span class="o">.</span><span class="n">get_global_rank</span><span class="p">()</span></div>


<div class="viewcode-block" id="DistGraph.find_edges">
<a class="viewcode-back" href="../../../api/python/dgl.distributed.html#dgl.distributed.DistGraph.find_edges">[docs]</a>
    <span class="k">def</span> <span class="nf">find_edges</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">edges</span><span class="p">,</span> <span class="n">etype</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Given an edge ID array, return the source</span>
<span class="sd">        and destination node ID array ``s`` and ``d``.  ``s[i]`` and ``d[i]``</span>
<span class="sd">        are source and destination node ID for edge ``eid[i]``.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        edges : Int Tensor</span>
<span class="sd">            Each element is an ID. The tensor must have the same device type</span>
<span class="sd">              and ID data type as the graph&#39;s.</span>

<span class="sd">        etype : str or (str, str, str), optional</span>
<span class="sd">            The type names of the edges. The allowed type name formats are:</span>

<span class="sd">            * ``(str, str, str)`` for source node type, edge type and destination node type.</span>
<span class="sd">            * or one ``str`` edge type name if the name can uniquely identify a</span>
<span class="sd">              triplet format in the graph.</span>

<span class="sd">            Can be omitted if the graph has only one type of edges.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        tensor</span>
<span class="sd">            The source node ID array.</span>
<span class="sd">        tensor</span>
<span class="sd">            The destination node ID array.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">etype</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">assert</span> <span class="p">(</span>
                <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">etypes</span><span class="p">)</span> <span class="o">==</span> <span class="mi">1</span>
            <span class="p">),</span> <span class="s2">&quot;find_edges requires etype for heterogeneous graphs.&quot;</span>

        <span class="n">gpb</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_partition_book</span><span class="p">()</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">gpb</span><span class="o">.</span><span class="n">etypes</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
            <span class="n">edges</span> <span class="o">=</span> <span class="n">gpb</span><span class="o">.</span><span class="n">map_to_homo_eid</span><span class="p">(</span><span class="n">edges</span><span class="p">,</span> <span class="n">etype</span><span class="p">)</span>
        <span class="n">src</span><span class="p">,</span> <span class="n">dst</span> <span class="o">=</span> <span class="n">dist_find_edges</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">edges</span><span class="p">)</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">gpb</span><span class="o">.</span><span class="n">ntypes</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
            <span class="n">_</span><span class="p">,</span> <span class="n">src</span> <span class="o">=</span> <span class="n">gpb</span><span class="o">.</span><span class="n">map_to_per_ntype</span><span class="p">(</span><span class="n">src</span><span class="p">)</span>
            <span class="n">_</span><span class="p">,</span> <span class="n">dst</span> <span class="o">=</span> <span class="n">gpb</span><span class="o">.</span><span class="n">map_to_per_ntype</span><span class="p">(</span><span class="n">dst</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">src</span><span class="p">,</span> <span class="n">dst</span></div>


    <span class="k">def</span> <span class="nf">edge_subgraph</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">edges</span><span class="p">,</span> <span class="n">relabel_nodes</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">store_ids</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Return a subgraph induced on the given edges.</span>

<span class="sd">        An edge-induced subgraph is equivalent to creating a new graph using the given</span>
<span class="sd">        edges. In addition to extracting the subgraph, DGL also copies the features</span>
<span class="sd">        of the extracted nodes and edges to the resulting graph. The copy is *lazy*</span>
<span class="sd">        and incurs data movement only when needed.</span>

<span class="sd">        If the graph is heterogeneous, DGL extracts a subgraph per relation and composes</span>
<span class="sd">        them as the resulting graph. Thus, the resulting graph has the same set of relations</span>
<span class="sd">        as the input one.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        edges : Int Tensor or dict[(str, str, str), Int Tensor]</span>
<span class="sd">            The edges to form the subgraph. Each element is an edge ID. The tensor must have</span>
<span class="sd">            the same device type and ID data type as the graph&#39;s.</span>

<span class="sd">            If the graph is homogeneous, one can directly pass an Int Tensor.</span>
<span class="sd">            Otherwise, the argument must be a dictionary with keys being edge types</span>
<span class="sd">            and values being the edge IDs in the above formats.</span>
<span class="sd">        relabel_nodes : bool, optional</span>
<span class="sd">            If True, it will remove the isolated nodes and relabel the incident nodes in the</span>
<span class="sd">            extracted subgraph.</span>
<span class="sd">        store_ids : bool, optional</span>
<span class="sd">            If True, it will store the raw IDs of the extracted edges in the ``edata`` of the</span>
<span class="sd">            resulting graph under name ``dgl.EID``; if ``relabel_nodes`` is ``True``, it will</span>
<span class="sd">            also store the raw IDs of the incident nodes in the ``ndata`` of the resulting</span>
<span class="sd">            graph under name ``dgl.NID``.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        G : DGLGraph</span>
<span class="sd">            The subgraph.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">edges</span><span class="p">,</span> <span class="nb">dict</span><span class="p">):</span>
            <span class="c1"># TODO(zhengda) we need to directly generate subgraph of all relations with</span>
            <span class="c1"># one invocation.</span>
            <span class="n">subg</span> <span class="o">=</span> <span class="p">{}</span>
            <span class="k">for</span> <span class="n">etype</span><span class="p">,</span> <span class="n">edge</span> <span class="ow">in</span> <span class="n">edges</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
                <span class="n">etype</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">to_canonical_etype</span><span class="p">(</span><span class="n">etype</span><span class="p">)</span>
                <span class="n">subg</span><span class="p">[</span><span class="n">etype</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">find_edges</span><span class="p">(</span><span class="n">edge</span><span class="p">,</span> <span class="n">etype</span><span class="p">)</span>
            <span class="n">num_nodes</span> <span class="o">=</span> <span class="p">{</span><span class="n">ntype</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_nodes</span><span class="p">(</span><span class="n">ntype</span><span class="p">)</span> <span class="k">for</span> <span class="n">ntype</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">ntypes</span><span class="p">}</span>
            <span class="n">subg</span> <span class="o">=</span> <span class="n">dgl_heterograph</span><span class="p">(</span><span class="n">subg</span><span class="p">,</span> <span class="n">num_nodes_dict</span><span class="o">=</span><span class="n">num_nodes</span><span class="p">)</span>
            <span class="k">for</span> <span class="n">etype</span> <span class="ow">in</span> <span class="n">edges</span><span class="p">:</span>
                <span class="n">subg</span><span class="o">.</span><span class="n">edges</span><span class="p">[</span><span class="n">etype</span><span class="p">]</span><span class="o">.</span><span class="n">data</span><span class="p">[</span><span class="n">EID</span><span class="p">]</span> <span class="o">=</span> <span class="n">edges</span><span class="p">[</span><span class="n">etype</span><span class="p">]</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">etypes</span><span class="p">)</span> <span class="o">==</span> <span class="mi">1</span>
            <span class="n">subg</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">find_edges</span><span class="p">(</span><span class="n">edges</span><span class="p">)</span>
            <span class="n">subg</span> <span class="o">=</span> <span class="n">dgl_graph</span><span class="p">(</span><span class="n">subg</span><span class="p">,</span> <span class="n">num_nodes</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">num_nodes</span><span class="p">())</span>
            <span class="n">subg</span><span class="o">.</span><span class="n">edata</span><span class="p">[</span><span class="n">EID</span><span class="p">]</span> <span class="o">=</span> <span class="n">edges</span>

        <span class="k">if</span> <span class="n">relabel_nodes</span><span class="p">:</span>
            <span class="n">subg</span> <span class="o">=</span> <span class="n">compact_graphs</span><span class="p">(</span><span class="n">subg</span><span class="p">)</span>
        <span class="k">assert</span> <span class="n">store_ids</span><span class="p">,</span> <span class="s2">&quot;edge_subgraph always stores original node/edge IDs.&quot;</span>
        <span class="k">return</span> <span class="n">subg</span>

<div class="viewcode-block" id="DistGraph.get_partition_book">
<a class="viewcode-back" href="../../../api/python/dgl.distributed.html#dgl.distributed.DistGraph.get_partition_book">[docs]</a>
    <span class="k">def</span> <span class="nf">get_partition_book</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Get the partition information.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        GraphPartitionBook</span>
<span class="sd">            Object that stores all graph partition information.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_gpb</span></div>


<div class="viewcode-block" id="DistGraph.get_node_partition_policy">
<a class="viewcode-back" href="../../../api/python/dgl.distributed.html#dgl.distributed.DistGraph.get_node_partition_policy">[docs]</a>
    <span class="k">def</span> <span class="nf">get_node_partition_policy</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">ntype</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Get the partition policy for a node type.</span>

<span class="sd">        When creating a new distributed tensor, we need to provide a partition policy</span>
<span class="sd">        that indicates how to distribute data of the distributed tensor in a cluster</span>
<span class="sd">        of machines. When we load a distributed graph in the cluster, we have pre-defined</span>
<span class="sd">        partition policies for each node type and each edge type. By providing</span>
<span class="sd">        the node type, we can reference to the pre-defined partition policy for the node type.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        ntype : str</span>
<span class="sd">            The node type</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        PartitionPolicy</span>
<span class="sd">            The partition policy for the node type.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="n">NodePartitionPolicy</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">get_partition_book</span><span class="p">(),</span> <span class="n">ntype</span><span class="p">)</span></div>


<div class="viewcode-block" id="DistGraph.get_edge_partition_policy">
<a class="viewcode-back" href="../../../api/python/dgl.distributed.html#dgl.distributed.DistGraph.get_edge_partition_policy">[docs]</a>
    <span class="k">def</span> <span class="nf">get_edge_partition_policy</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">etype</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Get the partition policy for an edge type.</span>

<span class="sd">        When creating a new distributed tensor, we need to provide a partition policy</span>
<span class="sd">        that indicates how to distribute data of the distributed tensor in a cluster</span>
<span class="sd">        of machines. When we load a distributed graph in the cluster, we have pre-defined</span>
<span class="sd">        partition policies for each node type and each edge type. By providing</span>
<span class="sd">        the edge type, we can reference to the pre-defined partition policy for the edge type.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        etype : str or (str, str, str)</span>
<span class="sd">            The edge type</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        PartitionPolicy</span>
<span class="sd">            The partition policy for the edge type.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">etype</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">to_canonical_etype</span><span class="p">(</span><span class="n">etype</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">EdgePartitionPolicy</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">get_partition_book</span><span class="p">(),</span> <span class="n">etype</span><span class="p">)</span></div>


<div class="viewcode-block" id="DistGraph.barrier">
<a class="viewcode-back" href="../../../api/python/dgl.distributed.html#dgl.distributed.DistGraph.barrier">[docs]</a>
    <span class="k">def</span> <span class="nf">barrier</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Barrier for all client nodes.</span>

<span class="sd">        This API blocks the current process untill all the clients invoke this API.</span>
<span class="sd">        Please use this API with caution.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_client</span><span class="o">.</span><span class="n">barrier</span><span class="p">()</span></div>


    <span class="k">def</span> <span class="nf">sample_neighbors</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">seed_nodes</span><span class="p">,</span>
        <span class="n">fanout</span><span class="p">,</span>
        <span class="n">edge_dir</span><span class="o">=</span><span class="s2">&quot;in&quot;</span><span class="p">,</span>
        <span class="n">prob</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">exclude_edges</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">replace</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
        <span class="n">etype_sorted</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
        <span class="n">output_device</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="p">):</span>
        <span class="c1"># pylint: disable=unused-argument</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Sample neighbors from a distributed graph.&quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">exclude_edges</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="c1"># Convert exclude edge IDs to homogeneous edge IDs.</span>
            <span class="n">gpb</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_partition_book</span><span class="p">()</span>
            <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">exclude_edges</span><span class="p">,</span> <span class="n">Mapping</span><span class="p">):</span>
                <span class="n">exclude_eids</span> <span class="o">=</span> <span class="p">[]</span>
                <span class="k">for</span> <span class="n">c_etype</span><span class="p">,</span> <span class="n">eids</span> <span class="ow">in</span> <span class="n">exclude_edges</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
                    <span class="n">exclude_eids</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">gpb</span><span class="o">.</span><span class="n">map_to_homo_eid</span><span class="p">(</span><span class="n">eids</span><span class="p">,</span> <span class="n">c_etype</span><span class="p">))</span>
                <span class="n">exclude_edges</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">(</span><span class="n">exclude_eids</span><span class="p">)</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">etypes</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
            <span class="n">frontier</span> <span class="o">=</span> <span class="n">graph_services</span><span class="o">.</span><span class="n">sample_etype_neighbors</span><span class="p">(</span>
                <span class="bp">self</span><span class="p">,</span>
                <span class="n">seed_nodes</span><span class="p">,</span>
                <span class="n">fanout</span><span class="p">,</span>
                <span class="n">replace</span><span class="o">=</span><span class="n">replace</span><span class="p">,</span>
                <span class="n">etype_sorted</span><span class="o">=</span><span class="n">etype_sorted</span><span class="p">,</span>
                <span class="n">prob</span><span class="o">=</span><span class="n">prob</span><span class="p">,</span>
                <span class="n">exclude_edges</span><span class="o">=</span><span class="n">exclude_edges</span><span class="p">,</span>
                <span class="n">use_graphbolt</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_use_graphbolt</span><span class="p">,</span>
            <span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">frontier</span> <span class="o">=</span> <span class="n">graph_services</span><span class="o">.</span><span class="n">sample_neighbors</span><span class="p">(</span>
                <span class="bp">self</span><span class="p">,</span>
                <span class="n">seed_nodes</span><span class="p">,</span>
                <span class="n">fanout</span><span class="p">,</span>
                <span class="n">replace</span><span class="o">=</span><span class="n">replace</span><span class="p">,</span>
                <span class="n">prob</span><span class="o">=</span><span class="n">prob</span><span class="p">,</span>
                <span class="n">exclude_edges</span><span class="o">=</span><span class="n">exclude_edges</span><span class="p">,</span>
                <span class="n">use_graphbolt</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_use_graphbolt</span><span class="p">,</span>
            <span class="p">)</span>
        <span class="k">return</span> <span class="n">frontier</span>

    <span class="k">def</span> <span class="nf">_get_ndata_names</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">ntype</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Get the names of all node data.&quot;&quot;&quot;</span>
        <span class="n">names</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_client</span><span class="o">.</span><span class="n">gdata_name_list</span><span class="p">()</span>
        <span class="n">ndata_names</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">name</span> <span class="ow">in</span> <span class="n">names</span><span class="p">:</span>
            <span class="n">name</span> <span class="o">=</span> <span class="n">parse_hetero_data_name</span><span class="p">(</span><span class="n">name</span><span class="p">)</span>
            <span class="n">right_type</span> <span class="o">=</span> <span class="p">(</span>
                <span class="p">(</span><span class="n">name</span><span class="o">.</span><span class="n">get_type</span><span class="p">()</span> <span class="o">==</span> <span class="n">ntype</span><span class="p">)</span> <span class="k">if</span> <span class="n">ntype</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="kc">True</span>
            <span class="p">)</span>
            <span class="k">if</span> <span class="n">name</span><span class="o">.</span><span class="n">is_node</span><span class="p">()</span> <span class="ow">and</span> <span class="n">right_type</span><span class="p">:</span>
                <span class="n">ndata_names</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">name</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">ndata_names</span>

    <span class="k">def</span> <span class="nf">_get_edata_names</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">etype</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Get the names of all edge data.&quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">etype</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">etype</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">to_canonical_etype</span><span class="p">(</span><span class="n">etype</span><span class="p">)</span>
        <span class="n">names</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_client</span><span class="o">.</span><span class="n">gdata_name_list</span><span class="p">()</span>
        <span class="n">edata_names</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">name</span> <span class="ow">in</span> <span class="n">names</span><span class="p">:</span>
            <span class="n">name</span> <span class="o">=</span> <span class="n">parse_hetero_data_name</span><span class="p">(</span><span class="n">name</span><span class="p">)</span>
            <span class="n">right_type</span> <span class="o">=</span> <span class="p">(</span>
                <span class="p">(</span><span class="n">name</span><span class="o">.</span><span class="n">get_type</span><span class="p">()</span> <span class="o">==</span> <span class="n">etype</span><span class="p">)</span> <span class="k">if</span> <span class="n">etype</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="kc">True</span>
            <span class="p">)</span>
            <span class="k">if</span> <span class="n">name</span><span class="o">.</span><span class="n">is_edge</span><span class="p">()</span> <span class="ow">and</span> <span class="n">right_type</span><span class="p">:</span>
                <span class="n">edata_names</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">name</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">edata_names</span>

    <span class="k">def</span> <span class="nf">add_edge_attribute</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">name</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Add an edge attribute into GraphBolt partition from edge data.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        name : str</span>
<span class="sd">            The name of the edge attribute.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># Sanity checks.</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">_use_graphbolt</span><span class="p">:</span>
            <span class="k">raise</span> <span class="n">DGLError</span><span class="p">(</span><span class="s2">&quot;GraphBolt is not used.&quot;</span><span class="p">)</span>

        <span class="c1"># Send add request to main server on the same machine.</span>
        <span class="n">kv_names</span> <span class="o">=</span> <span class="p">[</span>
            <span class="p">(</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">edges</span><span class="p">[</span><span class="n">etype</span><span class="p">]</span><span class="o">.</span><span class="n">data</span><span class="p">[</span><span class="n">name</span><span class="p">]</span><span class="o">.</span><span class="n">kvstore_key</span>
                <span class="k">if</span> <span class="n">name</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">edges</span><span class="p">[</span><span class="n">etype</span><span class="p">]</span><span class="o">.</span><span class="n">data</span>
                <span class="k">else</span> <span class="kc">None</span>
            <span class="p">)</span>
            <span class="k">for</span> <span class="n">etype</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">canonical_etypes</span>
        <span class="p">]</span>
        <span class="n">rpc</span><span class="o">.</span><span class="n">send_request</span><span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_client</span><span class="o">.</span><span class="n">_main_server_id</span><span class="p">,</span>
            <span class="n">AddEdgeAttributeFromKVRequest</span><span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="n">kv_names</span><span class="p">),</span>
        <span class="p">)</span>
        <span class="c1"># Wait for the response.</span>
        <span class="k">assert</span> <span class="n">rpc</span><span class="o">.</span><span class="n">recv_response</span><span class="p">()</span><span class="o">.</span><span class="n">_name</span> <span class="o">==</span> <span class="n">name</span>
        <span class="c1"># Send add request to local backup servers.</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_client</span><span class="o">.</span><span class="n">group_count</span> <span class="o">-</span> <span class="mi">1</span><span class="p">):</span>
            <span class="n">server_id</span> <span class="o">=</span> <span class="p">(</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">_client</span><span class="o">.</span><span class="n">machine_id</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">_client</span><span class="o">.</span><span class="n">group_count</span> <span class="o">+</span> <span class="n">i</span> <span class="o">+</span> <span class="mi">1</span>
            <span class="p">)</span>
            <span class="n">rpc</span><span class="o">.</span><span class="n">send_request</span><span class="p">(</span>
                <span class="n">server_id</span><span class="p">,</span> <span class="n">AddEdgeAttributeFromSharedMemRequest</span><span class="p">(</span><span class="n">name</span><span class="p">)</span>
            <span class="p">)</span>
        <span class="c1"># Receive response from local backup servers.</span>
        <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_client</span><span class="o">.</span><span class="n">group_count</span> <span class="o">-</span> <span class="mi">1</span><span class="p">):</span>
            <span class="n">response</span> <span class="o">=</span> <span class="n">rpc</span><span class="o">.</span><span class="n">recv_response</span><span class="p">()</span>
            <span class="k">assert</span> <span class="n">response</span><span class="o">.</span><span class="n">_name</span> <span class="o">==</span> <span class="n">name</span>
        <span class="c1"># Add edge attribute from main server&#39;s shared memory.</span>
        <span class="n">data</span> <span class="o">=</span> <span class="n">_copy_data_from_shared_mem</span><span class="p">(</span>
            <span class="s2">&quot;__edge__&quot;</span> <span class="o">+</span> <span class="n">name</span><span class="p">,</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">local_partition</span><span class="o">.</span><span class="n">total_num_edges</span><span class="p">,)</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">local_partition</span><span class="o">.</span><span class="n">add_edge_attribute</span><span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="n">data</span><span class="p">)</span>
        <span class="c1"># Sync local clients.</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_client</span><span class="o">.</span><span class="n">barrier</span><span class="p">()</span>

        <span class="c1"># Save the edge attribute into state. This is required by separate samplers.</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_added_edge_attributes</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">name</span><span class="p">)</span></div>



<span class="k">def</span> <span class="nf">_get_overlap</span><span class="p">(</span><span class="n">mask_arr</span><span class="p">,</span> <span class="n">ids</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Select the IDs given a boolean mask array.</span>

<span class="sd">    The boolean mask array indicates all of the IDs to be selected. We want to</span>
<span class="sd">    find the overlap between the IDs selected by the boolean mask array and</span>
<span class="sd">    the ID array.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    mask_arr : 1D tensor</span>
<span class="sd">        A boolean mask array.</span>
<span class="sd">    ids : 1D tensor</span>
<span class="sd">        A vector with IDs.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    1D tensor</span>
<span class="sd">        The selected IDs.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">mask_arr</span><span class="p">,</span> <span class="n">DistTensor</span><span class="p">):</span>
        <span class="n">masks</span> <span class="o">=</span> <span class="n">mask_arr</span><span class="p">[</span><span class="n">ids</span><span class="p">]</span>
        <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">boolean_mask</span><span class="p">(</span><span class="n">ids</span><span class="p">,</span> <span class="n">masks</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">masks</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">gather_row</span><span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">mask_arr</span><span class="p">),</span> <span class="n">ids</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">boolean_mask</span><span class="p">(</span><span class="n">ids</span><span class="p">,</span> <span class="n">masks</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">_split_local</span><span class="p">(</span><span class="n">partition_book</span><span class="p">,</span> <span class="n">rank</span><span class="p">,</span> <span class="n">elements</span><span class="p">,</span> <span class="n">local_eles</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Split the input element list with respect to data locality.&quot;&quot;&quot;</span>
    <span class="n">num_clients</span> <span class="o">=</span> <span class="n">role</span><span class="o">.</span><span class="n">get_num_trainers</span><span class="p">()</span>
    <span class="n">num_client_per_part</span> <span class="o">=</span> <span class="n">num_clients</span> <span class="o">//</span> <span class="n">partition_book</span><span class="o">.</span><span class="n">num_partitions</span><span class="p">()</span>
    <span class="k">if</span> <span class="n">rank</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">rank</span> <span class="o">=</span> <span class="n">role</span><span class="o">.</span><span class="n">get_trainer_rank</span><span class="p">()</span>
    <span class="k">assert</span> <span class="p">(</span>
        <span class="n">rank</span> <span class="o">&lt;</span> <span class="n">num_clients</span>
    <span class="p">),</span> <span class="s2">&quot;The input rank (</span><span class="si">{}</span><span class="s2">) is incorrect. #Trainers: </span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span>
        <span class="n">rank</span><span class="p">,</span> <span class="n">num_clients</span>
    <span class="p">)</span>
    <span class="c1"># all ranks of the clients in the same machine are in a contiguous range.</span>
    <span class="n">client_id_in_part</span> <span class="o">=</span> <span class="n">rank</span> <span class="o">%</span> <span class="n">num_client_per_part</span>
    <span class="n">local_eles</span> <span class="o">=</span> <span class="n">_get_overlap</span><span class="p">(</span><span class="n">elements</span><span class="p">,</span> <span class="n">local_eles</span><span class="p">)</span>

    <span class="c1"># get a subset for the local client.</span>
    <span class="n">size</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">local_eles</span><span class="p">)</span> <span class="o">//</span> <span class="n">num_client_per_part</span>
    <span class="c1"># if this isn&#39;t the last client in the partition.</span>
    <span class="k">if</span> <span class="n">client_id_in_part</span> <span class="o">+</span> <span class="mi">1</span> <span class="o">&lt;</span> <span class="n">num_client_per_part</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">local_eles</span><span class="p">[</span>
            <span class="p">(</span><span class="n">size</span> <span class="o">*</span> <span class="n">client_id_in_part</span><span class="p">)</span> <span class="p">:</span> <span class="p">(</span><span class="n">size</span> <span class="o">*</span> <span class="p">(</span><span class="n">client_id_in_part</span> <span class="o">+</span> <span class="mi">1</span><span class="p">))</span>
        <span class="p">]</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">local_eles</span><span class="p">[(</span><span class="n">size</span> <span class="o">*</span> <span class="n">client_id_in_part</span><span class="p">)</span> <span class="p">:]</span>


<span class="k">def</span> <span class="nf">_even_offset</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">k</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Split an array of length n into k segments and the difference of thier length is</span>
<span class="sd">    at most 1. Return the offset of each segment.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">eles_per_part</span> <span class="o">=</span> <span class="n">n</span> <span class="o">//</span> <span class="n">k</span>
    <span class="n">offset</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">0</span><span class="p">]</span> <span class="o">+</span> <span class="p">[</span><span class="n">eles_per_part</span><span class="p">]</span> <span class="o">*</span> <span class="n">k</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="nb">int</span><span class="p">)</span>
    <span class="n">offset</span><span class="p">[</span><span class="mi">1</span> <span class="p">:</span> <span class="n">n</span> <span class="o">-</span> <span class="n">eles_per_part</span> <span class="o">*</span> <span class="n">k</span> <span class="o">+</span> <span class="mi">1</span><span class="p">]</span> <span class="o">+=</span> <span class="mi">1</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">cumsum</span><span class="p">(</span><span class="n">offset</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">_split_even_to_part</span><span class="p">(</span><span class="n">partition_book</span><span class="p">,</span> <span class="n">elements</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Split the input element list evenly.&quot;&quot;&quot;</span>
    <span class="c1"># here we divide the element list as evenly as possible. If we use range partitioning,</span>
    <span class="c1"># the split results also respect the data locality. Range partitioning is the default</span>
    <span class="c1"># strategy.</span>
    <span class="c1"># TODO(zhengda) we need another way to divide the list for other partitioning strategy.</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">elements</span><span class="p">,</span> <span class="n">DistTensor</span><span class="p">):</span>
        <span class="n">nonzero_count</span> <span class="o">=</span> <span class="n">elements</span><span class="o">.</span><span class="n">count_nonzero</span><span class="p">()</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">elements</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">elements</span><span class="p">)</span>
        <span class="n">nonzero_count</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">count_nonzero</span><span class="p">(</span><span class="n">elements</span><span class="p">)</span>
    <span class="c1"># compute the offset of each split and ensure that the difference of each partition size</span>
    <span class="c1"># is 1.</span>
    <span class="n">offsets</span> <span class="o">=</span> <span class="n">_even_offset</span><span class="p">(</span><span class="n">nonzero_count</span><span class="p">,</span> <span class="n">partition_book</span><span class="o">.</span><span class="n">num_partitions</span><span class="p">())</span>
    <span class="k">assert</span> <span class="n">offsets</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">==</span> <span class="n">nonzero_count</span>

    <span class="c1"># Get the elements that belong to the partition.</span>
    <span class="n">partid</span> <span class="o">=</span> <span class="n">partition_book</span><span class="o">.</span><span class="n">partid</span>
    <span class="n">left</span><span class="p">,</span> <span class="n">right</span> <span class="o">=</span> <span class="n">offsets</span><span class="p">[</span><span class="n">partid</span><span class="p">],</span> <span class="n">offsets</span><span class="p">[</span><span class="n">partid</span> <span class="o">+</span> <span class="mi">1</span><span class="p">]</span>

    <span class="n">x</span> <span class="o">=</span> <span class="n">y</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">num_elements</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">elements</span><span class="p">)</span>
    <span class="n">block_size</span> <span class="o">=</span> <span class="n">num_elements</span> <span class="o">//</span> <span class="n">partition_book</span><span class="o">.</span><span class="n">num_partitions</span><span class="p">()</span>
    <span class="n">part_eles</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">tensor</span><span class="p">([],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">elements</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
    <span class="c1"># compute the nonzero tensor of each partition instead of whole tensor to save memory</span>
    <span class="k">for</span> <span class="n">idx</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">num_elements</span><span class="p">,</span> <span class="n">block_size</span><span class="p">):</span>
        <span class="n">nonzero_block</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">nonzero_1d</span><span class="p">(</span>
            <span class="n">elements</span><span class="p">[</span><span class="n">idx</span> <span class="p">:</span> <span class="nb">min</span><span class="p">(</span><span class="n">idx</span> <span class="o">+</span> <span class="n">block_size</span><span class="p">,</span> <span class="n">num_elements</span><span class="p">)]</span>
        <span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">y</span>
        <span class="n">y</span> <span class="o">+=</span> <span class="nb">len</span><span class="p">(</span><span class="n">nonzero_block</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">y</span> <span class="o">&gt;</span> <span class="n">left</span> <span class="ow">and</span> <span class="n">x</span> <span class="o">&lt;</span> <span class="n">right</span><span class="p">:</span>
            <span class="n">start</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">left</span><span class="p">)</span> <span class="o">-</span> <span class="n">x</span>
            <span class="n">end</span> <span class="o">=</span> <span class="nb">min</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">right</span><span class="p">)</span> <span class="o">-</span> <span class="n">x</span>
            <span class="n">tmp</span> <span class="o">=</span> <span class="n">nonzero_block</span><span class="p">[</span><span class="n">start</span><span class="p">:</span><span class="n">end</span><span class="p">]</span> <span class="o">+</span> <span class="n">idx</span>
            <span class="n">part_eles</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">cat</span><span class="p">((</span><span class="n">part_eles</span><span class="p">,</span> <span class="n">tmp</span><span class="p">),</span> <span class="mi">0</span><span class="p">)</span>
        <span class="k">elif</span> <span class="n">x</span> <span class="o">&gt;=</span> <span class="n">right</span><span class="p">:</span>
            <span class="k">break</span>

    <span class="k">return</span> <span class="n">part_eles</span>


<span class="k">def</span> <span class="nf">_split_random_within_part</span><span class="p">(</span><span class="n">partition_book</span><span class="p">,</span> <span class="n">rank</span><span class="p">,</span> <span class="n">part_eles</span><span class="p">):</span>
    <span class="c1"># If there are more than one client in a partition, we need to randomly select a subset of</span>
    <span class="c1"># elements in the partition for a client. We have to make sure that the set of elements</span>
    <span class="c1"># for different clients are disjoint.</span>

    <span class="n">num_clients</span> <span class="o">=</span> <span class="n">role</span><span class="o">.</span><span class="n">get_num_trainers</span><span class="p">()</span>
    <span class="n">num_client_per_part</span> <span class="o">=</span> <span class="n">num_clients</span> <span class="o">//</span> <span class="n">partition_book</span><span class="o">.</span><span class="n">num_partitions</span><span class="p">()</span>
    <span class="k">if</span> <span class="n">num_client_per_part</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">part_eles</span>
    <span class="k">if</span> <span class="n">rank</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">rank</span> <span class="o">=</span> <span class="n">role</span><span class="o">.</span><span class="n">get_trainer_rank</span><span class="p">()</span>
    <span class="k">assert</span> <span class="p">(</span>
        <span class="n">rank</span> <span class="o">&lt;</span> <span class="n">num_clients</span>
    <span class="p">),</span> <span class="s2">&quot;The input rank (</span><span class="si">{}</span><span class="s2">) is incorrect. #Trainers: </span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span>
        <span class="n">rank</span><span class="p">,</span> <span class="n">num_clients</span>
    <span class="p">)</span>
    <span class="n">client_id_in_part</span> <span class="o">=</span> <span class="n">rank</span> <span class="o">%</span> <span class="n">num_client_per_part</span>
    <span class="n">offset</span> <span class="o">=</span> <span class="n">_even_offset</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">part_eles</span><span class="p">),</span> <span class="n">num_client_per_part</span><span class="p">)</span>

    <span class="c1"># We set the random seed for each partition, so that each process (client) in a partition</span>
    <span class="c1"># permute the elements in a partition in the same way, so each process gets a disjoint subset</span>
    <span class="c1"># of elements.</span>
    <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="n">partition_book</span><span class="o">.</span><span class="n">partid</span><span class="p">)</span>
    <span class="n">rand_idx</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">permutation</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">part_eles</span><span class="p">))</span>
    <span class="n">rand_idx</span> <span class="o">=</span> <span class="n">rand_idx</span><span class="p">[</span>
        <span class="n">offset</span><span class="p">[</span><span class="n">client_id_in_part</span><span class="p">]</span> <span class="p">:</span> <span class="n">offset</span><span class="p">[</span><span class="n">client_id_in_part</span> <span class="o">+</span> <span class="mi">1</span><span class="p">]</span>
    <span class="p">]</span>
    <span class="n">idx</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">sort_1d</span><span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">rand_idx</span><span class="p">))</span>
    <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">gather_row</span><span class="p">(</span><span class="n">part_eles</span><span class="p">,</span> <span class="n">idx</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">_split_by_trainer_id</span><span class="p">(</span>
    <span class="n">partition_book</span><span class="p">,</span>
    <span class="n">part_eles</span><span class="p">,</span>
    <span class="n">trainer_id</span><span class="p">,</span>
    <span class="n">num_client_per_part</span><span class="p">,</span>
    <span class="n">client_id_in_part</span><span class="p">,</span>
<span class="p">):</span>
    <span class="c1"># TODO(zhengda): MXNet cannot deal with empty tensors, which makes the implementation</span>
    <span class="c1"># much more difficult. Let&#39;s just use numpy for the computation for now. We just</span>
    <span class="c1"># perform operations on vectors. It shouldn&#39;t be too difficult.</span>
    <span class="n">trainer_id</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">asnumpy</span><span class="p">(</span><span class="n">trainer_id</span><span class="p">)</span>
    <span class="n">part_eles</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">asnumpy</span><span class="p">(</span><span class="n">part_eles</span><span class="p">)</span>
    <span class="n">part_id</span> <span class="o">=</span> <span class="n">trainer_id</span> <span class="o">//</span> <span class="n">num_client_per_part</span>
    <span class="n">trainer_id</span> <span class="o">=</span> <span class="n">trainer_id</span> <span class="o">%</span> <span class="n">num_client_per_part</span>
    <span class="n">local_eles</span> <span class="o">=</span> <span class="n">part_eles</span><span class="p">[</span>
        <span class="n">np</span><span class="o">.</span><span class="n">nonzero</span><span class="p">(</span><span class="n">part_id</span><span class="p">[</span><span class="n">part_eles</span><span class="p">]</span> <span class="o">==</span> <span class="n">partition_book</span><span class="o">.</span><span class="n">partid</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
    <span class="p">]</span>
    <span class="c1"># these are the Ids of the local elements in the partition. The Ids are global Ids.</span>
    <span class="n">remote_eles</span> <span class="o">=</span> <span class="n">part_eles</span><span class="p">[</span>
        <span class="n">np</span><span class="o">.</span><span class="n">nonzero</span><span class="p">(</span><span class="n">part_id</span><span class="p">[</span><span class="n">part_eles</span><span class="p">]</span> <span class="o">!=</span> <span class="n">partition_book</span><span class="o">.</span><span class="n">partid</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
    <span class="p">]</span>
    <span class="c1"># these are the Ids of the remote nodes in the partition. The Ids are global Ids.</span>
    <span class="n">local_eles_idx</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">concatenate</span><span class="p">(</span>
        <span class="p">[</span>
            <span class="n">np</span><span class="o">.</span><span class="n">nonzero</span><span class="p">(</span><span class="n">trainer_id</span><span class="p">[</span><span class="n">local_eles</span><span class="p">]</span> <span class="o">==</span> <span class="n">i</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
            <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_client_per_part</span><span class="p">)</span>
        <span class="p">],</span>
        <span class="c1"># trainer_id[local_eles] is the trainer ids of local nodes in the partition and we</span>
        <span class="c1"># pick out the indices where the node belongs to each trainer i respectively, and</span>
        <span class="c1"># concatenate them.</span>
        <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="c1"># `local_eles_idx` is used to sort `local_eles` according to `trainer_id`. It is a</span>
    <span class="c1"># permutation of 0...(len(local_eles)-1)</span>
    <span class="n">local_eles</span> <span class="o">=</span> <span class="n">local_eles</span><span class="p">[</span><span class="n">local_eles_idx</span><span class="p">]</span>

    <span class="c1"># evenly split local nodes to trainers</span>
    <span class="n">local_offsets</span> <span class="o">=</span> <span class="n">_even_offset</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">local_eles</span><span class="p">),</span> <span class="n">num_client_per_part</span><span class="p">)</span>
    <span class="c1"># evenly split remote nodes to trainers</span>
    <span class="n">remote_offsets</span> <span class="o">=</span> <span class="n">_even_offset</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">remote_eles</span><span class="p">),</span> <span class="n">num_client_per_part</span><span class="p">)</span>

    <span class="n">client_local_eles</span> <span class="o">=</span> <span class="n">local_eles</span><span class="p">[</span>
        <span class="n">local_offsets</span><span class="p">[</span><span class="n">client_id_in_part</span><span class="p">]</span> <span class="p">:</span> <span class="n">local_offsets</span><span class="p">[</span><span class="n">client_id_in_part</span> <span class="o">+</span> <span class="mi">1</span><span class="p">]</span>
    <span class="p">]</span>
    <span class="n">client_remote_eles</span> <span class="o">=</span> <span class="n">remote_eles</span><span class="p">[</span>
        <span class="n">remote_offsets</span><span class="p">[</span><span class="n">client_id_in_part</span><span class="p">]</span> <span class="p">:</span> <span class="n">remote_offsets</span><span class="p">[</span>
            <span class="n">client_id_in_part</span> <span class="o">+</span> <span class="mi">1</span>
        <span class="p">]</span>
    <span class="p">]</span>
    <span class="n">client_eles</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">concatenate</span><span class="p">(</span>
        <span class="p">[</span><span class="n">client_local_eles</span><span class="p">,</span> <span class="n">client_remote_eles</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span>
    <span class="p">)</span>
    <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">client_eles</span><span class="p">)</span>


<div class="viewcode-block" id="node_split">
<a class="viewcode-back" href="../../../generated/dgl.distributed.node_split.html#dgl.distributed.node_split">[docs]</a>
<span class="k">def</span> <span class="nf">node_split</span><span class="p">(</span>
    <span class="n">nodes</span><span class="p">,</span>
    <span class="n">partition_book</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">ntype</span><span class="o">=</span><span class="s2">&quot;_N&quot;</span><span class="p">,</span>
    <span class="n">rank</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">force_even</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
    <span class="n">node_trainer_ids</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
<span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Split nodes and return a subset for the local rank.</span>

<span class="sd">    This function splits the input nodes based on the partition book and</span>
<span class="sd">    returns a subset of nodes for the local rank. This method is used for</span>
<span class="sd">    dividing workloads for distributed training.</span>

<span class="sd">    The input nodes are stored as a vector of masks. The length of the vector is</span>
<span class="sd">    the same as the number of nodes in a graph; 1 indicates that the vertex in</span>
<span class="sd">    the corresponding location exists.</span>

<span class="sd">    There are two strategies to split the nodes. By default, it splits the nodes</span>
<span class="sd">    in a way to maximize data locality. That is, all nodes that belong to a process</span>
<span class="sd">    are returned. If ``force_even`` is set to true, the nodes are split evenly so</span>
<span class="sd">    that each process gets almost the same number of nodes.</span>

<span class="sd">    When ``force_even`` is True, the data locality is still preserved if a graph is partitioned</span>
<span class="sd">    with Metis and the node/edge IDs are shuffled.</span>
<span class="sd">    In this case, majority of the nodes returned for a process are the ones that</span>
<span class="sd">    belong to the process. If node/edge IDs are not shuffled, data locality is not guaranteed.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    nodes : 1D tensor or DistTensor</span>
<span class="sd">        A boolean mask vector that indicates input nodes.</span>
<span class="sd">    partition_book : GraphPartitionBook, optional</span>
<span class="sd">        The graph partition book</span>
<span class="sd">    ntype : str, optional</span>
<span class="sd">        The node type of the input nodes.</span>
<span class="sd">    rank : int, optional</span>
<span class="sd">        The rank of a process. If not given, the rank of the current process is used.</span>
<span class="sd">    force_even : bool, optional</span>
<span class="sd">        Force the nodes are split evenly.</span>
<span class="sd">    node_trainer_ids : 1D tensor or DistTensor, optional</span>
<span class="sd">        If not None, split the nodes to the trainers on the same machine according to</span>
<span class="sd">        trainer IDs assigned to each node. Otherwise, split randomly.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    1D-tensor</span>
<span class="sd">        The vector of node IDs that belong to the rank.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">nodes</span><span class="p">,</span> <span class="n">DistTensor</span><span class="p">):</span>
        <span class="k">assert</span> <span class="p">(</span>
            <span class="n">partition_book</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
        <span class="p">),</span> <span class="s2">&quot;Regular tensor requires a partition book.&quot;</span>
    <span class="k">elif</span> <span class="n">partition_book</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">partition_book</span> <span class="o">=</span> <span class="n">nodes</span><span class="o">.</span><span class="n">part_policy</span><span class="o">.</span><span class="n">partition_book</span>

    <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">nodes</span><span class="p">)</span> <span class="o">==</span> <span class="n">partition_book</span><span class="o">.</span><span class="n">_num_nodes</span><span class="p">(</span>
        <span class="n">ntype</span>
    <span class="p">),</span> <span class="s2">&quot;The length of boolean mask vector should be the number of nodes in the graph.&quot;</span>
    <span class="k">if</span> <span class="n">rank</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">rank</span> <span class="o">=</span> <span class="n">role</span><span class="o">.</span><span class="n">get_trainer_rank</span><span class="p">()</span>
    <span class="k">if</span> <span class="n">force_even</span><span class="p">:</span>
        <span class="n">num_clients</span> <span class="o">=</span> <span class="n">role</span><span class="o">.</span><span class="n">get_num_trainers</span><span class="p">()</span>
        <span class="n">num_client_per_part</span> <span class="o">=</span> <span class="n">num_clients</span> <span class="o">//</span> <span class="n">partition_book</span><span class="o">.</span><span class="n">num_partitions</span><span class="p">()</span>
        <span class="k">assert</span> <span class="p">(</span>
            <span class="n">num_clients</span> <span class="o">%</span> <span class="n">partition_book</span><span class="o">.</span><span class="n">num_partitions</span><span class="p">()</span> <span class="o">==</span> <span class="mi">0</span>
        <span class="p">),</span> <span class="s2">&quot;The total number of clients should be multiple of the number of partitions.&quot;</span>
        <span class="n">part_nid</span> <span class="o">=</span> <span class="n">_split_even_to_part</span><span class="p">(</span><span class="n">partition_book</span><span class="p">,</span> <span class="n">nodes</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">num_client_per_part</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">part_nid</span>
        <span class="k">elif</span> <span class="n">node_trainer_ids</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">_split_random_within_part</span><span class="p">(</span><span class="n">partition_book</span><span class="p">,</span> <span class="n">rank</span><span class="p">,</span> <span class="n">part_nid</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">trainer_id</span> <span class="o">=</span> <span class="n">node_trainer_ids</span><span class="p">[</span><span class="mi">0</span> <span class="p">:</span> <span class="nb">len</span><span class="p">(</span><span class="n">node_trainer_ids</span><span class="p">)]</span>
            <span class="n">max_trainer_id</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">as_scalar</span><span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">reduce_max</span><span class="p">(</span><span class="n">trainer_id</span><span class="p">))</span> <span class="o">+</span> <span class="mi">1</span>

            <span class="k">if</span> <span class="n">max_trainer_id</span> <span class="o">&gt;</span> <span class="n">num_clients</span><span class="p">:</span>
                <span class="c1"># We hope the partition scheme with trainer_id could be used when the number of</span>
                <span class="c1"># trainers is less than the `num_trainers_per_machine` previously assigned during</span>
                <span class="c1"># partitioning.</span>
                <span class="k">assert</span> <span class="n">max_trainer_id</span> <span class="o">%</span> <span class="n">num_clients</span> <span class="o">==</span> <span class="mi">0</span>
                <span class="n">trainer_id</span> <span class="o">//=</span> <span class="n">max_trainer_id</span> <span class="o">//</span> <span class="n">num_clients</span>

            <span class="n">client_id_in_part</span> <span class="o">=</span> <span class="n">rank</span> <span class="o">%</span> <span class="n">num_client_per_part</span>
            <span class="k">return</span> <span class="n">_split_by_trainer_id</span><span class="p">(</span>
                <span class="n">partition_book</span><span class="p">,</span>
                <span class="n">part_nid</span><span class="p">,</span>
                <span class="n">trainer_id</span><span class="p">,</span>
                <span class="n">num_client_per_part</span><span class="p">,</span>
                <span class="n">client_id_in_part</span><span class="p">,</span>
            <span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="c1"># Get all nodes that belong to the rank.</span>
        <span class="n">local_nids</span> <span class="o">=</span> <span class="n">partition_book</span><span class="o">.</span><span class="n">partid2nids</span><span class="p">(</span>
            <span class="n">partition_book</span><span class="o">.</span><span class="n">partid</span><span class="p">,</span> <span class="n">ntype</span><span class="o">=</span><span class="n">ntype</span>
        <span class="p">)</span>
        <span class="k">return</span> <span class="n">_split_local</span><span class="p">(</span><span class="n">partition_book</span><span class="p">,</span> <span class="n">rank</span><span class="p">,</span> <span class="n">nodes</span><span class="p">,</span> <span class="n">local_nids</span><span class="p">)</span></div>



<div class="viewcode-block" id="edge_split">
<a class="viewcode-back" href="../../../generated/dgl.distributed.edge_split.html#dgl.distributed.edge_split">[docs]</a>
<span class="k">def</span> <span class="nf">edge_split</span><span class="p">(</span>
    <span class="n">edges</span><span class="p">,</span>
    <span class="n">partition_book</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">etype</span><span class="o">=</span><span class="s2">&quot;_E&quot;</span><span class="p">,</span>
    <span class="n">rank</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">force_even</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
    <span class="n">edge_trainer_ids</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
<span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Split edges and return a subset for the local rank.</span>

<span class="sd">    This function splits the input edges based on the partition book and</span>
<span class="sd">    returns a subset of edges for the local rank. This method is used for</span>
<span class="sd">    dividing workloads for distributed training.</span>

<span class="sd">    The input edges can be stored as a vector of masks. The length of the vector is</span>
<span class="sd">    the same as the number of edges in a graph; 1 indicates that the edge in</span>
<span class="sd">    the corresponding location exists.</span>

<span class="sd">    There are two strategies to split the edges. By default, it splits the edges</span>
<span class="sd">    in a way to maximize data locality. That is, all edges that belong to a process</span>
<span class="sd">    are returned. If ``force_even`` is set to true, the edges are split evenly so</span>
<span class="sd">    that each process gets almost the same number of edges.</span>

<span class="sd">    When ``force_even`` is True, the data locality is still preserved if a graph is partitioned</span>
<span class="sd">    with Metis and the node/edge IDs are shuffled.</span>
<span class="sd">    In this case, majority of the nodes returned for a process are the ones that</span>
<span class="sd">    belong to the process. If node/edge IDs are not shuffled, data locality is not guaranteed.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    edges : 1D tensor or DistTensor</span>
<span class="sd">        A boolean mask vector that indicates input edges.</span>
<span class="sd">    partition_book : GraphPartitionBook, optional</span>
<span class="sd">        The graph partition book</span>
<span class="sd">    etype : str or (str, str, str), optional</span>
<span class="sd">        The edge type of the input edges.</span>
<span class="sd">    rank : int, optional</span>
<span class="sd">        The rank of a process. If not given, the rank of the current process is used.</span>
<span class="sd">    force_even : bool, optional</span>
<span class="sd">        Force the edges are split evenly.</span>
<span class="sd">    edge_trainer_ids : 1D tensor or DistTensor, optional</span>
<span class="sd">        If not None, split the edges to the trainers on the same machine according to</span>
<span class="sd">        trainer IDs assigned to each edge. Otherwise, split randomly.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    1D-tensor</span>
<span class="sd">        The vector of edge IDs that belong to the rank.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">edges</span><span class="p">,</span> <span class="n">DistTensor</span><span class="p">):</span>
        <span class="k">assert</span> <span class="p">(</span>
            <span class="n">partition_book</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
        <span class="p">),</span> <span class="s2">&quot;Regular tensor requires a partition book.&quot;</span>
    <span class="k">elif</span> <span class="n">partition_book</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">partition_book</span> <span class="o">=</span> <span class="n">edges</span><span class="o">.</span><span class="n">part_policy</span><span class="o">.</span><span class="n">partition_book</span>
    <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">edges</span><span class="p">)</span> <span class="o">==</span> <span class="n">partition_book</span><span class="o">.</span><span class="n">_num_edges</span><span class="p">(</span>
        <span class="n">etype</span>
    <span class="p">),</span> <span class="s2">&quot;The length of boolean mask vector should be the number of edges in the graph.&quot;</span>
    <span class="k">if</span> <span class="n">rank</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">rank</span> <span class="o">=</span> <span class="n">role</span><span class="o">.</span><span class="n">get_trainer_rank</span><span class="p">()</span>
    <span class="k">if</span> <span class="n">force_even</span><span class="p">:</span>
        <span class="n">num_clients</span> <span class="o">=</span> <span class="n">role</span><span class="o">.</span><span class="n">get_num_trainers</span><span class="p">()</span>
        <span class="n">num_client_per_part</span> <span class="o">=</span> <span class="n">num_clients</span> <span class="o">//</span> <span class="n">partition_book</span><span class="o">.</span><span class="n">num_partitions</span><span class="p">()</span>
        <span class="k">assert</span> <span class="p">(</span>
            <span class="n">num_clients</span> <span class="o">%</span> <span class="n">partition_book</span><span class="o">.</span><span class="n">num_partitions</span><span class="p">()</span> <span class="o">==</span> <span class="mi">0</span>
        <span class="p">),</span> <span class="s2">&quot;The total number of clients should be multiple of the number of partitions.&quot;</span>
        <span class="n">part_eid</span> <span class="o">=</span> <span class="n">_split_even_to_part</span><span class="p">(</span><span class="n">partition_book</span><span class="p">,</span> <span class="n">edges</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">num_client_per_part</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">part_eid</span>
        <span class="k">elif</span> <span class="n">edge_trainer_ids</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">_split_random_within_part</span><span class="p">(</span><span class="n">partition_book</span><span class="p">,</span> <span class="n">rank</span><span class="p">,</span> <span class="n">part_eid</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">trainer_id</span> <span class="o">=</span> <span class="n">edge_trainer_ids</span><span class="p">[</span><span class="mi">0</span> <span class="p">:</span> <span class="nb">len</span><span class="p">(</span><span class="n">edge_trainer_ids</span><span class="p">)]</span>
            <span class="n">max_trainer_id</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">as_scalar</span><span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">reduce_max</span><span class="p">(</span><span class="n">trainer_id</span><span class="p">))</span> <span class="o">+</span> <span class="mi">1</span>

            <span class="k">if</span> <span class="n">max_trainer_id</span> <span class="o">&gt;</span> <span class="n">num_clients</span><span class="p">:</span>
                <span class="c1"># We hope the partition scheme with trainer_id could be used when the number of</span>
                <span class="c1"># trainers is less than the `num_trainers_per_machine` previously assigned during</span>
                <span class="c1"># partitioning.</span>
                <span class="k">assert</span> <span class="n">max_trainer_id</span> <span class="o">%</span> <span class="n">num_clients</span> <span class="o">==</span> <span class="mi">0</span>
                <span class="n">trainer_id</span> <span class="o">//=</span> <span class="n">max_trainer_id</span> <span class="o">//</span> <span class="n">num_clients</span>

            <span class="n">client_id_in_part</span> <span class="o">=</span> <span class="n">rank</span> <span class="o">%</span> <span class="n">num_client_per_part</span>
            <span class="k">return</span> <span class="n">_split_by_trainer_id</span><span class="p">(</span>
                <span class="n">partition_book</span><span class="p">,</span>
                <span class="n">part_eid</span><span class="p">,</span>
                <span class="n">trainer_id</span><span class="p">,</span>
                <span class="n">num_client_per_part</span><span class="p">,</span>
                <span class="n">client_id_in_part</span><span class="p">,</span>
            <span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="c1"># Get all edges that belong to the rank.</span>
        <span class="n">local_eids</span> <span class="o">=</span> <span class="n">partition_book</span><span class="o">.</span><span class="n">partid2eids</span><span class="p">(</span>
            <span class="n">partition_book</span><span class="o">.</span><span class="n">partid</span><span class="p">,</span> <span class="n">etype</span><span class="o">=</span><span class="n">etype</span>
        <span class="p">)</span>
        <span class="k">return</span> <span class="n">_split_local</span><span class="p">(</span><span class="n">partition_book</span><span class="p">,</span> <span class="n">rank</span><span class="p">,</span> <span class="n">edges</span><span class="p">,</span> <span class="n">local_eids</span><span class="p">)</span></div>



<span class="n">rpc</span><span class="o">.</span><span class="n">register_service</span><span class="p">(</span><span class="n">INIT_GRAPH</span><span class="p">,</span> <span class="n">InitGraphRequest</span><span class="p">,</span> <span class="n">InitGraphResponse</span><span class="p">)</span>
<span class="n">rpc</span><span class="o">.</span><span class="n">register_service</span><span class="p">(</span>
    <span class="n">QUERY_IF_USE_GRAPHBOLT</span><span class="p">,</span>
    <span class="n">QueryIfUseGraphBoltRequest</span><span class="p">,</span>
    <span class="n">QueryIfUseGraphBoltResponse</span><span class="p">,</span>
<span class="p">)</span>
<span class="n">rpc</span><span class="o">.</span><span class="n">register_service</span><span class="p">(</span>
    <span class="n">ADD_EDGE_ATTRIBUTE_FROM_KV</span><span class="p">,</span>
    <span class="n">AddEdgeAttributeFromKVRequest</span><span class="p">,</span>
    <span class="n">AddEdgeAttributeFromKVResponse</span><span class="p">,</span>
<span class="p">)</span>
<span class="n">rpc</span><span class="o">.</span><span class="n">register_service</span><span class="p">(</span>
    <span class="n">ADD_EDGE_ATTRIBUTE_FROM_SHARED_MEM</span><span class="p">,</span>
    <span class="n">AddEdgeAttributeFromSharedMemRequest</span><span class="p">,</span>
    <span class="n">AddEdgeAttributeFromSharedMemResponse</span><span class="p">,</span>
<span class="p">)</span>
</pre></div>

           </div>
          </div>
          <footer>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2018, DGL Team.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>