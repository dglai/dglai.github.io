<!DOCTYPE html>
<html class="writer-html5" lang="en" data-content_root="../../../../">
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>dgl.nn.pytorch.utils &mdash; DGL 2.4 documentation</title>
      <link rel="stylesheet" type="text/css" href="../../../../_static/pygments.css?v=80d5e7a1" />
      <link rel="stylesheet" type="text/css" href="../../../../_static/css/theme.css?v=19f00094" />
      <link rel="stylesheet" type="text/css" href="../../../../_static/graphviz.css?v=fd3f3429" />
      <link rel="stylesheet" type="text/css" href="../../../../_static/copybutton.css?v=76b2166b" />
      <link rel="stylesheet" type="text/css" href="../../../../_static/sg_gallery.css?v=61a4c737" />
      <link rel="stylesheet" type="text/css" href="../../../../_static/sg_gallery-binder.css?v=f4aeca0c" />
      <link rel="stylesheet" type="text/css" href="../../../../_static/sg_gallery-dataframe.css?v=2082cf3c" />
      <link rel="stylesheet" type="text/css" href="../../../../_static/sg_gallery-rendered-html.css?v=1277b6f3" />
      <link rel="stylesheet" type="text/css" href="../../../../_static/css/custom.css?v=0bf289b5" />

  
  <!--[if lt IE 9]>
    <script src="../../../../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script src="../../../../_static/jquery.js?v=5d32c60e"></script>
        <script src="../../../../_static/_sphinx_javascript_frameworks_compat.js?v=2cd50e6c"></script>
        <script src="../../../../_static/documentation_options.js?v=9caaf7ed"></script>
        <script src="../../../../_static/doctools.js?v=9a2dae69"></script>
        <script src="../../../../_static/sphinx_highlight.js?v=dc90522c"></script>
        <script src="../../../../_static/clipboard.min.js?v=a7894cd8"></script>
        <script src="../../../../_static/copybutton.js?v=ccdb6887"></script>
        <script crossorigin="anonymous" integrity="sha256-Ae2Vz/4ePdIu6ZyI/5ZGsYnb+m0JlOmKPjt6XZ9JJkA=" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
        <script src="https://cdn.jsdelivr.net/npm/@jupyter-widgets/html-manager@^1.0.1/dist/embed-amd.js"></script>
    <script src="../../../../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../../../../genindex.html" />
    <link rel="search" title="Search" href="../../../../search.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="../../../../index.html" class="icon icon-home">
            DGL
          </a>
              <div class="version">
                2.4
              </div>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">Get Started</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../install/index.html">Install and Setup</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../tutorials/blitz/index.html">A Blitz Introduction to DGL</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Advanced Materials</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../stochastic_training/index.html">üÜï Stochastic Training of GNNs with GraphBolt</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../guide/index.html">User Guide</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../guide_cn/index.html">Áî®Êà∑ÊåáÂçó„ÄêÂåÖÂê´ËøáÊó∂‰ø°ÊÅØ„Äë</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../guide_ko/index.html">ÏÇ¨Ïö©Ïûê Í∞ÄÏù¥Îìú[ÏãúÎåÄÏóê Îí§Ï≥êÏßÑ]</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../graphtransformer/index.html">üÜï Tutorial: Graph Transformer</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../notebooks/sparse/index.html">Tutorials: dgl.sparse</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../tutorials/cpu/index.html">Training on CPUs</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../tutorials/multi/index.html">Training on Multiple GPUs</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../tutorials/dist/index.html">Distributed training</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../tutorials/models/index.html">Paper Study with DGL</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">API Reference</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../api/python/dgl.html">dgl</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../api/python/dgl.data.html">dgl.data</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../api/python/dgl.dataloading.html">dgl.dataloading</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../api/python/dgl.DGLGraph.html">dgl.DGLGraph</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../api/python/dgl.distributed.html">dgl.distributed</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../api/python/dgl.function.html">dgl.function</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../api/python/dgl.geometry.html">dgl.geometry</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../api/python/dgl.graphbolt.html">üÜï dgl.graphbolt</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../api/python/nn-pytorch.html">dgl.nn (PyTorch)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../api/python/nn.functional.html">dgl.nn.functional</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../api/python/dgl.ops.html">dgl.ops</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../api/python/dgl.optim.html">dgl.optim</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../api/python/dgl.sampling.html">dgl.sampling</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../api/python/dgl.sparse_v0.html">dgl.sparse</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../api/python/dgl.multiprocessing.html">dgl.multiprocessing</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../api/python/transforms.html">dgl.transforms</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../api/python/udf.html">User-defined Functions</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Notes</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../contribute.html">Contribute to DGL</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../developer/ffi.html">DGL Foreign Function Interface (FFI)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../performance.html">Performance Benchmarks</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Misc</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../faq.html">Frequently Asked Questions (FAQ)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../env_var.html">Environment Variables</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../resources.html">Resources</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../../../index.html">DGL</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../../../../index.html" class="icon icon-home" aria-label="Home"></a></li>
          <li class="breadcrumb-item"><a href="../../../index.html">Module code</a></li>
      <li class="breadcrumb-item active">dgl.nn.pytorch.utils</li>
      <li class="wy-breadcrumbs-aside">
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <h1>Source code for dgl.nn.pytorch.utils</h1><div class="highlight"><pre>
<span></span><span class="sd">&quot;&quot;&quot;Utilities for pytorch NN package&quot;&quot;&quot;</span>
<span class="c1"># pylint: disable=no-member, invalid-name</span>

<span class="kn">import</span> <span class="nn">torch</span> <span class="k">as</span> <span class="nn">th</span>
<span class="kn">import</span> <span class="nn">torch.nn.functional</span> <span class="k">as</span> <span class="nn">F</span>
<span class="kn">from</span> <span class="nn">torch</span> <span class="kn">import</span> <span class="n">nn</span>

<span class="kn">from</span> <span class="nn">...</span> <span class="kn">import</span> <span class="n">DGLGraph</span><span class="p">,</span> <span class="n">function</span> <span class="k">as</span> <span class="n">fn</span>
<span class="kn">from</span> <span class="nn">...base</span> <span class="kn">import</span> <span class="n">dgl_warning</span>


<span class="k">def</span> <span class="nf">matmul_maybe_select</span><span class="p">(</span><span class="n">A</span><span class="p">,</span> <span class="n">B</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Perform Matrix multiplication C = A * B but A could be an integer id vector.</span>

<span class="sd">    If A is an integer vector, we treat it as multiplying a one-hot encoded tensor.</span>
<span class="sd">    In this case, the expensive dense matrix multiply can be replaced by a much</span>
<span class="sd">    cheaper index lookup.</span>

<span class="sd">    For example,</span>
<span class="sd">    ::</span>

<span class="sd">        A = [2, 0, 1],</span>
<span class="sd">        B = [[0.1, 0.2],</span>
<span class="sd">             [0.3, 0.4],</span>
<span class="sd">             [0.5, 0.6]]</span>

<span class="sd">    then matmul_maybe_select(A, B) is equivalent to</span>
<span class="sd">    ::</span>

<span class="sd">        [[0, 0, 1],     [[0.1, 0.2],</span>
<span class="sd">         [1, 0, 0],  *   [0.3, 0.4],</span>
<span class="sd">         [0, 1, 0]]      [0.5, 0.6]]</span>

<span class="sd">    In all other cases, perform a normal matmul.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    A : torch.Tensor</span>
<span class="sd">        lhs tensor</span>
<span class="sd">    B : torch.Tensor</span>
<span class="sd">        rhs tensor</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    C : torch.Tensor</span>
<span class="sd">        result tensor</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">A</span><span class="o">.</span><span class="n">dtype</span> <span class="o">==</span> <span class="n">th</span><span class="o">.</span><span class="n">int64</span> <span class="ow">and</span> <span class="nb">len</span><span class="p">(</span><span class="n">A</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">B</span><span class="o">.</span><span class="n">index_select</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">A</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">th</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">A</span><span class="p">,</span> <span class="n">B</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">bmm_maybe_select</span><span class="p">(</span><span class="n">A</span><span class="p">,</span> <span class="n">B</span><span class="p">,</span> <span class="n">index</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Slice submatrices of A by the given index and perform bmm.</span>

<span class="sd">    B is a 3D tensor of shape (N, D1, D2), which can be viewed as a stack of</span>
<span class="sd">    N matrices of shape (D1, D2). The input index is an integer vector of length M.</span>
<span class="sd">    A could be either:</span>
<span class="sd">    (1) a dense tensor of shape (M, D1),</span>
<span class="sd">    (2) an integer vector of length M.</span>
<span class="sd">    The result C is a 2D matrix of shape (M, D2)</span>

<span class="sd">    For case (1), C is computed by bmm:</span>
<span class="sd">    ::</span>

<span class="sd">        C[i, :] = matmul(A[i, :], B[index[i], :, :])</span>

<span class="sd">    For case (2), C is computed by index select:</span>
<span class="sd">    ::</span>

<span class="sd">        C[i, :] = B[index[i], A[i], :]</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    A : torch.Tensor</span>
<span class="sd">        lhs tensor</span>
<span class="sd">    B : torch.Tensor</span>
<span class="sd">        rhs tensor</span>
<span class="sd">    index : torch.Tensor</span>
<span class="sd">        index tensor</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    C : torch.Tensor</span>
<span class="sd">        return tensor</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">A</span><span class="o">.</span><span class="n">dtype</span> <span class="o">==</span> <span class="n">th</span><span class="o">.</span><span class="n">int64</span> <span class="ow">and</span> <span class="nb">len</span><span class="p">(</span><span class="n">A</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
        <span class="c1"># following is a faster version of B[index, A, :]</span>
        <span class="n">B</span> <span class="o">=</span> <span class="n">B</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">B</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">2</span><span class="p">])</span>
        <span class="n">flatidx</span> <span class="o">=</span> <span class="n">index</span> <span class="o">*</span> <span class="n">B</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="n">A</span>
        <span class="k">return</span> <span class="n">B</span><span class="o">.</span><span class="n">index_select</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">flatidx</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">BB</span> <span class="o">=</span> <span class="n">B</span><span class="o">.</span><span class="n">index_select</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">index</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">th</span><span class="o">.</span><span class="n">bmm</span><span class="p">(</span><span class="n">A</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">),</span> <span class="n">BB</span><span class="p">)</span><span class="o">.</span><span class="n">squeeze</span><span class="p">()</span>


<span class="c1"># pylint: disable=W0235</span>
<span class="k">class</span> <span class="nc">Identity</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;A placeholder identity operator that is argument-insensitive.</span>
<span class="sd">    (Identity has already been supported by PyTorch 1.2, we will directly</span>
<span class="sd">    import torch.nn.Identity in the future)</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">Identity</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Return input&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="n">x</span>


<div class="viewcode-block" id="Sequential">
<a class="viewcode-back" href="../../../../generated/dgl.nn.pytorch.utils.Sequential.html#dgl.nn.pytorch.utils.Sequential">[docs]</a>
<span class="k">class</span> <span class="nc">Sequential</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;A sequential container for stacking graph neural network modules</span>

<span class="sd">    DGL supports two modes: sequentially apply GNN modules on 1) the same graph or</span>
<span class="sd">    2) a list of given graphs. In the second case, the number of graphs equals the</span>
<span class="sd">    number of modules inside this container.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    *args :</span>
<span class="sd">        Sub-modules of torch.nn.Module that will be added to the container in</span>
<span class="sd">        the order by which they are passed in the constructor.</span>

<span class="sd">    Examples</span>
<span class="sd">    --------</span>
<span class="sd">    The following example uses PyTorch backend.</span>

<span class="sd">    Mode 1: sequentially apply GNN modules on the same graph</span>

<span class="sd">    &gt;&gt;&gt; import torch</span>
<span class="sd">    &gt;&gt;&gt; import dgl</span>
<span class="sd">    &gt;&gt;&gt; import torch.nn as nn</span>
<span class="sd">    &gt;&gt;&gt; import dgl.function as fn</span>
<span class="sd">    &gt;&gt;&gt; from dgl.nn.pytorch import Sequential</span>
<span class="sd">    &gt;&gt;&gt; class ExampleLayer(nn.Module):</span>
<span class="sd">    &gt;&gt;&gt;     def __init__(self):</span>
<span class="sd">    &gt;&gt;&gt;         super().__init__()</span>
<span class="sd">    &gt;&gt;&gt;     def forward(self, graph, n_feat, e_feat):</span>
<span class="sd">    &gt;&gt;&gt;         with graph.local_scope():</span>
<span class="sd">    &gt;&gt;&gt;             graph.ndata[&#39;h&#39;] = n_feat</span>
<span class="sd">    &gt;&gt;&gt;             graph.update_all(fn.copy_u(&#39;h&#39;, &#39;m&#39;), fn.sum(&#39;m&#39;, &#39;h&#39;))</span>
<span class="sd">    &gt;&gt;&gt;             n_feat += graph.ndata[&#39;h&#39;]</span>
<span class="sd">    &gt;&gt;&gt;             graph.apply_edges(fn.u_add_v(&#39;h&#39;, &#39;h&#39;, &#39;e&#39;))</span>
<span class="sd">    &gt;&gt;&gt;             e_feat += graph.edata[&#39;e&#39;]</span>
<span class="sd">    &gt;&gt;&gt;             return n_feat, e_feat</span>
<span class="sd">    &gt;&gt;&gt;</span>
<span class="sd">    &gt;&gt;&gt; g = dgl.DGLGraph()</span>
<span class="sd">    &gt;&gt;&gt; g.add_nodes(3)</span>
<span class="sd">    &gt;&gt;&gt; g.add_edges([0, 1, 2, 0, 1, 2, 0, 1, 2], [0, 0, 0, 1, 1, 1, 2, 2, 2])</span>
<span class="sd">    &gt;&gt;&gt; net = Sequential(ExampleLayer(), ExampleLayer(), ExampleLayer())</span>
<span class="sd">    &gt;&gt;&gt; n_feat = torch.rand(3, 4)</span>
<span class="sd">    &gt;&gt;&gt; e_feat = torch.rand(9, 4)</span>
<span class="sd">    &gt;&gt;&gt; net(g, n_feat, e_feat)</span>
<span class="sd">    (tensor([[39.8597, 45.4542, 25.1877, 30.8086],</span>
<span class="sd">             [40.7095, 45.3985, 25.4590, 30.0134],</span>
<span class="sd">             [40.7894, 45.2556, 25.5221, 30.4220]]),</span>
<span class="sd">     tensor([[80.3772, 89.7752, 50.7762, 60.5520],</span>
<span class="sd">             [80.5671, 89.3736, 50.6558, 60.6418],</span>
<span class="sd">             [80.4620, 89.5142, 50.3643, 60.3126],</span>
<span class="sd">             [80.4817, 89.8549, 50.9430, 59.9108],</span>
<span class="sd">             [80.2284, 89.6954, 50.0448, 60.1139],</span>
<span class="sd">             [79.7846, 89.6882, 50.5097, 60.6213],</span>
<span class="sd">             [80.2654, 90.2330, 50.2787, 60.6937],</span>
<span class="sd">             [80.3468, 90.0341, 50.2062, 60.2659],</span>
<span class="sd">             [80.0556, 90.2789, 50.2882, 60.5845]]))</span>

<span class="sd">    Mode 2: sequentially apply GNN modules on different graphs</span>

<span class="sd">    &gt;&gt;&gt; import torch</span>
<span class="sd">    &gt;&gt;&gt; import dgl</span>
<span class="sd">    &gt;&gt;&gt; import torch.nn as nn</span>
<span class="sd">    &gt;&gt;&gt; import dgl.function as fn</span>
<span class="sd">    &gt;&gt;&gt; import networkx as nx</span>
<span class="sd">    &gt;&gt;&gt; from dgl.nn.pytorch import Sequential</span>
<span class="sd">    &gt;&gt;&gt; class ExampleLayer(nn.Module):</span>
<span class="sd">    &gt;&gt;&gt;     def __init__(self):</span>
<span class="sd">    &gt;&gt;&gt;         super().__init__()</span>
<span class="sd">    &gt;&gt;&gt;     def forward(self, graph, n_feat):</span>
<span class="sd">    &gt;&gt;&gt;         with graph.local_scope():</span>
<span class="sd">    &gt;&gt;&gt;             graph.ndata[&#39;h&#39;] = n_feat</span>
<span class="sd">    &gt;&gt;&gt;             graph.update_all(fn.copy_u(&#39;h&#39;, &#39;m&#39;), fn.sum(&#39;m&#39;, &#39;h&#39;))</span>
<span class="sd">    &gt;&gt;&gt;             n_feat += graph.ndata[&#39;h&#39;]</span>
<span class="sd">    &gt;&gt;&gt;             return n_feat.view(graph.num_nodes() // 2, 2, -1).sum(1)</span>
<span class="sd">    &gt;&gt;&gt;</span>
<span class="sd">    &gt;&gt;&gt; g1 = dgl.DGLGraph(nx.erdos_renyi_graph(32, 0.05))</span>
<span class="sd">    &gt;&gt;&gt; g2 = dgl.DGLGraph(nx.erdos_renyi_graph(16, 0.2))</span>
<span class="sd">    &gt;&gt;&gt; g3 = dgl.DGLGraph(nx.erdos_renyi_graph(8, 0.8))</span>
<span class="sd">    &gt;&gt;&gt; net = Sequential(ExampleLayer(), ExampleLayer(), ExampleLayer())</span>
<span class="sd">    &gt;&gt;&gt; n_feat = torch.rand(32, 4)</span>
<span class="sd">    &gt;&gt;&gt; net([g1, g2, g3], n_feat)</span>
<span class="sd">    tensor([[209.6221, 225.5312, 193.8920, 220.1002],</span>
<span class="sd">            [250.0169, 271.9156, 240.2467, 267.7766],</span>
<span class="sd">            [220.4007, 239.7365, 213.8648, 234.9637],</span>
<span class="sd">            [196.4630, 207.6319, 184.2927, 208.7465]])</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">Sequential</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">)</span>

<div class="viewcode-block" id="Sequential.forward">
<a class="viewcode-back" href="../../../../generated/dgl.nn.pytorch.utils.Sequential.html#dgl.nn.pytorch.utils.Sequential.forward">[docs]</a>
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">graph</span><span class="p">,</span> <span class="o">*</span><span class="n">feats</span><span class="p">):</span>
<span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>

<span class="sd">        Sequentially apply modules to the input.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        graph : DGLGraph or list of DGLGraphs</span>
<span class="sd">            The graph(s) to apply modules on.</span>

<span class="sd">        *feats :</span>
<span class="sd">            Input features.</span>
<span class="sd">            The output of the :math:`i`-th module should match the input</span>
<span class="sd">            of the :math:`(i+1)`-th module in the sequential.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">graph</span><span class="p">,</span> <span class="nb">list</span><span class="p">):</span>
            <span class="k">for</span> <span class="n">graph_i</span><span class="p">,</span> <span class="n">module</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">graph</span><span class="p">,</span> <span class="bp">self</span><span class="p">):</span>
                <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">feats</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">):</span>
                    <span class="n">feats</span> <span class="o">=</span> <span class="p">(</span><span class="n">feats</span><span class="p">,)</span>
                <span class="n">feats</span> <span class="o">=</span> <span class="n">module</span><span class="p">(</span><span class="n">graph_i</span><span class="p">,</span> <span class="o">*</span><span class="n">feats</span><span class="p">)</span>
        <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">graph</span><span class="p">,</span> <span class="n">DGLGraph</span><span class="p">):</span>
            <span class="k">for</span> <span class="n">module</span> <span class="ow">in</span> <span class="bp">self</span><span class="p">:</span>
                <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">feats</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">):</span>
                    <span class="n">feats</span> <span class="o">=</span> <span class="p">(</span><span class="n">feats</span><span class="p">,)</span>
                <span class="n">feats</span> <span class="o">=</span> <span class="n">module</span><span class="p">(</span><span class="n">graph</span><span class="p">,</span> <span class="o">*</span><span class="n">feats</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span>
                <span class="s2">&quot;The first argument of forward must be a DGLGraph&quot;</span>
                <span class="s2">&quot; or a list of DGLGraph s&quot;</span>
            <span class="p">)</span>
        <span class="k">return</span> <span class="n">feats</span></div>
</div>



<div class="viewcode-block" id="WeightBasis">
<a class="viewcode-back" href="../../../../generated/dgl.nn.pytorch.utils.WeightBasis.html#dgl.nn.pytorch.utils.WeightBasis">[docs]</a>
<span class="k">class</span> <span class="nc">WeightBasis</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Basis decomposition from `Modeling Relational Data with Graph</span>
<span class="sd">    Convolutional Networks &lt;https://arxiv.org/abs/1703.06103&gt;`__</span>

<span class="sd">    It can be described as below:</span>

<span class="sd">    .. math::</span>

<span class="sd">        W_o = \sum_{b=1}^B a_{ob} V_b</span>

<span class="sd">    Each weight output :math:`W_o` is essentially a linear combination of basis</span>
<span class="sd">    transformations :math:`V_b` with coefficients :math:`a_{ob}`.</span>

<span class="sd">    If is useful as a form of regularization on a large parameter matrix. Thus,</span>
<span class="sd">    the number of weight outputs is usually larger than the number of bases.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    shape : tuple[int]</span>
<span class="sd">        Shape of the basis parameter.</span>
<span class="sd">    num_bases : int</span>
<span class="sd">        Number of bases.</span>
<span class="sd">    num_outputs : int</span>
<span class="sd">        Number of outputs.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">shape</span><span class="p">,</span> <span class="n">num_bases</span><span class="p">,</span> <span class="n">num_outputs</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">WeightBasis</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">shape</span> <span class="o">=</span> <span class="n">shape</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">num_bases</span> <span class="o">=</span> <span class="n">num_bases</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">num_outputs</span> <span class="o">=</span> <span class="n">num_outputs</span>

        <span class="k">if</span> <span class="n">num_outputs</span> <span class="o">&lt;=</span> <span class="n">num_bases</span><span class="p">:</span>
            <span class="n">dgl_warning</span><span class="p">(</span>
                <span class="s2">&quot;The number of weight outputs should be larger than the number&quot;</span>
                <span class="s2">&quot; of bases.&quot;</span>
            <span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">weight</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">th</span><span class="o">.</span><span class="n">Tensor</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">num_bases</span><span class="p">,</span> <span class="o">*</span><span class="n">shape</span><span class="p">))</span>
        <span class="n">nn</span><span class="o">.</span><span class="n">init</span><span class="o">.</span><span class="n">xavier_uniform_</span><span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">weight</span><span class="p">,</span> <span class="n">gain</span><span class="o">=</span><span class="n">nn</span><span class="o">.</span><span class="n">init</span><span class="o">.</span><span class="n">calculate_gain</span><span class="p">(</span><span class="s2">&quot;relu&quot;</span><span class="p">)</span>
        <span class="p">)</span>
        <span class="c1"># linear combination coefficients</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">w_comp</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">th</span><span class="o">.</span><span class="n">Tensor</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">num_outputs</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_bases</span><span class="p">))</span>
        <span class="n">nn</span><span class="o">.</span><span class="n">init</span><span class="o">.</span><span class="n">xavier_uniform_</span><span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">w_comp</span><span class="p">,</span> <span class="n">gain</span><span class="o">=</span><span class="n">nn</span><span class="o">.</span><span class="n">init</span><span class="o">.</span><span class="n">calculate_gain</span><span class="p">(</span><span class="s2">&quot;relu&quot;</span><span class="p">)</span>
        <span class="p">)</span>

<div class="viewcode-block" id="WeightBasis.forward">
<a class="viewcode-back" href="../../../../generated/dgl.nn.pytorch.utils.WeightBasis.html#dgl.nn.pytorch.utils.WeightBasis.forward">[docs]</a>
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Forward computation</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        weight : torch.Tensor</span>
<span class="sd">            Composed weight tensor of shape ``(num_outputs,) + shape``</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># generate all weights from bases</span>
        <span class="n">weight</span> <span class="o">=</span> <span class="n">th</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">w_comp</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">num_bases</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">))</span>
        <span class="k">return</span> <span class="n">weight</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">num_outputs</span><span class="p">,</span> <span class="o">*</span><span class="bp">self</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span></div>
</div>



<div class="viewcode-block" id="JumpingKnowledge">
<a class="viewcode-back" href="../../../../generated/dgl.nn.pytorch.utils.JumpingKnowledge.html#dgl.nn.pytorch.utils.JumpingKnowledge">[docs]</a>
<span class="k">class</span> <span class="nc">JumpingKnowledge</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;The Jumping Knowledge aggregation module from `Representation Learning on</span>
<span class="sd">    Graphs with Jumping Knowledge Networks &lt;https://arxiv.org/abs/1806.03536&gt;`__</span>

<span class="sd">    It aggregates the output representations of multiple GNN layers with</span>

<span class="sd">    **concatenation**</span>

<span class="sd">    .. math::</span>

<span class="sd">        h_i^{(1)} \, \Vert \, \ldots \, \Vert \, h_i^{(T)}</span>

<span class="sd">    or **max pooling**</span>

<span class="sd">    .. math::</span>

<span class="sd">        \max \left( h_i^{(1)}, \ldots, h_i^{(T)} \right)</span>

<span class="sd">    or **LSTM**</span>

<span class="sd">    .. math::</span>

<span class="sd">        \sum_{t=1}^T \alpha_i^{(t)} h_i^{(t)}</span>

<span class="sd">    with attention scores :math:`\alpha_i^{(t)}` obtained from a BiLSTM</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    mode : str</span>
<span class="sd">        The aggregation to apply. It can be &#39;cat&#39;, &#39;max&#39;, or &#39;lstm&#39;,</span>
<span class="sd">        corresponding to the equations above in order.</span>
<span class="sd">    in_feats : int, optional</span>
<span class="sd">        This argument is only required if :attr:`mode` is ``&#39;lstm&#39;``.</span>
<span class="sd">        The output representation size of a single GNN layer. Note that</span>
<span class="sd">        all GNN layers need to have the same output representation size.</span>
<span class="sd">    num_layers : int, optional</span>
<span class="sd">        This argument is only required if :attr:`mode` is ``&#39;lstm&#39;``.</span>
<span class="sd">        The number of GNN layers for output aggregation.</span>

<span class="sd">    Examples</span>
<span class="sd">    --------</span>
<span class="sd">    &gt;&gt;&gt; import dgl</span>
<span class="sd">    &gt;&gt;&gt; import torch as th</span>
<span class="sd">    &gt;&gt;&gt; from dgl.nn import JumpingKnowledge</span>

<span class="sd">    &gt;&gt;&gt; # Output representations of two GNN layers</span>
<span class="sd">    &gt;&gt;&gt; num_nodes = 3</span>
<span class="sd">    &gt;&gt;&gt; in_feats = 4</span>
<span class="sd">    &gt;&gt;&gt; feat_list = [th.zeros(num_nodes, in_feats), th.ones(num_nodes, in_feats)]</span>

<span class="sd">    &gt;&gt;&gt; # Case1</span>
<span class="sd">    &gt;&gt;&gt; model = JumpingKnowledge()</span>
<span class="sd">    &gt;&gt;&gt; model(feat_list).shape</span>
<span class="sd">    torch.Size([3, 8])</span>

<span class="sd">    &gt;&gt;&gt; # Case2</span>
<span class="sd">    &gt;&gt;&gt; model = JumpingKnowledge(mode=&#39;max&#39;)</span>
<span class="sd">    &gt;&gt;&gt; model(feat_list).shape</span>
<span class="sd">    torch.Size([3, 4])</span>

<span class="sd">    &gt;&gt;&gt; # Case3</span>
<span class="sd">    &gt;&gt;&gt; model = JumpingKnowledge(mode=&#39;max&#39;, in_feats=in_feats, num_layers=len(feat_list))</span>
<span class="sd">    &gt;&gt;&gt; model(feat_list).shape</span>
<span class="sd">    torch.Size([3, 4])</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="s2">&quot;cat&quot;</span><span class="p">,</span> <span class="n">in_feats</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">num_layers</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">JumpingKnowledge</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="k">assert</span> <span class="n">mode</span> <span class="ow">in</span> <span class="p">[</span>
            <span class="s2">&quot;cat&quot;</span><span class="p">,</span>
            <span class="s2">&quot;max&quot;</span><span class="p">,</span>
            <span class="s2">&quot;lstm&quot;</span><span class="p">,</span>
        <span class="p">],</span> <span class="s2">&quot;Expect mode to be &#39;cat&#39;, or &#39;max&#39; or &#39;lstm&#39;, got </span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">mode</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">mode</span> <span class="o">=</span> <span class="n">mode</span>

        <span class="k">if</span> <span class="n">mode</span> <span class="o">==</span> <span class="s2">&quot;lstm&quot;</span><span class="p">:</span>
            <span class="k">assert</span> <span class="n">in_feats</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">,</span> <span class="s2">&quot;in_feats is required for lstm mode&quot;</span>
            <span class="k">assert</span> <span class="p">(</span>
                <span class="n">num_layers</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
            <span class="p">),</span> <span class="s2">&quot;num_layers is required for lstm mode&quot;</span>
            <span class="n">hidden_size</span> <span class="o">=</span> <span class="p">(</span><span class="n">num_layers</span> <span class="o">*</span> <span class="n">in_feats</span><span class="p">)</span> <span class="o">//</span> <span class="mi">2</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">lstm</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">LSTM</span><span class="p">(</span>
                <span class="n">in_feats</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">,</span> <span class="n">bidirectional</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">batch_first</span><span class="o">=</span><span class="kc">True</span>
            <span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">att</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="n">hidden_size</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>

<div class="viewcode-block" id="JumpingKnowledge.reset_parameters">
<a class="viewcode-back" href="../../../../generated/dgl.nn.pytorch.utils.JumpingKnowledge.html#dgl.nn.pytorch.utils.JumpingKnowledge.reset_parameters">[docs]</a>
    <span class="k">def</span> <span class="nf">reset_parameters</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>

<span class="sd">        Description</span>
<span class="sd">        -----------</span>
<span class="sd">        Reinitialize learnable parameters. This comes into effect only for the lstm mode.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">mode</span> <span class="o">==</span> <span class="s2">&quot;lstm&quot;</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">lstm</span><span class="o">.</span><span class="n">reset_parameters</span><span class="p">()</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">att</span><span class="o">.</span><span class="n">reset_parameters</span><span class="p">()</span></div>


<div class="viewcode-block" id="JumpingKnowledge.forward">
<a class="viewcode-back" href="../../../../generated/dgl.nn.pytorch.utils.JumpingKnowledge.html#dgl.nn.pytorch.utils.JumpingKnowledge.forward">[docs]</a>
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">feat_list</span><span class="p">):</span>
<span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>

<span class="sd">        Description</span>
<span class="sd">        -----------</span>
<span class="sd">        Aggregate output representations across multiple GNN layers.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        feat_list : list[Tensor]</span>
<span class="sd">            feat_list[i] is the output representations of a GNN layer.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        Tensor</span>
<span class="sd">            The aggregated representations.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">mode</span> <span class="o">==</span> <span class="s2">&quot;cat&quot;</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">th</span><span class="o">.</span><span class="n">cat</span><span class="p">(</span><span class="n">feat_list</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
        <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">mode</span> <span class="o">==</span> <span class="s2">&quot;max&quot;</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">th</span><span class="o">.</span><span class="n">stack</span><span class="p">(</span><span class="n">feat_list</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="c1"># LSTM</span>
            <span class="n">stacked_feat_list</span> <span class="o">=</span> <span class="n">th</span><span class="o">.</span><span class="n">stack</span><span class="p">(</span>
                <span class="n">feat_list</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span>
            <span class="p">)</span>  <span class="c1"># (N, num_layers, in_feats)</span>
            <span class="n">alpha</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">lstm</span><span class="p">(</span><span class="n">stacked_feat_list</span><span class="p">)</span>
            <span class="n">alpha</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">att</span><span class="p">(</span><span class="n">alpha</span><span class="p">)</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>  <span class="c1"># (N, num_layers)</span>
            <span class="n">alpha</span> <span class="o">=</span> <span class="n">th</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">alpha</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
            <span class="k">return</span> <span class="p">(</span><span class="n">stacked_feat_list</span> <span class="o">*</span> <span class="n">alpha</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">))</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span></div>
</div>



<div class="viewcode-block" id="LabelPropagation">
<a class="viewcode-back" href="../../../../generated/dgl.nn.pytorch.utils.LabelPropagation.html#dgl.nn.pytorch.utils.LabelPropagation">[docs]</a>
<span class="k">class</span> <span class="nc">LabelPropagation</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Label Propagation from `Learning from Labeled and Unlabeled Data with Label</span>
<span class="sd">    Propagation &lt;http://mlg.eng.cam.ac.uk/zoubin/papers/CMU-CALD-02-107.pdf&gt;`__</span>

<span class="sd">    .. math::</span>

<span class="sd">        \mathbf{Y}^{(t+1)} = \alpha \tilde{A} \mathbf{Y}^{(t)} + (1 - \alpha) \mathbf{Y}^{(0)}</span>

<span class="sd">    where unlabeled data is initially set to zero and inferred from labeled data via</span>
<span class="sd">    propagation. :math:`\alpha` is a weight parameter for balancing between updated labels</span>
<span class="sd">    and initial labels. :math:`\tilde{A}` denotes the normalized adjacency matrix.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    k: int</span>
<span class="sd">        The number of propagation steps.</span>
<span class="sd">    alpha : float</span>
<span class="sd">        The :math:`\alpha` coefficient in range [0, 1].</span>
<span class="sd">    norm_type : str, optional</span>
<span class="sd">        The type of normalization applied to the adjacency matrix, must be one of the</span>
<span class="sd">        following choices:</span>

<span class="sd">        * ``row``: row-normalized adjacency as :math:`D^{-1}A`</span>

<span class="sd">        * ``sym``: symmetrically normalized adjacency as :math:`D^{-1/2}AD^{-1/2}`</span>

<span class="sd">        Default: &#39;sym&#39;.</span>
<span class="sd">    clamp : bool, optional</span>
<span class="sd">        A bool flag to indicate whether to clamp the labels to [0, 1] after propagation.</span>
<span class="sd">        Default: True.</span>
<span class="sd">    normalize: bool, optional</span>
<span class="sd">        A bool flag to indicate whether to apply row-normalization after propagation.</span>
<span class="sd">        Default: False.</span>
<span class="sd">    reset : bool, optional</span>
<span class="sd">        A bool flag to indicate whether to reset the known labels after each</span>
<span class="sd">        propagation step. Default: False.</span>

<span class="sd">    Examples</span>
<span class="sd">    --------</span>
<span class="sd">    &gt;&gt;&gt; import torch</span>
<span class="sd">    &gt;&gt;&gt; import dgl</span>
<span class="sd">    &gt;&gt;&gt; from dgl.nn import LabelPropagation</span>

<span class="sd">    &gt;&gt;&gt; label_propagation = LabelPropagation(k=5, alpha=0.5, clamp=False, normalize=True)</span>
<span class="sd">    &gt;&gt;&gt; g = dgl.rand_graph(5, 10)</span>
<span class="sd">    &gt;&gt;&gt; labels = torch.tensor([0, 2, 1, 3, 0]).long()</span>
<span class="sd">    &gt;&gt;&gt; mask = torch.tensor([0, 1, 1, 1, 0]).bool()</span>
<span class="sd">    &gt;&gt;&gt; new_labels = label_propagation(g, labels, mask)</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">k</span><span class="p">,</span>
        <span class="n">alpha</span><span class="p">,</span>
        <span class="n">norm_type</span><span class="o">=</span><span class="s2">&quot;sym&quot;</span><span class="p">,</span>
        <span class="n">clamp</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
        <span class="n">normalize</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
        <span class="n">reset</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">LabelPropagation</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">k</span> <span class="o">=</span> <span class="n">k</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">alpha</span> <span class="o">=</span> <span class="n">alpha</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">norm_type</span> <span class="o">=</span> <span class="n">norm_type</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">clamp</span> <span class="o">=</span> <span class="n">clamp</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">normalize</span> <span class="o">=</span> <span class="n">normalize</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">reset</span> <span class="o">=</span> <span class="n">reset</span>

<div class="viewcode-block" id="LabelPropagation.forward">
<a class="viewcode-back" href="../../../../generated/dgl.nn.pytorch.utils.LabelPropagation.html#dgl.nn.pytorch.utils.LabelPropagation.forward">[docs]</a>
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">g</span><span class="p">,</span> <span class="n">labels</span><span class="p">,</span> <span class="n">mask</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Compute the label propagation process.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        g : DGLGraph</span>
<span class="sd">            The input graph.</span>
<span class="sd">        labels : torch.Tensor</span>
<span class="sd">            The input node labels. There are three cases supported.</span>

<span class="sd">            * A LongTensor of shape :math:`(N, 1)` or :math:`(N,)` for node class labels in</span>
<span class="sd">              multiclass classification, where :math:`N` is the number of nodes.</span>
<span class="sd">            * A LongTensor of shape :math:`(N, C)` for one-hot encoding of node class labels</span>
<span class="sd">              in multiclass classification, where :math:`C` is the number of classes.</span>
<span class="sd">            * A LongTensor of shape :math:`(N, L)` for node labels in multilabel binary</span>
<span class="sd">              classification, where :math:`L` is the number of labels.</span>
<span class="sd">        mask : torch.Tensor</span>
<span class="sd">            The bool indicators of shape :math:`(N,)` with True denoting labeled nodes.</span>
<span class="sd">            Default: None, indicating all nodes are labeled.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        torch.Tensor</span>
<span class="sd">            The propagated node labels of shape :math:`(N, D)` with float type, where :math:`D`</span>
<span class="sd">            is the number of classes or labels.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">with</span> <span class="n">g</span><span class="o">.</span><span class="n">local_scope</span><span class="p">():</span>
            <span class="c1"># multi-label / multi-class</span>
            <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">labels</span><span class="o">.</span><span class="n">size</span><span class="p">())</span> <span class="o">&gt;</span> <span class="mi">1</span> <span class="ow">and</span> <span class="n">labels</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
                <span class="n">labels</span> <span class="o">=</span> <span class="n">labels</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">th</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
            <span class="c1"># single-label multi-class</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">labels</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">one_hot</span><span class="p">(</span><span class="n">labels</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">))</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">th</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>

            <span class="n">y</span> <span class="o">=</span> <span class="n">labels</span>
            <span class="k">if</span> <span class="n">mask</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">y</span> <span class="o">=</span> <span class="n">th</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">labels</span><span class="p">)</span>
                <span class="n">y</span><span class="p">[</span><span class="n">mask</span><span class="p">]</span> <span class="o">=</span> <span class="n">labels</span><span class="p">[</span><span class="n">mask</span><span class="p">]</span>

            <span class="n">init</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">alpha</span><span class="p">)</span> <span class="o">*</span> <span class="n">y</span>
            <span class="n">in_degs</span> <span class="o">=</span> <span class="n">g</span><span class="o">.</span><span class="n">in_degrees</span><span class="p">()</span><span class="o">.</span><span class="n">float</span><span class="p">()</span><span class="o">.</span><span class="n">clamp</span><span class="p">(</span><span class="nb">min</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
            <span class="n">out_degs</span> <span class="o">=</span> <span class="n">g</span><span class="o">.</span><span class="n">out_degrees</span><span class="p">()</span><span class="o">.</span><span class="n">float</span><span class="p">()</span><span class="o">.</span><span class="n">clamp</span><span class="p">(</span><span class="nb">min</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">norm_type</span> <span class="o">==</span> <span class="s2">&quot;sym&quot;</span><span class="p">:</span>
                <span class="n">norm_i</span> <span class="o">=</span> <span class="n">th</span><span class="o">.</span><span class="n">pow</span><span class="p">(</span><span class="n">in_degs</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.5</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">labels</span><span class="o">.</span><span class="n">device</span><span class="p">)</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
                <span class="n">norm_j</span> <span class="o">=</span> <span class="n">th</span><span class="o">.</span><span class="n">pow</span><span class="p">(</span><span class="n">out_degs</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.5</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">labels</span><span class="o">.</span><span class="n">device</span><span class="p">)</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
            <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">norm_type</span> <span class="o">==</span> <span class="s2">&quot;row&quot;</span><span class="p">:</span>
                <span class="n">norm_i</span> <span class="o">=</span> <span class="n">th</span><span class="o">.</span><span class="n">pow</span><span class="p">(</span><span class="n">in_degs</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.0</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">labels</span><span class="o">.</span><span class="n">device</span><span class="p">)</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                    <span class="sa">f</span><span class="s2">&quot;Expect norm_type to be &#39;sym&#39; or &#39;row&#39;, got </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">norm_type</span><span class="si">}</span><span class="s2">&quot;</span>
                <span class="p">)</span>

            <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">k</span><span class="p">):</span>
                <span class="n">g</span><span class="o">.</span><span class="n">ndata</span><span class="p">[</span><span class="s2">&quot;h&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">y</span> <span class="o">*</span> <span class="n">norm_j</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">norm_type</span> <span class="o">==</span> <span class="s2">&quot;sym&quot;</span> <span class="k">else</span> <span class="n">y</span>
                <span class="n">g</span><span class="o">.</span><span class="n">update_all</span><span class="p">(</span><span class="n">fn</span><span class="o">.</span><span class="n">copy_u</span><span class="p">(</span><span class="s2">&quot;h&quot;</span><span class="p">,</span> <span class="s2">&quot;m&quot;</span><span class="p">),</span> <span class="n">fn</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="s2">&quot;m&quot;</span><span class="p">,</span> <span class="s2">&quot;h&quot;</span><span class="p">))</span>
                <span class="n">y</span> <span class="o">=</span> <span class="n">init</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">alpha</span> <span class="o">*</span> <span class="n">g</span><span class="o">.</span><span class="n">ndata</span><span class="p">[</span><span class="s2">&quot;h&quot;</span><span class="p">]</span> <span class="o">*</span> <span class="n">norm_i</span>

                <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">clamp</span><span class="p">:</span>
                    <span class="n">y</span> <span class="o">=</span> <span class="n">y</span><span class="o">.</span><span class="n">clamp_</span><span class="p">(</span><span class="mf">0.0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">)</span>
                <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">normalize</span><span class="p">:</span>
                    <span class="n">y</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">normalize</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
                <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">reset</span><span class="p">:</span>
                    <span class="n">y</span><span class="p">[</span><span class="n">mask</span><span class="p">]</span> <span class="o">=</span> <span class="n">labels</span><span class="p">[</span><span class="n">mask</span><span class="p">]</span>

            <span class="k">return</span> <span class="n">y</span></div>
</div>

</pre></div>

           </div>
          </div>
          <footer>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2018, DGL Team.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>