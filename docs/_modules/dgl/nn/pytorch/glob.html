<!DOCTYPE html>
<html class="writer-html5" lang="en" data-content_root="../../../../">
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>dgl.nn.pytorch.glob &mdash; DGL 2.4 documentation</title>
      <link rel="stylesheet" type="text/css" href="../../../../_static/pygments.css?v=80d5e7a1" />
      <link rel="stylesheet" type="text/css" href="../../../../_static/css/theme.css?v=19f00094" />
      <link rel="stylesheet" type="text/css" href="../../../../_static/graphviz.css?v=fd3f3429" />
      <link rel="stylesheet" type="text/css" href="../../../../_static/copybutton.css?v=76b2166b" />
      <link rel="stylesheet" type="text/css" href="../../../../_static/sg_gallery.css?v=61a4c737" />
      <link rel="stylesheet" type="text/css" href="../../../../_static/sg_gallery-binder.css?v=f4aeca0c" />
      <link rel="stylesheet" type="text/css" href="../../../../_static/sg_gallery-dataframe.css?v=2082cf3c" />
      <link rel="stylesheet" type="text/css" href="../../../../_static/sg_gallery-rendered-html.css?v=1277b6f3" />
      <link rel="stylesheet" type="text/css" href="../../../../_static/css/custom.css?v=0bf289b5" />

  
  <!--[if lt IE 9]>
    <script src="../../../../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script src="../../../../_static/jquery.js?v=5d32c60e"></script>
        <script src="../../../../_static/_sphinx_javascript_frameworks_compat.js?v=2cd50e6c"></script>
        <script src="../../../../_static/documentation_options.js?v=9caaf7ed"></script>
        <script src="../../../../_static/doctools.js?v=9a2dae69"></script>
        <script src="../../../../_static/sphinx_highlight.js?v=dc90522c"></script>
        <script src="../../../../_static/clipboard.min.js?v=a7894cd8"></script>
        <script src="../../../../_static/copybutton.js?v=ccdb6887"></script>
        <script crossorigin="anonymous" integrity="sha256-Ae2Vz/4ePdIu6ZyI/5ZGsYnb+m0JlOmKPjt6XZ9JJkA=" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
        <script src="https://cdn.jsdelivr.net/npm/@jupyter-widgets/html-manager@^1.0.1/dist/embed-amd.js"></script>
    <script src="../../../../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../../../../genindex.html" />
    <link rel="search" title="Search" href="../../../../search.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="../../../../index.html" class="icon icon-home">
            DGL
          </a>
              <div class="version">
                2.4
              </div>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">Get Started</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../install/index.html">Install and Setup</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../tutorials/blitz/index.html">A Blitz Introduction to DGL</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Advanced Materials</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../stochastic_training/index.html">üÜï Stochastic Training of GNNs with GraphBolt</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../guide/index.html">User Guide</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../guide_cn/index.html">Áî®Êà∑ÊåáÂçó„ÄêÂåÖÂê´ËøáÊó∂‰ø°ÊÅØ„Äë</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../guide_ko/index.html">ÏÇ¨Ïö©Ïûê Í∞ÄÏù¥Îìú[ÏãúÎåÄÏóê Îí§Ï≥êÏßÑ]</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../graphtransformer/index.html">üÜï Tutorial: Graph Transformer</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../notebooks/sparse/index.html">Tutorials: dgl.sparse</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../tutorials/cpu/index.html">Training on CPUs</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../tutorials/multi/index.html">Training on Multiple GPUs</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../tutorials/dist/index.html">Distributed training</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../tutorials/models/index.html">Paper Study with DGL</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">API Reference</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../api/python/dgl.html">dgl</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../api/python/dgl.data.html">dgl.data</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../api/python/dgl.dataloading.html">dgl.dataloading</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../api/python/dgl.DGLGraph.html">dgl.DGLGraph</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../api/python/dgl.distributed.html">dgl.distributed</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../api/python/dgl.function.html">dgl.function</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../api/python/dgl.geometry.html">dgl.geometry</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../api/python/dgl.graphbolt.html">üÜï dgl.graphbolt</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../api/python/nn-pytorch.html">dgl.nn (PyTorch)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../api/python/nn.functional.html">dgl.nn.functional</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../api/python/dgl.ops.html">dgl.ops</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../api/python/dgl.optim.html">dgl.optim</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../api/python/dgl.sampling.html">dgl.sampling</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../api/python/dgl.sparse_v0.html">dgl.sparse</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../api/python/dgl.multiprocessing.html">dgl.multiprocessing</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../api/python/transforms.html">dgl.transforms</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../api/python/udf.html">User-defined Functions</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Notes</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../contribute.html">Contribute to DGL</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../developer/ffi.html">DGL Foreign Function Interface (FFI)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../performance.html">Performance Benchmarks</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Misc</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../faq.html">Frequently Asked Questions (FAQ)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../env_var.html">Environment Variables</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../resources.html">Resources</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../../../index.html">DGL</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../../../../index.html" class="icon icon-home" aria-label="Home"></a></li>
          <li class="breadcrumb-item"><a href="../../../index.html">Module code</a></li>
      <li class="breadcrumb-item active">dgl.nn.pytorch.glob</li>
      <li class="wy-breadcrumbs-aside">
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <h1>Source code for dgl.nn.pytorch.glob</h1><div class="highlight"><pre>
<span></span><span class="sd">&quot;&quot;&quot;Torch modules for graph global pooling.&quot;&quot;&quot;</span>
<span class="c1"># pylint: disable= no-member, arguments-differ, invalid-name, W0235</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">torch</span> <span class="k">as</span> <span class="nn">th</span>
<span class="kn">import</span> <span class="nn">torch.nn</span> <span class="k">as</span> <span class="nn">nn</span>

<span class="kn">from</span> <span class="nn">...backend</span> <span class="kn">import</span> <span class="n">pytorch</span> <span class="k">as</span> <span class="n">F</span>
<span class="kn">from</span> <span class="nn">...base</span> <span class="kn">import</span> <span class="n">dgl_warning</span>
<span class="kn">from</span> <span class="nn">...readout</span> <span class="kn">import</span> <span class="p">(</span>
    <span class="n">broadcast_nodes</span><span class="p">,</span>
    <span class="n">max_nodes</span><span class="p">,</span>
    <span class="n">mean_nodes</span><span class="p">,</span>
    <span class="n">softmax_nodes</span><span class="p">,</span>
    <span class="n">sum_nodes</span><span class="p">,</span>
    <span class="n">topk_nodes</span><span class="p">,</span>
<span class="p">)</span>

<span class="n">__all__</span> <span class="o">=</span> <span class="p">[</span>
    <span class="s2">&quot;SumPooling&quot;</span><span class="p">,</span>
    <span class="s2">&quot;AvgPooling&quot;</span><span class="p">,</span>
    <span class="s2">&quot;MaxPooling&quot;</span><span class="p">,</span>
    <span class="s2">&quot;SortPooling&quot;</span><span class="p">,</span>
    <span class="s2">&quot;GlobalAttentionPooling&quot;</span><span class="p">,</span>
    <span class="s2">&quot;Set2Set&quot;</span><span class="p">,</span>
    <span class="s2">&quot;SetTransformerEncoder&quot;</span><span class="p">,</span>
    <span class="s2">&quot;SetTransformerDecoder&quot;</span><span class="p">,</span>
    <span class="s2">&quot;WeightAndSum&quot;</span><span class="p">,</span>
<span class="p">]</span>


<div class="viewcode-block" id="SumPooling">
<a class="viewcode-back" href="../../../../generated/dgl.nn.pytorch.glob.SumPooling.html#dgl.nn.pytorch.glob.SumPooling">[docs]</a>
<span class="k">class</span> <span class="nc">SumPooling</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Apply sum pooling over the nodes in a graph.</span>

<span class="sd">    .. math::</span>
<span class="sd">        r^{(i)} = \sum_{k=1}^{N_i} x^{(i)}_k</span>

<span class="sd">    Notes</span>
<span class="sd">    -----</span>
<span class="sd">        Input: Could be one graph, or a batch of graphs. If using a batch of graphs,</span>
<span class="sd">        make sure nodes in all graphs have the same feature size, and concatenate</span>
<span class="sd">        nodes&#39; feature together as the input.</span>

<span class="sd">    Examples</span>
<span class="sd">    --------</span>
<span class="sd">    The following example uses PyTorch backend.</span>

<span class="sd">    &gt;&gt;&gt; import dgl</span>
<span class="sd">    &gt;&gt;&gt; import torch as th</span>
<span class="sd">    &gt;&gt;&gt; from dgl.nn import SumPooling</span>
<span class="sd">    &gt;&gt;&gt;</span>
<span class="sd">    &gt;&gt;&gt; g1 = dgl.rand_graph(3, 4)  # g1 is a random graph with 3 nodes and 4 edges</span>
<span class="sd">    &gt;&gt;&gt; g1_node_feats = th.rand(3, 5)  # feature size is 5</span>
<span class="sd">    &gt;&gt;&gt; g1_node_feats</span>
<span class="sd">    tensor([[0.8948, 0.0699, 0.9137, 0.7567, 0.3637],</span>
<span class="sd">            [0.8137, 0.8938, 0.8377, 0.4249, 0.6118],</span>
<span class="sd">            [0.5197, 0.9030, 0.6825, 0.5725, 0.4755]])</span>
<span class="sd">    &gt;&gt;&gt;</span>
<span class="sd">    &gt;&gt;&gt; g2 = dgl.rand_graph(4, 6)  # g2 is a random graph with 4 nodes and 6 edges</span>
<span class="sd">    &gt;&gt;&gt; g2_node_feats = th.rand(4, 5)  # feature size is 5</span>
<span class="sd">    &gt;&gt;&gt; g2_node_feats</span>
<span class="sd">    tensor([[0.2053, 0.2426, 0.4111, 0.9028, 0.5658],</span>
<span class="sd">            [0.5278, 0.6365, 0.9990, 0.2351, 0.8945],</span>
<span class="sd">            [0.3134, 0.0580, 0.4349, 0.7949, 0.3891],</span>
<span class="sd">            [0.0142, 0.2709, 0.3330, 0.8521, 0.6925]])</span>
<span class="sd">    &gt;&gt;&gt;</span>
<span class="sd">    &gt;&gt;&gt; sumpool = SumPooling()  # create a sum pooling layer</span>

<span class="sd">    Case 1: Input a single graph</span>

<span class="sd">    &gt;&gt;&gt; sumpool(g1, g1_node_feats)</span>
<span class="sd">    tensor([[2.2282, 1.8667, 2.4338, 1.7540, 1.4511]])</span>

<span class="sd">    Case 2: Input a batch of graphs</span>

<span class="sd">    Build a batch of DGL graphs and concatenate all graphs&#39; node features into one tensor.</span>

<span class="sd">    &gt;&gt;&gt; batch_g = dgl.batch([g1, g2])</span>
<span class="sd">    &gt;&gt;&gt; batch_f = th.cat([g1_node_feats, g2_node_feats])</span>
<span class="sd">    &gt;&gt;&gt;</span>
<span class="sd">    &gt;&gt;&gt; sumpool(batch_g, batch_f)</span>
<span class="sd">    tensor([[2.2282, 1.8667, 2.4338, 1.7540, 1.4511],</span>
<span class="sd">            [1.0608, 1.2080, 2.1780, 2.7849, 2.5420]])</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">SumPooling</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>

<div class="viewcode-block" id="SumPooling.forward">
<a class="viewcode-back" href="../../../../generated/dgl.nn.pytorch.glob.SumPooling.html#dgl.nn.pytorch.glob.SumPooling.forward">[docs]</a>
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">graph</span><span class="p">,</span> <span class="n">feat</span><span class="p">):</span>
<span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>

<span class="sd">        Compute sum pooling.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        graph : DGLGraph</span>
<span class="sd">            a DGLGraph or a batch of DGLGraphs</span>
<span class="sd">        feat : torch.Tensor</span>
<span class="sd">            The input feature with shape :math:`(N, D)`, where :math:`N` is the number</span>
<span class="sd">            of nodes in the graph, and :math:`D` means the size of features.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        torch.Tensor</span>
<span class="sd">            The output feature with shape :math:`(B, D)`, where :math:`B` refers to the</span>
<span class="sd">            batch size of input graphs.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">with</span> <span class="n">graph</span><span class="o">.</span><span class="n">local_scope</span><span class="p">():</span>
            <span class="n">graph</span><span class="o">.</span><span class="n">ndata</span><span class="p">[</span><span class="s2">&quot;h&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">feat</span>
            <span class="n">readout</span> <span class="o">=</span> <span class="n">sum_nodes</span><span class="p">(</span><span class="n">graph</span><span class="p">,</span> <span class="s2">&quot;h&quot;</span><span class="p">)</span>
            <span class="k">return</span> <span class="n">readout</span></div>
</div>



<div class="viewcode-block" id="AvgPooling">
<a class="viewcode-back" href="../../../../generated/dgl.nn.pytorch.glob.AvgPooling.html#dgl.nn.pytorch.glob.AvgPooling">[docs]</a>
<span class="k">class</span> <span class="nc">AvgPooling</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Apply average pooling over the nodes in a graph.</span>

<span class="sd">    .. math::</span>
<span class="sd">        r^{(i)} = \frac{1}{N_i}\sum_{k=1}^{N_i} x^{(i)}_k</span>

<span class="sd">    Notes</span>
<span class="sd">    -----</span>
<span class="sd">        Input: Could be one graph, or a batch of graphs. If using a batch of graphs,</span>
<span class="sd">        make sure nodes in all graphs have the same feature size, and concatenate</span>
<span class="sd">        nodes&#39; feature together as the input.</span>

<span class="sd">    Examples</span>
<span class="sd">    --------</span>
<span class="sd">    The following example uses PyTorch backend.</span>

<span class="sd">    &gt;&gt;&gt; import dgl</span>
<span class="sd">    &gt;&gt;&gt; import torch as th</span>
<span class="sd">    &gt;&gt;&gt; from dgl.nn import AvgPooling</span>
<span class="sd">    &gt;&gt;&gt;</span>
<span class="sd">    &gt;&gt;&gt; g1 = dgl.rand_graph(3, 4)  # g1 is a random graph with 3 nodes and 4 edges</span>
<span class="sd">    &gt;&gt;&gt; g1_node_feats = th.rand(3, 5)  # feature size is 5</span>
<span class="sd">    &gt;&gt;&gt; g1_node_feats</span>
<span class="sd">    tensor([[0.8948, 0.0699, 0.9137, 0.7567, 0.3637],</span>
<span class="sd">            [0.8137, 0.8938, 0.8377, 0.4249, 0.6118],</span>
<span class="sd">            [0.5197, 0.9030, 0.6825, 0.5725, 0.4755]])</span>
<span class="sd">    &gt;&gt;&gt;</span>
<span class="sd">    &gt;&gt;&gt; g2 = dgl.rand_graph(4, 6)  # g2 is a random graph with 4 nodes and 6 edges</span>
<span class="sd">    &gt;&gt;&gt; g2_node_feats = th.rand(4, 5)  # feature size is 5</span>
<span class="sd">    &gt;&gt;&gt; g2_node_feats</span>
<span class="sd">    tensor([[0.2053, 0.2426, 0.4111, 0.9028, 0.5658],</span>
<span class="sd">            [0.5278, 0.6365, 0.9990, 0.2351, 0.8945],</span>
<span class="sd">            [0.3134, 0.0580, 0.4349, 0.7949, 0.3891],</span>
<span class="sd">            [0.0142, 0.2709, 0.3330, 0.8521, 0.6925]])</span>
<span class="sd">    &gt;&gt;&gt;</span>
<span class="sd">    &gt;&gt;&gt; avgpool = AvgPooling()  # create an average pooling layer</span>

<span class="sd">    Case 1: Input single graph</span>

<span class="sd">    &gt;&gt;&gt; avgpool(g1, g1_node_feats)</span>
<span class="sd">    tensor([[0.7427, 0.6222, 0.8113, 0.5847, 0.4837]])</span>

<span class="sd">    Case 2: Input a batch of graphs</span>

<span class="sd">    Build a batch of DGL graphs and concatenate all graphs&#39; note features into one tensor.</span>

<span class="sd">    &gt;&gt;&gt; batch_g = dgl.batch([g1, g2])</span>
<span class="sd">    &gt;&gt;&gt; batch_f = th.cat([g1_node_feats, g2_node_feats])</span>
<span class="sd">    &gt;&gt;&gt;</span>
<span class="sd">    &gt;&gt;&gt; avgpool(batch_g, batch_f)</span>
<span class="sd">    tensor([[0.7427, 0.6222, 0.8113, 0.5847, 0.4837],</span>
<span class="sd">            [0.2652, 0.3020, 0.5445, 0.6962, 0.6355]])</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">AvgPooling</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>

<div class="viewcode-block" id="AvgPooling.forward">
<a class="viewcode-back" href="../../../../generated/dgl.nn.pytorch.glob.AvgPooling.html#dgl.nn.pytorch.glob.AvgPooling.forward">[docs]</a>
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">graph</span><span class="p">,</span> <span class="n">feat</span><span class="p">):</span>
<span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>

<span class="sd">        Compute average pooling.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        graph : DGLGraph</span>
<span class="sd">            A DGLGraph or a batch of DGLGraphs.</span>
<span class="sd">        feat : torch.Tensor</span>
<span class="sd">            The input feature with shape :math:`(N, D)`, where :math:`N` is the number</span>
<span class="sd">            of nodes in the graph, and :math:`D` means the size of features.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        torch.Tensor</span>
<span class="sd">            The output feature with shape :math:`(B, D)`, where</span>
<span class="sd">            :math:`B` refers to the batch size of input graphs.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">with</span> <span class="n">graph</span><span class="o">.</span><span class="n">local_scope</span><span class="p">():</span>
            <span class="n">graph</span><span class="o">.</span><span class="n">ndata</span><span class="p">[</span><span class="s2">&quot;h&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">feat</span>
            <span class="n">readout</span> <span class="o">=</span> <span class="n">mean_nodes</span><span class="p">(</span><span class="n">graph</span><span class="p">,</span> <span class="s2">&quot;h&quot;</span><span class="p">)</span>
            <span class="k">return</span> <span class="n">readout</span></div>
</div>



<div class="viewcode-block" id="MaxPooling">
<a class="viewcode-back" href="../../../../generated/dgl.nn.pytorch.glob.MaxPooling.html#dgl.nn.pytorch.glob.MaxPooling">[docs]</a>
<span class="k">class</span> <span class="nc">MaxPooling</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Apply max pooling over the nodes in a graph.</span>

<span class="sd">    .. math::</span>
<span class="sd">        r^{(i)} = \max_{k=1}^{N_i}\left( x^{(i)}_k \right)</span>

<span class="sd">    Notes</span>
<span class="sd">    -----</span>
<span class="sd">        Input: Could be one graph, or a batch of graphs. If using a batch of graphs,</span>
<span class="sd">        make sure nodes in all graphs have the same feature size, and concatenate</span>
<span class="sd">        nodes&#39; feature together as the input.</span>

<span class="sd">    Examples</span>
<span class="sd">    --------</span>
<span class="sd">    The following example uses PyTorch backend.</span>

<span class="sd">    &gt;&gt;&gt; import dgl</span>
<span class="sd">    &gt;&gt;&gt; import torch as th</span>
<span class="sd">    &gt;&gt;&gt; from dgl.nn import MaxPooling</span>
<span class="sd">    &gt;&gt;&gt;</span>
<span class="sd">    &gt;&gt;&gt; g1 = dgl.rand_graph(3, 4)  # g1 is a random graph with 3 nodes and 4 edges</span>
<span class="sd">    &gt;&gt;&gt; g1_node_feats = th.rand(3, 5)  # feature size is 5</span>
<span class="sd">    &gt;&gt;&gt; g1_node_feats</span>
<span class="sd">    tensor([[0.8948, 0.0699, 0.9137, 0.7567, 0.3637],</span>
<span class="sd">            [0.8137, 0.8938, 0.8377, 0.4249, 0.6118],</span>
<span class="sd">            [0.5197, 0.9030, 0.6825, 0.5725, 0.4755]])</span>
<span class="sd">    &gt;&gt;&gt;</span>
<span class="sd">    &gt;&gt;&gt; g2 = dgl.rand_graph(4, 6)  # g2 is a random graph with 4 nodes and 6 edges</span>
<span class="sd">    &gt;&gt;&gt; g2_node_feats = th.rand(4, 5)  # feature size is 5</span>
<span class="sd">    &gt;&gt;&gt; g2_node_feats</span>
<span class="sd">    tensor([[0.2053, 0.2426, 0.4111, 0.9028, 0.5658],</span>
<span class="sd">            [0.5278, 0.6365, 0.9990, 0.2351, 0.8945],</span>
<span class="sd">            [0.3134, 0.0580, 0.4349, 0.7949, 0.3891],</span>
<span class="sd">            [0.0142, 0.2709, 0.3330, 0.8521, 0.6925]])</span>
<span class="sd">    &gt;&gt;&gt;</span>
<span class="sd">    &gt;&gt;&gt; maxpool = MaxPooling()  # create a max pooling layer</span>

<span class="sd">    Case 1: Input a single graph</span>

<span class="sd">    &gt;&gt;&gt; maxpool(g1, g1_node_feats)</span>
<span class="sd">    tensor([[0.8948, 0.9030, 0.9137, 0.7567, 0.6118]])</span>

<span class="sd">    Case 2: Input a batch of graphs</span>

<span class="sd">    Build a batch of DGL graphs and concatenate all graphs&#39; node features into one tensor.</span>

<span class="sd">    &gt;&gt;&gt; batch_g = dgl.batch([g1, g2])</span>
<span class="sd">    &gt;&gt;&gt; batch_f = th.cat([g1_node_feats, g2_node_feats])</span>
<span class="sd">    &gt;&gt;&gt;</span>
<span class="sd">    &gt;&gt;&gt; maxpool(batch_g, batch_f)</span>
<span class="sd">    tensor([[0.8948, 0.9030, 0.9137, 0.7567, 0.6118],</span>
<span class="sd">            [0.5278, 0.6365, 0.9990, 0.9028, 0.8945]])</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">MaxPooling</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>

<div class="viewcode-block" id="MaxPooling.forward">
<a class="viewcode-back" href="../../../../generated/dgl.nn.pytorch.glob.MaxPooling.html#dgl.nn.pytorch.glob.MaxPooling.forward">[docs]</a>
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">graph</span><span class="p">,</span> <span class="n">feat</span><span class="p">):</span>
<span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Compute max pooling.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        graph : DGLGraph</span>
<span class="sd">            A DGLGraph or a batch of DGLGraphs.</span>
<span class="sd">        feat : torch.Tensor</span>
<span class="sd">            The input feature with shape :math:`(N, *)`, where</span>
<span class="sd">            :math:`N` is the number of nodes in the graph.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        torch.Tensor</span>
<span class="sd">            The output feature with shape :math:`(B, *)`, where</span>
<span class="sd">            :math:`B` refers to the batch size.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">with</span> <span class="n">graph</span><span class="o">.</span><span class="n">local_scope</span><span class="p">():</span>
            <span class="n">graph</span><span class="o">.</span><span class="n">ndata</span><span class="p">[</span><span class="s2">&quot;h&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">feat</span>
            <span class="n">readout</span> <span class="o">=</span> <span class="n">max_nodes</span><span class="p">(</span><span class="n">graph</span><span class="p">,</span> <span class="s2">&quot;h&quot;</span><span class="p">)</span>
            <span class="k">return</span> <span class="n">readout</span></div>
</div>



<div class="viewcode-block" id="SortPooling">
<a class="viewcode-back" href="../../../../generated/dgl.nn.pytorch.glob.SortPooling.html#dgl.nn.pytorch.glob.SortPooling">[docs]</a>
<span class="k">class</span> <span class="nc">SortPooling</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Sort Pooling from `An End-to-End Deep Learning Architecture for Graph Classification</span>
<span class="sd">    &lt;https://www.cse.wustl.edu/~ychen/public/DGCNN.pdf&gt;`__</span>

<span class="sd">    It first sorts the node features in ascending order along the feature dimension,</span>
<span class="sd">    and selects the sorted features of top-k nodes (ranked by the largest value of each node).</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    k : int</span>
<span class="sd">        The number of nodes to hold for each graph.</span>

<span class="sd">    Notes</span>
<span class="sd">    -----</span>
<span class="sd">        Input: Could be one graph, or a batch of graphs. If using a batch of graphs,</span>
<span class="sd">        make sure nodes in all graphs have the same feature size, and concatenate</span>
<span class="sd">        nodes&#39; feature together as the input.</span>

<span class="sd">    Examples</span>
<span class="sd">    --------</span>

<span class="sd">    &gt;&gt;&gt; import dgl</span>
<span class="sd">    &gt;&gt;&gt; import torch as th</span>
<span class="sd">    &gt;&gt;&gt; from dgl.nn import SortPooling</span>
<span class="sd">    &gt;&gt;&gt;</span>
<span class="sd">    &gt;&gt;&gt; g1 = dgl.rand_graph(3, 4)  # g1 is a random graph with 3 nodes and 4 edges</span>
<span class="sd">    &gt;&gt;&gt; g1_node_feats = th.rand(3, 5)  # feature size is 5</span>
<span class="sd">    &gt;&gt;&gt; g1_node_feats</span>
<span class="sd">    tensor([[0.8948, 0.0699, 0.9137, 0.7567, 0.3637],</span>
<span class="sd">            [0.8137, 0.8938, 0.8377, 0.4249, 0.6118],</span>
<span class="sd">            [0.5197, 0.9030, 0.6825, 0.5725, 0.4755]])</span>
<span class="sd">    &gt;&gt;&gt;</span>
<span class="sd">    &gt;&gt;&gt; g2 = dgl.rand_graph(4, 6)  # g2 is a random graph with 4 nodes and 6 edges</span>
<span class="sd">    &gt;&gt;&gt; g2_node_feats = th.rand(4, 5)  # feature size is 5</span>
<span class="sd">    &gt;&gt;&gt; g2_node_feats</span>
<span class="sd">    tensor([[0.2053, 0.2426, 0.4111, 0.9028, 0.5658],</span>
<span class="sd">            [0.5278, 0.6365, 0.9990, 0.2351, 0.8945],</span>
<span class="sd">            [0.3134, 0.0580, 0.4349, 0.7949, 0.3891],</span>
<span class="sd">            [0.0142, 0.2709, 0.3330, 0.8521, 0.6925]])</span>
<span class="sd">    &gt;&gt;&gt;</span>
<span class="sd">    &gt;&gt;&gt; sortpool = SortPooling(k=2)  # create a sort pooling layer</span>

<span class="sd">    Case 1: Input a single graph</span>

<span class="sd">    &gt;&gt;&gt; sortpool(g1, g1_node_feats)</span>
<span class="sd">    tensor([[0.0699, 0.3637, 0.7567, 0.8948, 0.9137, 0.4755, 0.5197, 0.5725, 0.6825,</span>
<span class="sd">             0.9030]])</span>

<span class="sd">    Case 2: Input a batch of graphs</span>

<span class="sd">    Build a batch of DGL graphs and concatenate all graphs&#39; node features into one tensor.</span>

<span class="sd">    &gt;&gt;&gt; batch_g = dgl.batch([g1, g2])</span>
<span class="sd">    &gt;&gt;&gt; batch_f = th.cat([g1_node_feats, g2_node_feats])</span>
<span class="sd">    &gt;&gt;&gt;</span>
<span class="sd">    &gt;&gt;&gt; sortpool(batch_g, batch_f)</span>
<span class="sd">    tensor([[0.0699, 0.3637, 0.7567, 0.8948, 0.9137, 0.4755, 0.5197, 0.5725, 0.6825,</span>
<span class="sd">             0.9030],</span>
<span class="sd">            [0.2351, 0.5278, 0.6365, 0.8945, 0.9990, 0.2053, 0.2426, 0.4111, 0.5658,</span>
<span class="sd">             0.9028]])</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">k</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">SortPooling</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">k</span> <span class="o">=</span> <span class="n">k</span>

<div class="viewcode-block" id="SortPooling.forward">
<a class="viewcode-back" href="../../../../generated/dgl.nn.pytorch.glob.SortPooling.html#dgl.nn.pytorch.glob.SortPooling.forward">[docs]</a>
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">graph</span><span class="p">,</span> <span class="n">feat</span><span class="p">):</span>
<span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>

<span class="sd">        Compute sort pooling.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        graph : DGLGraph</span>
<span class="sd">            A DGLGraph or a batch of DGLGraphs.</span>
<span class="sd">        feat : torch.Tensor</span>
<span class="sd">            The input node feature with shape :math:`(N, D)`, where :math:`N` is the</span>
<span class="sd">            number of nodes in the graph, and :math:`D` means the size of features.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        torch.Tensor</span>
<span class="sd">            The output feature with shape :math:`(B, k * D)`, where :math:`B` refers</span>
<span class="sd">            to the batch size of input graphs.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">with</span> <span class="n">graph</span><span class="o">.</span><span class="n">local_scope</span><span class="p">():</span>
            <span class="c1"># Sort the feature of each node in ascending order.</span>
            <span class="n">feat</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">feat</span><span class="o">.</span><span class="n">sort</span><span class="p">(</span><span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
            <span class="n">graph</span><span class="o">.</span><span class="n">ndata</span><span class="p">[</span><span class="s2">&quot;h&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">feat</span>
            <span class="c1"># Sort nodes according to their last features.</span>
            <span class="n">ret</span> <span class="o">=</span> <span class="n">topk_nodes</span><span class="p">(</span><span class="n">graph</span><span class="p">,</span> <span class="s2">&quot;h&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">k</span><span class="p">,</span> <span class="n">sortby</span><span class="o">=-</span><span class="mi">1</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">view</span><span class="p">(</span>
                <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">k</span> <span class="o">*</span> <span class="n">feat</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
            <span class="p">)</span>
            <span class="k">return</span> <span class="n">ret</span></div>
</div>



<div class="viewcode-block" id="GlobalAttentionPooling">
<a class="viewcode-back" href="../../../../generated/dgl.nn.pytorch.glob.GlobalAttentionPooling.html#dgl.nn.pytorch.glob.GlobalAttentionPooling">[docs]</a>
<span class="k">class</span> <span class="nc">GlobalAttentionPooling</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Global Attention Pooling from `Gated Graph Sequence Neural Networks</span>
<span class="sd">    &lt;https://arxiv.org/abs/1511.05493&gt;`__</span>

<span class="sd">    .. math::</span>
<span class="sd">        r^{(i)} = \sum_{k=1}^{N_i}\mathrm{softmax}\left(f_{gate}</span>
<span class="sd">        \left(x^{(i)}_k\right)\right) f_{feat}\left(x^{(i)}_k\right)</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    gate_nn : torch.nn.Module</span>
<span class="sd">        A neural network that computes attention scores for each feature.</span>
<span class="sd">    feat_nn : torch.nn.Module, optional</span>
<span class="sd">        A neural network applied to each feature before combining them with attention</span>
<span class="sd">        scores.</span>

<span class="sd">    Examples</span>
<span class="sd">    --------</span>
<span class="sd">    The following example uses PyTorch backend.</span>

<span class="sd">    &gt;&gt;&gt; import dgl</span>
<span class="sd">    &gt;&gt;&gt; import torch as th</span>
<span class="sd">    &gt;&gt;&gt; from dgl.nn import GlobalAttentionPooling</span>
<span class="sd">    &gt;&gt;&gt;</span>
<span class="sd">    &gt;&gt;&gt; g1 = dgl.rand_graph(3, 4)  # g1 is a random graph with 3 nodes and 4 edges</span>
<span class="sd">    &gt;&gt;&gt; g1_node_feats = th.rand(3, 5)  # feature size is 5</span>
<span class="sd">    &gt;&gt;&gt; g1_node_feats</span>
<span class="sd">    tensor([[0.8948, 0.0699, 0.9137, 0.7567, 0.3637],</span>
<span class="sd">            [0.8137, 0.8938, 0.8377, 0.4249, 0.6118],</span>
<span class="sd">            [0.5197, 0.9030, 0.6825, 0.5725, 0.4755]])</span>
<span class="sd">    &gt;&gt;&gt;</span>
<span class="sd">    &gt;&gt;&gt; g2 = dgl.rand_graph(4, 6)  # g2 is a random graph with 4 nodes and 6 edges</span>
<span class="sd">    &gt;&gt;&gt; g2_node_feats = th.rand(4, 5)  # feature size is 5</span>
<span class="sd">    &gt;&gt;&gt; g2_node_feats</span>
<span class="sd">    tensor([[0.2053, 0.2426, 0.4111, 0.9028, 0.5658],</span>
<span class="sd">            [0.5278, 0.6365, 0.9990, 0.2351, 0.8945],</span>
<span class="sd">            [0.3134, 0.0580, 0.4349, 0.7949, 0.3891],</span>
<span class="sd">            [0.0142, 0.2709, 0.3330, 0.8521, 0.6925]])</span>
<span class="sd">    &gt;&gt;&gt;</span>
<span class="sd">    &gt;&gt;&gt; gate_nn = th.nn.Linear(5, 1)  # the gate layer that maps node feature to scalar</span>
<span class="sd">    &gt;&gt;&gt; gap = GlobalAttentionPooling(gate_nn)  # create a Global Attention Pooling layer</span>

<span class="sd">    Case 1: Input a single graph</span>

<span class="sd">    &gt;&gt;&gt; gap(g1, g1_node_feats)</span>
<span class="sd">    tensor([[0.7410, 0.6032, 0.8111, 0.5942, 0.4762]],</span>
<span class="sd">           grad_fn=&lt;SegmentReduceBackward&gt;)</span>

<span class="sd">    Case 2: Input a batch of graphs</span>

<span class="sd">    Build a batch of DGL graphs and concatenate all graphs&#39; node features into one tensor.</span>

<span class="sd">    &gt;&gt;&gt; batch_g = dgl.batch([g1, g2])</span>
<span class="sd">    &gt;&gt;&gt; batch_f = th.cat([g1_node_feats, g2_node_feats], 0)</span>
<span class="sd">    &gt;&gt;&gt;</span>
<span class="sd">    &gt;&gt;&gt; gap(batch_g, batch_f)</span>
<span class="sd">    tensor([[0.7410, 0.6032, 0.8111, 0.5942, 0.4762],</span>
<span class="sd">            [0.2417, 0.2743, 0.5054, 0.7356, 0.6146]],</span>
<span class="sd">           grad_fn=&lt;SegmentReduceBackward&gt;)</span>
<span class="sd">    Notes</span>
<span class="sd">    -----</span>
<span class="sd">    See our `GGNN example &lt;https://github.com/dmlc/dgl/tree/master/examples/pytorch/ggnn&gt;`_</span>
<span class="sd">    on how to use GatedGraphConv and GlobalAttentionPooling layer to build a Graph Neural</span>
<span class="sd">    Networks that can solve Soduku.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">gate_nn</span><span class="p">,</span> <span class="n">feat_nn</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">GlobalAttentionPooling</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">gate_nn</span> <span class="o">=</span> <span class="n">gate_nn</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">feat_nn</span> <span class="o">=</span> <span class="n">feat_nn</span>

<div class="viewcode-block" id="GlobalAttentionPooling.forward">
<a class="viewcode-back" href="../../../../generated/dgl.nn.pytorch.glob.GlobalAttentionPooling.html#dgl.nn.pytorch.glob.GlobalAttentionPooling.forward">[docs]</a>
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">graph</span><span class="p">,</span> <span class="n">feat</span><span class="p">,</span> <span class="n">get_attention</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
<span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>

<span class="sd">        Compute global attention pooling.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        graph : DGLGraph</span>
<span class="sd">            A DGLGraph or a batch of DGLGraphs.</span>
<span class="sd">        feat : torch.Tensor</span>
<span class="sd">            The input node feature with shape :math:`(N, D)` where :math:`N` is the</span>
<span class="sd">            number of nodes in the graph, and :math:`D` means the size of features.</span>
<span class="sd">        get_attention : bool, optional</span>
<span class="sd">            Whether to return the attention values from gate_nn. Default to False.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        torch.Tensor</span>
<span class="sd">            The output feature with shape :math:`(B, D)`, where :math:`B` refers</span>
<span class="sd">            to the batch size.</span>
<span class="sd">        torch.Tensor, optional</span>
<span class="sd">            The attention values of shape :math:`(N, 1)`, where :math:`N` is the number of</span>
<span class="sd">            nodes in the graph. This is returned only when :attr:`get_attention` is ``True``.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">with</span> <span class="n">graph</span><span class="o">.</span><span class="n">local_scope</span><span class="p">():</span>
            <span class="n">gate</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">gate_nn</span><span class="p">(</span><span class="n">feat</span><span class="p">)</span>
            <span class="k">assert</span> <span class="p">(</span>
                <span class="n">gate</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">==</span> <span class="mi">1</span>
            <span class="p">),</span> <span class="s2">&quot;The output of gate_nn should have size 1 at the last axis.&quot;</span>
            <span class="n">feat</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">feat_nn</span><span class="p">(</span><span class="n">feat</span><span class="p">)</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">feat_nn</span> <span class="k">else</span> <span class="n">feat</span>

            <span class="n">graph</span><span class="o">.</span><span class="n">ndata</span><span class="p">[</span><span class="s2">&quot;gate&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">gate</span>
            <span class="n">gate</span> <span class="o">=</span> <span class="n">softmax_nodes</span><span class="p">(</span><span class="n">graph</span><span class="p">,</span> <span class="s2">&quot;gate&quot;</span><span class="p">)</span>
            <span class="n">graph</span><span class="o">.</span><span class="n">ndata</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;gate&quot;</span><span class="p">)</span>

            <span class="n">graph</span><span class="o">.</span><span class="n">ndata</span><span class="p">[</span><span class="s2">&quot;r&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">feat</span> <span class="o">*</span> <span class="n">gate</span>
            <span class="n">readout</span> <span class="o">=</span> <span class="n">sum_nodes</span><span class="p">(</span><span class="n">graph</span><span class="p">,</span> <span class="s2">&quot;r&quot;</span><span class="p">)</span>
            <span class="n">graph</span><span class="o">.</span><span class="n">ndata</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;r&quot;</span><span class="p">)</span>

            <span class="k">if</span> <span class="n">get_attention</span><span class="p">:</span>
                <span class="k">return</span> <span class="n">readout</span><span class="p">,</span> <span class="n">gate</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="k">return</span> <span class="n">readout</span></div>
</div>



<div class="viewcode-block" id="Set2Set">
<a class="viewcode-back" href="../../../../generated/dgl.nn.pytorch.glob.Set2Set.html#dgl.nn.pytorch.glob.Set2Set">[docs]</a>
<span class="k">class</span> <span class="nc">Set2Set</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Set2Set operator from `Order Matters: Sequence to sequence for sets</span>
<span class="sd">    &lt;https://arxiv.org/pdf/1511.06391.pdf&gt;`__</span>

<span class="sd">    For each individual graph in the batch, set2set computes</span>

<span class="sd">    .. math::</span>
<span class="sd">        q_t &amp;= \mathrm{LSTM} (q^*_{t-1})</span>

<span class="sd">        \alpha_{i,t} &amp;= \mathrm{softmax}(x_i \cdot q_t)</span>

<span class="sd">        r_t &amp;= \sum_{i=1}^N \alpha_{i,t} x_i</span>

<span class="sd">        q^*_t &amp;= q_t \Vert r_t</span>

<span class="sd">    for this graph.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    input_dim : int</span>
<span class="sd">        The size of each input sample.</span>
<span class="sd">    n_iters : int</span>
<span class="sd">        The number of iterations.</span>
<span class="sd">    n_layers : int</span>
<span class="sd">        The number of recurrent layers.</span>

<span class="sd">    Examples</span>
<span class="sd">    --------</span>
<span class="sd">    The following example uses PyTorch backend.</span>

<span class="sd">    &gt;&gt;&gt; import dgl</span>
<span class="sd">    &gt;&gt;&gt; import torch as th</span>
<span class="sd">    &gt;&gt;&gt; from dgl.nn import Set2Set</span>
<span class="sd">    &gt;&gt;&gt;</span>
<span class="sd">    &gt;&gt;&gt; g1 = dgl.rand_graph(3, 4)  # g1 is a random graph with 3 nodes and 4 edges</span>
<span class="sd">    &gt;&gt;&gt; g1_node_feats = th.rand(3, 5)  # feature size is 5</span>
<span class="sd">    &gt;&gt;&gt; g1_node_feats</span>
<span class="sd">    tensor([[0.8948, 0.0699, 0.9137, 0.7567, 0.3637],</span>
<span class="sd">            [0.8137, 0.8938, 0.8377, 0.4249, 0.6118],</span>
<span class="sd">            [0.5197, 0.9030, 0.6825, 0.5725, 0.4755]])</span>
<span class="sd">    &gt;&gt;&gt;</span>
<span class="sd">    &gt;&gt;&gt; g2 = dgl.rand_graph(4, 6)  # g2 is a random graph with 4 nodes and 6 edges</span>
<span class="sd">    &gt;&gt;&gt; g2_node_feats = th.rand(4, 5)  # feature size is 5</span>
<span class="sd">    &gt;&gt;&gt; g2_node_feats</span>
<span class="sd">    tensor([[0.2053, 0.2426, 0.4111, 0.9028, 0.5658],</span>
<span class="sd">            [0.5278, 0.6365, 0.9990, 0.2351, 0.8945],</span>
<span class="sd">            [0.3134, 0.0580, 0.4349, 0.7949, 0.3891],</span>
<span class="sd">            [0.0142, 0.2709, 0.3330, 0.8521, 0.6925]])</span>
<span class="sd">    &gt;&gt;&gt;</span>
<span class="sd">    &gt;&gt;&gt; s2s = Set2Set(5, 2, 1)  # create a Set2Set layer(n_iters=2, n_layers=1)</span>

<span class="sd">    Case 1: Input a single graph</span>

<span class="sd">    &gt;&gt;&gt; s2s(g1, g1_node_feats)</span>
<span class="sd">        tensor([[-0.0235, -0.2291,  0.2654,  0.0376,  0.1349,  0.7560,  0.5822,  0.8199,</span>
<span class="sd">                  0.5960,  0.4760]], grad_fn=&lt;CatBackward&gt;)</span>

<span class="sd">    Case 2: Input a batch of graphs</span>

<span class="sd">    Build a batch of DGL graphs and concatenate all graphs&#39; node features into one tensor.</span>

<span class="sd">    &gt;&gt;&gt; batch_g = dgl.batch([g1, g2])</span>
<span class="sd">    &gt;&gt;&gt; batch_f = th.cat([g1_node_feats, g2_node_feats], 0)</span>
<span class="sd">    &gt;&gt;&gt;</span>
<span class="sd">    &gt;&gt;&gt; s2s(batch_g, batch_f)</span>
<span class="sd">    tensor([[-0.0235, -0.2291,  0.2654,  0.0376,  0.1349,  0.7560,  0.5822,  0.8199,</span>
<span class="sd">              0.5960,  0.4760],</span>
<span class="sd">            [-0.0483, -0.2010,  0.2324,  0.0145,  0.1361,  0.2703,  0.3078,  0.5529,</span>
<span class="sd">              0.6876,  0.6399]], grad_fn=&lt;CatBackward&gt;)</span>

<span class="sd">    Notes</span>
<span class="sd">    -----</span>
<span class="sd">    Set2Set is widely used in molecular property predictions, see</span>
<span class="sd">    `dgl-lifesci&#39;s MPNN example &lt;https://github.com/awslabs/dgl-lifesci/blob/</span>
<span class="sd">    ecd95c905479ec048097777039cf9a19cfdcf223/python/dgllife/model/model_zoo/</span>
<span class="sd">    mpnn_predictor.py&gt;`__</span>
<span class="sd">    on how to use DGL&#39;s Set2Set layer in graph property prediction applications.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_dim</span><span class="p">,</span> <span class="n">n_iters</span><span class="p">,</span> <span class="n">n_layers</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">Set2Set</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">input_dim</span> <span class="o">=</span> <span class="n">input_dim</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">output_dim</span> <span class="o">=</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">input_dim</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">n_iters</span> <span class="o">=</span> <span class="n">n_iters</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">n_layers</span> <span class="o">=</span> <span class="n">n_layers</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">lstm</span> <span class="o">=</span> <span class="n">th</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">LSTM</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">output_dim</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">input_dim</span><span class="p">,</span> <span class="n">n_layers</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">reset_parameters</span><span class="p">()</span>

<div class="viewcode-block" id="Set2Set.reset_parameters">
<a class="viewcode-back" href="../../../../generated/dgl.nn.pytorch.glob.Set2Set.html#dgl.nn.pytorch.glob.Set2Set.reset_parameters">[docs]</a>
    <span class="k">def</span> <span class="nf">reset_parameters</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Reinitialize learnable parameters.&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">lstm</span><span class="o">.</span><span class="n">reset_parameters</span><span class="p">()</span></div>


<div class="viewcode-block" id="Set2Set.forward">
<a class="viewcode-back" href="../../../../generated/dgl.nn.pytorch.glob.Set2Set.html#dgl.nn.pytorch.glob.Set2Set.forward">[docs]</a>
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">graph</span><span class="p">,</span> <span class="n">feat</span><span class="p">):</span>
<span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Compute set2set pooling.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        graph : DGLGraph</span>
<span class="sd">            The input graph.</span>
<span class="sd">        feat : torch.Tensor</span>
<span class="sd">            The input feature with shape :math:`(N, D)` where  :math:`N` is the</span>
<span class="sd">            number of nodes in the graph, and :math:`D` means the size of features.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        torch.Tensor</span>
<span class="sd">            The output feature with shape :math:`(B, D)`, where :math:`B` refers to</span>
<span class="sd">            the batch size, and :math:`D` means the size of features.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">with</span> <span class="n">graph</span><span class="o">.</span><span class="n">local_scope</span><span class="p">():</span>
            <span class="n">batch_size</span> <span class="o">=</span> <span class="n">graph</span><span class="o">.</span><span class="n">batch_size</span>

            <span class="n">h</span> <span class="o">=</span> <span class="p">(</span>
                <span class="n">feat</span><span class="o">.</span><span class="n">new_zeros</span><span class="p">((</span><span class="bp">self</span><span class="o">.</span><span class="n">n_layers</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">input_dim</span><span class="p">)),</span>
                <span class="n">feat</span><span class="o">.</span><span class="n">new_zeros</span><span class="p">((</span><span class="bp">self</span><span class="o">.</span><span class="n">n_layers</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">input_dim</span><span class="p">)),</span>
            <span class="p">)</span>

            <span class="n">q_star</span> <span class="o">=</span> <span class="n">feat</span><span class="o">.</span><span class="n">new_zeros</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">output_dim</span><span class="p">)</span>

            <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">n_iters</span><span class="p">):</span>
                <span class="n">q</span><span class="p">,</span> <span class="n">h</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">lstm</span><span class="p">(</span><span class="n">q_star</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">),</span> <span class="n">h</span><span class="p">)</span>
                <span class="n">q</span> <span class="o">=</span> <span class="n">q</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">input_dim</span><span class="p">)</span>
                <span class="n">e</span> <span class="o">=</span> <span class="p">(</span><span class="n">feat</span> <span class="o">*</span> <span class="n">broadcast_nodes</span><span class="p">(</span><span class="n">graph</span><span class="p">,</span> <span class="n">q</span><span class="p">))</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
                <span class="n">graph</span><span class="o">.</span><span class="n">ndata</span><span class="p">[</span><span class="s2">&quot;e&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">e</span>
                <span class="n">alpha</span> <span class="o">=</span> <span class="n">softmax_nodes</span><span class="p">(</span><span class="n">graph</span><span class="p">,</span> <span class="s2">&quot;e&quot;</span><span class="p">)</span>
                <span class="n">graph</span><span class="o">.</span><span class="n">ndata</span><span class="p">[</span><span class="s2">&quot;r&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">feat</span> <span class="o">*</span> <span class="n">alpha</span>
                <span class="n">readout</span> <span class="o">=</span> <span class="n">sum_nodes</span><span class="p">(</span><span class="n">graph</span><span class="p">,</span> <span class="s2">&quot;r&quot;</span><span class="p">)</span>
                <span class="n">q_star</span> <span class="o">=</span> <span class="n">th</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">q</span><span class="p">,</span> <span class="n">readout</span><span class="p">],</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>

            <span class="k">return</span> <span class="n">q_star</span></div>


    <span class="k">def</span> <span class="nf">extra_repr</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Set the extra representation of the module.</span>
<span class="sd">        which will come into effect when printing the model.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">summary</span> <span class="o">=</span> <span class="s2">&quot;n_iters=</span><span class="si">{n_iters}</span><span class="s2">&quot;</span>
        <span class="k">return</span> <span class="n">summary</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="o">**</span><span class="bp">self</span><span class="o">.</span><span class="vm">__dict__</span><span class="p">)</span></div>



<span class="k">def</span> <span class="nf">_gen_mask</span><span class="p">(</span><span class="n">lengths_x</span><span class="p">,</span> <span class="n">lengths_y</span><span class="p">,</span> <span class="n">max_len_x</span><span class="p">,</span> <span class="n">max_len_y</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Generate binary mask array for given x and y input pairs.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    lengths_x : Tensor</span>
<span class="sd">        The int tensor indicates the segment information of x.</span>
<span class="sd">    lengths_y : Tensor</span>
<span class="sd">        The int tensor indicates the segment information of y.</span>
<span class="sd">    max_len_x : int</span>
<span class="sd">        The maximum element in lengths_x.</span>
<span class="sd">    max_len_y : int</span>
<span class="sd">        The maximum element in lengths_y.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    Tensor</span>
<span class="sd">        the mask tensor with shape (batch_size, 1, max_len_x, max_len_y)</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">device</span> <span class="o">=</span> <span class="n">lengths_x</span><span class="o">.</span><span class="n">device</span>
    <span class="c1"># x_mask: (batch_size, max_len_x)</span>
    <span class="n">x_mask</span> <span class="o">=</span> <span class="n">th</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">max_len_x</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span>
        <span class="mi">0</span>
    <span class="p">)</span> <span class="o">&lt;</span> <span class="n">lengths_x</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
    <span class="c1"># y_mask: (batch_size, max_len_y)</span>
    <span class="n">y_mask</span> <span class="o">=</span> <span class="n">th</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">max_len_y</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span>
        <span class="mi">0</span>
    <span class="p">)</span> <span class="o">&lt;</span> <span class="n">lengths_y</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
    <span class="c1"># mask: (batch_size, 1, max_len_x, max_len_y)</span>
    <span class="n">mask</span> <span class="o">=</span> <span class="p">(</span><span class="n">x_mask</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span> <span class="o">&amp;</span> <span class="n">y_mask</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">))</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">mask</span>


<span class="k">class</span> <span class="nc">MultiHeadAttention</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Multi-Head Attention block, used in Transformer, Set Transformer and so on</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    d_model : int</span>
<span class="sd">        The feature size (input and output) in Multi-Head Attention layer.</span>
<span class="sd">    num_heads : int</span>
<span class="sd">        The number of heads.</span>
<span class="sd">    d_head : int</span>
<span class="sd">        The hidden size per head.</span>
<span class="sd">    d_ff : int</span>
<span class="sd">        The inner hidden size in the Feed-Forward Neural Network.</span>
<span class="sd">    dropouth : float</span>
<span class="sd">        The dropout rate of each sublayer.</span>
<span class="sd">    dropouta : float</span>
<span class="sd">        The dropout rate of attention heads.</span>

<span class="sd">    Notes</span>
<span class="sd">    -----</span>
<span class="sd">    This module was used in SetTransformer layer.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span> <span class="n">d_model</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">,</span> <span class="n">d_head</span><span class="p">,</span> <span class="n">d_ff</span><span class="p">,</span> <span class="n">dropouth</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">dropouta</span><span class="o">=</span><span class="mf">0.0</span>
    <span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">MultiHeadAttention</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">d_model</span> <span class="o">=</span> <span class="n">d_model</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">num_heads</span> <span class="o">=</span> <span class="n">num_heads</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">d_head</span> <span class="o">=</span> <span class="n">d_head</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">d_ff</span> <span class="o">=</span> <span class="n">d_ff</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">proj_q</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">num_heads</span> <span class="o">*</span> <span class="n">d_head</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">proj_k</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">num_heads</span> <span class="o">*</span> <span class="n">d_head</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">proj_v</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">num_heads</span> <span class="o">*</span> <span class="n">d_head</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">proj_o</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">num_heads</span> <span class="o">*</span> <span class="n">d_head</span><span class="p">,</span> <span class="n">d_model</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">ffn</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">d_ff</span><span class="p">),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">(),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">dropouth</span><span class="p">),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">d_ff</span><span class="p">,</span> <span class="n">d_model</span><span class="p">),</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">droph</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">dropouth</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dropa</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">dropouta</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">norm_in</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">LayerNorm</span><span class="p">(</span><span class="n">d_model</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">norm_inter</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">LayerNorm</span><span class="p">(</span><span class="n">d_model</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">reset_parameters</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">reset_parameters</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Reinitialize learnable parameters.&quot;&quot;&quot;</span>
        <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">parameters</span><span class="p">():</span>
            <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">dim</span><span class="p">()</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
                <span class="n">nn</span><span class="o">.</span><span class="n">init</span><span class="o">.</span><span class="n">xavier_uniform_</span><span class="p">(</span><span class="n">p</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">mem</span><span class="p">,</span> <span class="n">lengths_x</span><span class="p">,</span> <span class="n">lengths_mem</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Compute multi-head self-attention.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        x : torch.Tensor</span>
<span class="sd">            The input tensor used to compute queries.</span>
<span class="sd">        mem : torch.Tensor</span>
<span class="sd">            The memory tensor used to compute keys and values.</span>
<span class="sd">        lengths_x : list</span>
<span class="sd">            The array of node numbers, used to segment x.</span>
<span class="sd">        lengths_mem : list</span>
<span class="sd">            The array of node numbers, used to segment mem.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">batch_size</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">lengths_x</span><span class="p">)</span>
        <span class="n">max_len_x</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="n">lengths_x</span><span class="p">)</span>
        <span class="n">max_len_mem</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="n">lengths_mem</span><span class="p">)</span>
        <span class="n">device</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">device</span>
        <span class="n">lengths_x</span> <span class="o">=</span> <span class="n">th</span><span class="o">.</span><span class="n">as_tensor</span><span class="p">(</span><span class="n">lengths_x</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">th</span><span class="o">.</span><span class="n">int64</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>
        <span class="n">lengths_mem</span> <span class="o">=</span> <span class="n">th</span><span class="o">.</span><span class="n">as_tensor</span><span class="p">(</span><span class="n">lengths_mem</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">th</span><span class="o">.</span><span class="n">int64</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>

        <span class="n">queries</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">proj_q</span><span class="p">(</span><span class="n">x</span><span class="p">)</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_heads</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">d_head</span><span class="p">)</span>
        <span class="n">keys</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">proj_k</span><span class="p">(</span><span class="n">mem</span><span class="p">)</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_heads</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">d_head</span><span class="p">)</span>
        <span class="n">values</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">proj_v</span><span class="p">(</span><span class="n">mem</span><span class="p">)</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_heads</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">d_head</span><span class="p">)</span>

        <span class="c1"># padding to (B, max_len_x/mem, num_heads, d_head)</span>
        <span class="n">queries</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">pad_packed_tensor</span><span class="p">(</span><span class="n">queries</span><span class="p">,</span> <span class="n">lengths_x</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
        <span class="n">keys</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">pad_packed_tensor</span><span class="p">(</span><span class="n">keys</span><span class="p">,</span> <span class="n">lengths_mem</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
        <span class="n">values</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">pad_packed_tensor</span><span class="p">(</span><span class="n">values</span><span class="p">,</span> <span class="n">lengths_mem</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>

        <span class="c1"># attention score with shape (B, num_heads, max_len_x, max_len_mem)</span>
        <span class="n">e</span> <span class="o">=</span> <span class="n">th</span><span class="o">.</span><span class="n">einsum</span><span class="p">(</span><span class="s2">&quot;bxhd,byhd-&gt;bhxy&quot;</span><span class="p">,</span> <span class="n">queries</span><span class="p">,</span> <span class="n">keys</span><span class="p">)</span>
        <span class="c1"># normalize</span>
        <span class="n">e</span> <span class="o">=</span> <span class="n">e</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">d_head</span><span class="p">)</span>

        <span class="c1"># generate mask</span>
        <span class="n">mask</span> <span class="o">=</span> <span class="n">_gen_mask</span><span class="p">(</span><span class="n">lengths_x</span><span class="p">,</span> <span class="n">lengths_mem</span><span class="p">,</span> <span class="n">max_len_x</span><span class="p">,</span> <span class="n">max_len_mem</span><span class="p">)</span>
        <span class="n">e</span> <span class="o">=</span> <span class="n">e</span><span class="o">.</span><span class="n">masked_fill</span><span class="p">(</span><span class="n">mask</span> <span class="o">==</span> <span class="mi">0</span><span class="p">,</span> <span class="o">-</span><span class="nb">float</span><span class="p">(</span><span class="s2">&quot;inf&quot;</span><span class="p">))</span>

        <span class="c1"># apply softmax</span>
        <span class="n">alpha</span> <span class="o">=</span> <span class="n">th</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">e</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
        <span class="c1"># the following line addresses the NaN issue, see</span>
        <span class="c1"># https://github.com/dmlc/dgl/issues/2657</span>
        <span class="n">alpha</span> <span class="o">=</span> <span class="n">alpha</span><span class="o">.</span><span class="n">masked_fill</span><span class="p">(</span><span class="n">mask</span> <span class="o">==</span> <span class="mi">0</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">)</span>

        <span class="c1"># sum of value weighted by alpha</span>
        <span class="n">out</span> <span class="o">=</span> <span class="n">th</span><span class="o">.</span><span class="n">einsum</span><span class="p">(</span><span class="s2">&quot;bhxy,byhd-&gt;bxhd&quot;</span><span class="p">,</span> <span class="n">alpha</span><span class="p">,</span> <span class="n">values</span><span class="p">)</span>
        <span class="c1"># project to output</span>
        <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">proj_o</span><span class="p">(</span>
            <span class="n">out</span><span class="o">.</span><span class="n">contiguous</span><span class="p">()</span><span class="o">.</span><span class="n">view</span><span class="p">(</span>
                <span class="n">batch_size</span><span class="p">,</span> <span class="n">max_len_x</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_heads</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">d_head</span>
            <span class="p">)</span>
        <span class="p">)</span>
        <span class="c1"># pack tensor</span>
        <span class="n">out</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">pack_padded_tensor</span><span class="p">(</span><span class="n">out</span><span class="p">,</span> <span class="n">lengths_x</span><span class="p">)</span>

        <span class="c1"># intra norm</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">norm_in</span><span class="p">(</span><span class="n">x</span> <span class="o">+</span> <span class="n">out</span><span class="p">)</span>

        <span class="c1"># inter norm</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">norm_inter</span><span class="p">(</span><span class="n">x</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">ffn</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>

        <span class="k">return</span> <span class="n">x</span>


<span class="k">class</span> <span class="nc">SetAttentionBlock</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;SAB block from `Set Transformer: A Framework for Attention-based</span>
<span class="sd">    Permutation-Invariant Neural Networks &lt;https://arxiv.org/abs/1810.00825&gt;`__</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    d_model : int</span>
<span class="sd">        The feature size (input and output) in Multi-Head Attention layer.</span>
<span class="sd">    num_heads : int</span>
<span class="sd">        The number of heads.</span>
<span class="sd">    d_head : int</span>
<span class="sd">        The hidden size per head.</span>
<span class="sd">    d_ff : int</span>
<span class="sd">        The inner hidden size in the Feed-Forward Neural Network.</span>
<span class="sd">    dropouth : float</span>
<span class="sd">        The dropout rate of each sublayer.</span>
<span class="sd">    dropouta : float</span>
<span class="sd">        The dropout rate of attention heads.</span>

<span class="sd">    Notes</span>
<span class="sd">    -----</span>
<span class="sd">    This module was used in SetTransformer layer.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span> <span class="n">d_model</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">,</span> <span class="n">d_head</span><span class="p">,</span> <span class="n">d_ff</span><span class="p">,</span> <span class="n">dropouth</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">dropouta</span><span class="o">=</span><span class="mf">0.0</span>
    <span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">SetAttentionBlock</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">mha</span> <span class="o">=</span> <span class="n">MultiHeadAttention</span><span class="p">(</span>
            <span class="n">d_model</span><span class="p">,</span>
            <span class="n">num_heads</span><span class="p">,</span>
            <span class="n">d_head</span><span class="p">,</span>
            <span class="n">d_ff</span><span class="p">,</span>
            <span class="n">dropouth</span><span class="o">=</span><span class="n">dropouth</span><span class="p">,</span>
            <span class="n">dropouta</span><span class="o">=</span><span class="n">dropouta</span><span class="p">,</span>
        <span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">feat</span><span class="p">,</span> <span class="n">lengths</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Compute a Set Attention Block.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        feat : torch.Tensor</span>
<span class="sd">            The input feature.</span>
<span class="sd">        lengths : list</span>
<span class="sd">            The array of node numbers, used to segment feat tensor.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">mha</span><span class="p">(</span><span class="n">feat</span><span class="p">,</span> <span class="n">feat</span><span class="p">,</span> <span class="n">lengths</span><span class="p">,</span> <span class="n">lengths</span><span class="p">)</span>


<span class="k">class</span> <span class="nc">InducedSetAttentionBlock</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;ISAB block from `Set Transformer: A Framework for Attention-based</span>
<span class="sd">    Permutation-Invariant Neural Networks &lt;https://arxiv.org/abs/1810.00825&gt;`__</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    m : int</span>
<span class="sd">        The number of induced vectors.</span>
<span class="sd">    d_model : int</span>
<span class="sd">        The feature size (input and output) in Multi-Head Attention layer.</span>
<span class="sd">    num_heads : int</span>
<span class="sd">        The number of heads.</span>
<span class="sd">    d_head : int</span>
<span class="sd">        The hidden size per head.</span>
<span class="sd">    d_ff : int</span>
<span class="sd">        The inner hidden size in the Feed-Forward Neural Network.</span>
<span class="sd">    dropouth : float</span>
<span class="sd">        The dropout rate of each sublayer.</span>
<span class="sd">    dropouta : float</span>
<span class="sd">        The dropout rate of attention heads.</span>

<span class="sd">    Notes</span>
<span class="sd">    -----</span>
<span class="sd">    This module was used in SetTransformer layer.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span> <span class="n">m</span><span class="p">,</span> <span class="n">d_model</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">,</span> <span class="n">d_head</span><span class="p">,</span> <span class="n">d_ff</span><span class="p">,</span> <span class="n">dropouth</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">dropouta</span><span class="o">=</span><span class="mf">0.0</span>
    <span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">InducedSetAttentionBlock</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">m</span> <span class="o">=</span> <span class="n">m</span>
        <span class="k">if</span> <span class="n">m</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
            <span class="n">dgl_warning</span><span class="p">(</span>
                <span class="s2">&quot;if m is set to 1, the parameters corresponding to query and key &quot;</span>
                <span class="s2">&quot;projections would not get updated during training.&quot;</span>
            <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">d_model</span> <span class="o">=</span> <span class="n">d_model</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">inducing_points</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">th</span><span class="o">.</span><span class="n">FloatTensor</span><span class="p">(</span><span class="n">m</span><span class="p">,</span> <span class="n">d_model</span><span class="p">))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">mha</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ModuleList</span><span class="p">(</span>
            <span class="p">[</span>
                <span class="n">MultiHeadAttention</span><span class="p">(</span>
                    <span class="n">d_model</span><span class="p">,</span>
                    <span class="n">num_heads</span><span class="p">,</span>
                    <span class="n">d_head</span><span class="p">,</span>
                    <span class="n">d_ff</span><span class="p">,</span>
                    <span class="n">dropouth</span><span class="o">=</span><span class="n">dropouth</span><span class="p">,</span>
                    <span class="n">dropouta</span><span class="o">=</span><span class="n">dropouta</span><span class="p">,</span>
                <span class="p">)</span>
                <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>
            <span class="p">]</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">reset_parameters</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">reset_parameters</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Reinitialize learnable parameters.&quot;&quot;&quot;</span>
        <span class="n">nn</span><span class="o">.</span><span class="n">init</span><span class="o">.</span><span class="n">xavier_uniform_</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">inducing_points</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">feat</span><span class="p">,</span> <span class="n">lengths</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Compute an Induced Set Attention Block.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        feat : torch.Tensor</span>
<span class="sd">            The input feature.</span>
<span class="sd">        lengths : list</span>
<span class="sd">            The array of node numbers, used to segment feat tensor.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        torch.Tensor</span>
<span class="sd">            The output feature</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">batch_size</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">lengths</span><span class="p">)</span>
        <span class="n">query</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">inducing_points</span><span class="o">.</span><span class="n">repeat</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
        <span class="n">memory</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">mha</span><span class="p">[</span><span class="mi">0</span><span class="p">](</span><span class="n">query</span><span class="p">,</span> <span class="n">feat</span><span class="p">,</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">m</span><span class="p">]</span> <span class="o">*</span> <span class="n">batch_size</span><span class="p">,</span> <span class="n">lengths</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">mha</span><span class="p">[</span><span class="mi">1</span><span class="p">](</span><span class="n">feat</span><span class="p">,</span> <span class="n">memory</span><span class="p">,</span> <span class="n">lengths</span><span class="p">,</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">m</span><span class="p">]</span> <span class="o">*</span> <span class="n">batch_size</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">extra_repr</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Set the extra representation of the module.</span>
<span class="sd">        which will come into effect when printing the model.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">shape_str</span> <span class="o">=</span> <span class="s2">&quot;(</span><span class="si">{}</span><span class="s2">, </span><span class="si">{}</span><span class="s2">)&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">inducing_points</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">inducing_points</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
        <span class="p">)</span>
        <span class="k">return</span> <span class="s2">&quot;InducedVector: &quot;</span> <span class="o">+</span> <span class="n">shape_str</span>


<span class="k">class</span> <span class="nc">PMALayer</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Pooling by Multihead Attention from `Set Transformer: A Framework for Attention-based</span>
<span class="sd">    Permutation-Invariant Neural Networks &lt;https://arxiv.org/abs/1810.00825&gt;`__</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    k : int</span>
<span class="sd">        The number of seed vectors.</span>
<span class="sd">    d_model : int</span>
<span class="sd">        The feature size (input and output) in Multi-Head Attention layer.</span>
<span class="sd">    num_heads : int</span>
<span class="sd">        The number of heads.</span>
<span class="sd">    d_head : int</span>
<span class="sd">        The hidden size per head.</span>
<span class="sd">    d_ff : int</span>
<span class="sd">        The kernel size in FFN (Positionwise Feed-Forward Network) layer.</span>
<span class="sd">    dropouth : float</span>
<span class="sd">        The dropout rate of each sublayer.</span>
<span class="sd">    dropouta : float</span>
<span class="sd">        The dropout rate of attention heads.</span>

<span class="sd">    Notes</span>
<span class="sd">    -----</span>
<span class="sd">    This module was used in SetTransformer layer.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">d_model</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">,</span> <span class="n">d_head</span><span class="p">,</span> <span class="n">d_ff</span><span class="p">,</span> <span class="n">dropouth</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">dropouta</span><span class="o">=</span><span class="mf">0.0</span>
    <span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">PMALayer</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">k</span> <span class="o">=</span> <span class="n">k</span>
        <span class="k">if</span> <span class="n">k</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
            <span class="n">dgl_warning</span><span class="p">(</span>
                <span class="s2">&quot;if k is set to 1, the parameters corresponding to query and key &quot;</span>
                <span class="s2">&quot;projections would not get updated during training.&quot;</span>
            <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">d_model</span> <span class="o">=</span> <span class="n">d_model</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">seed_vectors</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">th</span><span class="o">.</span><span class="n">FloatTensor</span><span class="p">(</span><span class="n">k</span><span class="p">,</span> <span class="n">d_model</span><span class="p">))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">mha</span> <span class="o">=</span> <span class="n">MultiHeadAttention</span><span class="p">(</span>
            <span class="n">d_model</span><span class="p">,</span>
            <span class="n">num_heads</span><span class="p">,</span>
            <span class="n">d_head</span><span class="p">,</span>
            <span class="n">d_ff</span><span class="p">,</span>
            <span class="n">dropouth</span><span class="o">=</span><span class="n">dropouth</span><span class="p">,</span>
            <span class="n">dropouta</span><span class="o">=</span><span class="n">dropouta</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">ffn</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">d_ff</span><span class="p">),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">(),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">dropouth</span><span class="p">),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">d_ff</span><span class="p">,</span> <span class="n">d_model</span><span class="p">),</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">reset_parameters</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">reset_parameters</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Reinitialize learnable parameters.&quot;&quot;&quot;</span>
        <span class="n">nn</span><span class="o">.</span><span class="n">init</span><span class="o">.</span><span class="n">xavier_uniform_</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">seed_vectors</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">feat</span><span class="p">,</span> <span class="n">lengths</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Compute Pooling by Multihead Attention.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        feat : torch.Tensor</span>
<span class="sd">            The input feature.</span>
<span class="sd">        lengths : list</span>
<span class="sd">            The array of node numbers, used to segment feat tensor.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        torch.Tensor</span>
<span class="sd">            The output feature</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">batch_size</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">lengths</span><span class="p">)</span>
        <span class="n">query</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">seed_vectors</span><span class="o">.</span><span class="n">repeat</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">mha</span><span class="p">(</span><span class="n">query</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">ffn</span><span class="p">(</span><span class="n">feat</span><span class="p">),</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">k</span><span class="p">]</span> <span class="o">*</span> <span class="n">batch_size</span><span class="p">,</span> <span class="n">lengths</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">extra_repr</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Set the extra representation of the module.</span>
<span class="sd">        which will come into effect when printing the model.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">shape_str</span> <span class="o">=</span> <span class="s2">&quot;(</span><span class="si">{}</span><span class="s2">, </span><span class="si">{}</span><span class="s2">)&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">seed_vectors</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">seed_vectors</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
        <span class="p">)</span>
        <span class="k">return</span> <span class="s2">&quot;SeedVector: &quot;</span> <span class="o">+</span> <span class="n">shape_str</span>


<div class="viewcode-block" id="SetTransformerEncoder">
<a class="viewcode-back" href="../../../../generated/dgl.nn.pytorch.glob.SetTransformerEncoder.html#dgl.nn.pytorch.glob.SetTransformerEncoder">[docs]</a>
<span class="k">class</span> <span class="nc">SetTransformerEncoder</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;The Encoder module from `Set Transformer: A Framework for Attention-based</span>
<span class="sd">    Permutation-Invariant Neural Networks &lt;https://arxiv.org/pdf/1810.00825.pdf&gt;`__</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    d_model : int</span>
<span class="sd">        The hidden size of the model.</span>
<span class="sd">    n_heads : int</span>
<span class="sd">        The number of heads.</span>
<span class="sd">    d_head : int</span>
<span class="sd">        The hidden size of each head.</span>
<span class="sd">    d_ff : int</span>
<span class="sd">        The kernel size in FFN (Positionwise Feed-Forward Network) layer.</span>
<span class="sd">    n_layers : int</span>
<span class="sd">        The number of layers.</span>
<span class="sd">    block_type : str</span>
<span class="sd">        Building block type: &#39;sab&#39; (Set Attention Block) or &#39;isab&#39; (Induced</span>
<span class="sd">        Set Attention Block).</span>
<span class="sd">    m : int or None</span>
<span class="sd">        The number of induced vectors in ISAB Block. Set to None if block type</span>
<span class="sd">        is &#39;sab&#39;.</span>
<span class="sd">    dropouth : float</span>
<span class="sd">        The dropout rate of each sublayer.</span>
<span class="sd">    dropouta : float</span>
<span class="sd">        The dropout rate of attention heads.</span>

<span class="sd">    Examples</span>
<span class="sd">    --------</span>
<span class="sd">    &gt;&gt;&gt; import dgl</span>
<span class="sd">    &gt;&gt;&gt; import torch as th</span>
<span class="sd">    &gt;&gt;&gt; from dgl.nn import SetTransformerEncoder</span>
<span class="sd">    &gt;&gt;&gt;</span>
<span class="sd">    &gt;&gt;&gt; g1 = dgl.rand_graph(3, 4)  # g1 is a random graph with 3 nodes and 4 edges</span>
<span class="sd">    &gt;&gt;&gt; g1_node_feats = th.rand(3, 5)  # feature size is 5</span>
<span class="sd">    &gt;&gt;&gt; g1_node_feats</span>
<span class="sd">    tensor([[0.8948, 0.0699, 0.9137, 0.7567, 0.3637],</span>
<span class="sd">            [0.8137, 0.8938, 0.8377, 0.4249, 0.6118],</span>
<span class="sd">            [0.5197, 0.9030, 0.6825, 0.5725, 0.4755]])</span>
<span class="sd">    &gt;&gt;&gt;</span>
<span class="sd">    &gt;&gt;&gt; g2 = dgl.rand_graph(4, 6)  # g2 is a random graph with 4 nodes and 6 edges</span>
<span class="sd">    &gt;&gt;&gt; g2_node_feats = th.rand(4, 5)  # feature size is 5</span>
<span class="sd">    &gt;&gt;&gt; g2_node_feats</span>
<span class="sd">    tensor([[0.2053, 0.2426, 0.4111, 0.9028, 0.5658],</span>
<span class="sd">            [0.5278, 0.6365, 0.9990, 0.2351, 0.8945],</span>
<span class="sd">            [0.3134, 0.0580, 0.4349, 0.7949, 0.3891],</span>
<span class="sd">            [0.0142, 0.2709, 0.3330, 0.8521, 0.6925]])</span>
<span class="sd">    &gt;&gt;&gt;</span>
<span class="sd">    &gt;&gt;&gt; set_trans_enc = SetTransformerEncoder(5, 4, 4, 20)  # create a settrans encoder.</span>

<span class="sd">    Case 1: Input a single graph</span>

<span class="sd">    &gt;&gt;&gt; set_trans_enc(g1, g1_node_feats)</span>
<span class="sd">    tensor([[ 0.1262, -1.9081,  0.7287,  0.1678,  0.8854],</span>
<span class="sd">            [-0.0634, -1.1996,  0.6955, -0.9230,  1.4904],</span>
<span class="sd">            [-0.9972, -0.7924,  0.6907, -0.5221,  1.6211]],</span>
<span class="sd">           grad_fn=&lt;NativeLayerNormBackward&gt;)</span>

<span class="sd">    Case 2: Input a batch of graphs</span>

<span class="sd">    Build a batch of DGL graphs and concatenate all graphs&#39; node features into one tensor.</span>

<span class="sd">    &gt;&gt;&gt; batch_g = dgl.batch([g1, g2])</span>
<span class="sd">    &gt;&gt;&gt; batch_f = th.cat([g1_node_feats, g2_node_feats])</span>
<span class="sd">    &gt;&gt;&gt;</span>
<span class="sd">    &gt;&gt;&gt; set_trans_enc(batch_g, batch_f)</span>
<span class="sd">    tensor([[ 0.1262, -1.9081,  0.7287,  0.1678,  0.8854],</span>
<span class="sd">            [-0.0634, -1.1996,  0.6955, -0.9230,  1.4904],</span>
<span class="sd">            [-0.9972, -0.7924,  0.6907, -0.5221,  1.6211],</span>
<span class="sd">            [-0.7973, -1.3203,  0.0634,  0.5237,  1.5306],</span>
<span class="sd">            [-0.4497, -1.0920,  0.8470, -0.8030,  1.4977],</span>
<span class="sd">            [-0.4940, -1.6045,  0.2363,  0.4885,  1.3737],</span>
<span class="sd">            [-0.9840, -1.0913, -0.0099,  0.4653,  1.6199]],</span>
<span class="sd">           grad_fn=&lt;NativeLayerNormBackward&gt;)</span>

<span class="sd">    See Also</span>
<span class="sd">    --------</span>
<span class="sd">    SetTransformerDecoder</span>

<span class="sd">    Notes</span>
<span class="sd">    -----</span>
<span class="sd">    SetTransformerEncoder is not a readout layer, the tensor it returned is nodewise</span>
<span class="sd">    representation instead out graphwise representation, and the SetTransformerDecoder</span>
<span class="sd">    would return a graph readout tensor.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">d_model</span><span class="p">,</span>
        <span class="n">n_heads</span><span class="p">,</span>
        <span class="n">d_head</span><span class="p">,</span>
        <span class="n">d_ff</span><span class="p">,</span>
        <span class="n">n_layers</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
        <span class="n">block_type</span><span class="o">=</span><span class="s2">&quot;sab&quot;</span><span class="p">,</span>
        <span class="n">m</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">dropouth</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span>
        <span class="n">dropouta</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span>
    <span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">SetTransformerEncoder</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">n_layers</span> <span class="o">=</span> <span class="n">n_layers</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">block_type</span> <span class="o">=</span> <span class="n">block_type</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">m</span> <span class="o">=</span> <span class="n">m</span>
        <span class="n">layers</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">if</span> <span class="n">block_type</span> <span class="o">==</span> <span class="s2">&quot;isab&quot;</span> <span class="ow">and</span> <span class="n">m</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">KeyError</span><span class="p">(</span>
                <span class="s2">&quot;The number of inducing points is not specified in ISAB block.&quot;</span>
            <span class="p">)</span>

        <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_layers</span><span class="p">):</span>
            <span class="k">if</span> <span class="n">block_type</span> <span class="o">==</span> <span class="s2">&quot;sab&quot;</span><span class="p">:</span>
                <span class="n">layers</span><span class="o">.</span><span class="n">append</span><span class="p">(</span>
                    <span class="n">SetAttentionBlock</span><span class="p">(</span>
                        <span class="n">d_model</span><span class="p">,</span>
                        <span class="n">n_heads</span><span class="p">,</span>
                        <span class="n">d_head</span><span class="p">,</span>
                        <span class="n">d_ff</span><span class="p">,</span>
                        <span class="n">dropouth</span><span class="o">=</span><span class="n">dropouth</span><span class="p">,</span>
                        <span class="n">dropouta</span><span class="o">=</span><span class="n">dropouta</span><span class="p">,</span>
                    <span class="p">)</span>
                <span class="p">)</span>
            <span class="k">elif</span> <span class="n">block_type</span> <span class="o">==</span> <span class="s2">&quot;isab&quot;</span><span class="p">:</span>
                <span class="n">layers</span><span class="o">.</span><span class="n">append</span><span class="p">(</span>
                    <span class="n">InducedSetAttentionBlock</span><span class="p">(</span>
                        <span class="n">m</span><span class="p">,</span>
                        <span class="n">d_model</span><span class="p">,</span>
                        <span class="n">n_heads</span><span class="p">,</span>
                        <span class="n">d_head</span><span class="p">,</span>
                        <span class="n">d_ff</span><span class="p">,</span>
                        <span class="n">dropouth</span><span class="o">=</span><span class="n">dropouth</span><span class="p">,</span>
                        <span class="n">dropouta</span><span class="o">=</span><span class="n">dropouta</span><span class="p">,</span>
                    <span class="p">)</span>
                <span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">KeyError</span><span class="p">(</span>
                    <span class="s2">&quot;Unrecognized block type </span><span class="si">{}</span><span class="s2">: we only support sab/isab&quot;</span>
                <span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">layers</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ModuleList</span><span class="p">(</span><span class="n">layers</span><span class="p">)</span>

<div class="viewcode-block" id="SetTransformerEncoder.forward">
<a class="viewcode-back" href="../../../../generated/dgl.nn.pytorch.glob.SetTransformerEncoder.html#dgl.nn.pytorch.glob.SetTransformerEncoder.forward">[docs]</a>
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">graph</span><span class="p">,</span> <span class="n">feat</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Compute the Encoder part of Set Transformer.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        graph : DGLGraph</span>
<span class="sd">            The input graph.</span>
<span class="sd">        feat : torch.Tensor</span>
<span class="sd">            The input feature with shape :math:`(N, D)`, where :math:`N` is the</span>
<span class="sd">            number of nodes in the graph.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        torch.Tensor</span>
<span class="sd">            The output feature with shape :math:`(N, D)`.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">lengths</span> <span class="o">=</span> <span class="n">graph</span><span class="o">.</span><span class="n">batch_num_nodes</span><span class="p">()</span>
        <span class="k">for</span> <span class="n">layer</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">layers</span><span class="p">:</span>
            <span class="n">feat</span> <span class="o">=</span> <span class="n">layer</span><span class="p">(</span><span class="n">feat</span><span class="p">,</span> <span class="n">lengths</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">feat</span></div>
</div>



<div class="viewcode-block" id="SetTransformerDecoder">
<a class="viewcode-back" href="../../../../generated/dgl.nn.pytorch.glob.SetTransformerDecoder.html#dgl.nn.pytorch.glob.SetTransformerDecoder">[docs]</a>
<span class="k">class</span> <span class="nc">SetTransformerDecoder</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;The Decoder module from `Set Transformer: A Framework for Attention-based</span>
<span class="sd">    Permutation-Invariant Neural Networks &lt;https://arxiv.org/pdf/1810.00825.pdf&gt;`__</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    d_model : int</span>
<span class="sd">        Hidden size of the model.</span>
<span class="sd">    num_heads : int</span>
<span class="sd">        The number of heads.</span>
<span class="sd">    d_head : int</span>
<span class="sd">        Hidden size of each head.</span>
<span class="sd">    d_ff : int</span>
<span class="sd">        Kernel size in FFN (Positionwise Feed-Forward Network) layer.</span>
<span class="sd">    n_layers : int</span>
<span class="sd">        The number of layers.</span>
<span class="sd">    k : int</span>
<span class="sd">        The number of seed vectors in PMA (Pooling by Multihead Attention) layer.</span>
<span class="sd">    dropouth : float</span>
<span class="sd">        Dropout rate of each sublayer.</span>
<span class="sd">    dropouta : float</span>
<span class="sd">        Dropout rate of attention heads.</span>

<span class="sd">    Examples</span>
<span class="sd">    --------</span>
<span class="sd">    &gt;&gt;&gt; import dgl</span>
<span class="sd">    &gt;&gt;&gt; import torch as th</span>
<span class="sd">    &gt;&gt;&gt; from dgl.nn import SetTransformerDecoder</span>
<span class="sd">    &gt;&gt;&gt;</span>
<span class="sd">    &gt;&gt;&gt; g1 = dgl.rand_graph(3, 4)  # g1 is a random graph with 3 nodes and 4 edges</span>
<span class="sd">    &gt;&gt;&gt; g1_node_feats = th.rand(3, 5)  # feature size is 5</span>
<span class="sd">    &gt;&gt;&gt; g1_node_feats</span>
<span class="sd">    tensor([[0.8948, 0.0699, 0.9137, 0.7567, 0.3637],</span>
<span class="sd">            [0.8137, 0.8938, 0.8377, 0.4249, 0.6118],</span>
<span class="sd">            [0.5197, 0.9030, 0.6825, 0.5725, 0.4755]])</span>
<span class="sd">    &gt;&gt;&gt;</span>
<span class="sd">    &gt;&gt;&gt; g2 = dgl.rand_graph(4, 6)  # g2 is a random graph with 4 nodes and 6 edges</span>
<span class="sd">    &gt;&gt;&gt; g2_node_feats = th.rand(4, 5)  # feature size is 5</span>
<span class="sd">    &gt;&gt;&gt; g2_node_feats</span>
<span class="sd">    tensor([[0.2053, 0.2426, 0.4111, 0.9028, 0.5658],</span>
<span class="sd">            [0.5278, 0.6365, 0.9990, 0.2351, 0.8945],</span>
<span class="sd">            [0.3134, 0.0580, 0.4349, 0.7949, 0.3891],</span>
<span class="sd">            [0.0142, 0.2709, 0.3330, 0.8521, 0.6925]])</span>
<span class="sd">    &gt;&gt;&gt;</span>
<span class="sd">    &gt;&gt;&gt; set_trans_dec = SetTransformerDecoder(5, 4, 4, 20, 1, 3)  # define the layer</span>

<span class="sd">    Case 1: Input a single graph</span>

<span class="sd">    &gt;&gt;&gt; set_trans_dec(g1, g1_node_feats)</span>
<span class="sd">    tensor([[-0.5538,  1.8726, -1.0470,  0.0276, -0.2994, -0.6317,  1.6754, -1.3189,</span>
<span class="sd">              0.2291,  0.0461, -0.4042,  0.8387, -1.7091,  1.0845,  0.1902]],</span>
<span class="sd">           grad_fn=&lt;ViewBackward&gt;)</span>

<span class="sd">    Case 2: Input a batch of graphs</span>

<span class="sd">    Build a batch of DGL graphs and concatenate all graphs&#39; node features into one tensor.</span>

<span class="sd">    &gt;&gt;&gt; batch_g = dgl.batch([g1, g2])</span>
<span class="sd">    &gt;&gt;&gt; batch_f = th.cat([g1_node_feats, g2_node_feats])</span>
<span class="sd">    &gt;&gt;&gt;</span>
<span class="sd">    &gt;&gt;&gt; set_trans_dec(batch_g, batch_f)</span>
<span class="sd">    tensor([[-0.5538,  1.8726, -1.0470,  0.0276, -0.2994, -0.6317,  1.6754, -1.3189,</span>
<span class="sd">              0.2291,  0.0461, -0.4042,  0.8387, -1.7091,  1.0845,  0.1902],</span>
<span class="sd">            [-0.5511,  1.8869, -1.0156,  0.0028, -0.3231, -0.6305,  1.6845, -1.3105,</span>
<span class="sd">              0.2136,  0.0428, -0.3820,  0.8043, -1.7138,  1.1126,  0.1789]],</span>
<span class="sd">           grad_fn=&lt;ViewBackward&gt;)</span>

<span class="sd">    See Also</span>
<span class="sd">    --------</span>
<span class="sd">    SetTransformerEncoder</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">d_model</span><span class="p">,</span>
        <span class="n">num_heads</span><span class="p">,</span>
        <span class="n">d_head</span><span class="p">,</span>
        <span class="n">d_ff</span><span class="p">,</span>
        <span class="n">n_layers</span><span class="p">,</span>
        <span class="n">k</span><span class="p">,</span>
        <span class="n">dropouth</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span>
        <span class="n">dropouta</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span>
    <span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">SetTransformerDecoder</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">n_layers</span> <span class="o">=</span> <span class="n">n_layers</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">k</span> <span class="o">=</span> <span class="n">k</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">d_model</span> <span class="o">=</span> <span class="n">d_model</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">pma</span> <span class="o">=</span> <span class="n">PMALayer</span><span class="p">(</span>
            <span class="n">k</span><span class="p">,</span>
            <span class="n">d_model</span><span class="p">,</span>
            <span class="n">num_heads</span><span class="p">,</span>
            <span class="n">d_head</span><span class="p">,</span>
            <span class="n">d_ff</span><span class="p">,</span>
            <span class="n">dropouth</span><span class="o">=</span><span class="n">dropouth</span><span class="p">,</span>
            <span class="n">dropouta</span><span class="o">=</span><span class="n">dropouta</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="n">layers</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_layers</span><span class="p">):</span>
            <span class="n">layers</span><span class="o">.</span><span class="n">append</span><span class="p">(</span>
                <span class="n">SetAttentionBlock</span><span class="p">(</span>
                    <span class="n">d_model</span><span class="p">,</span>
                    <span class="n">num_heads</span><span class="p">,</span>
                    <span class="n">d_head</span><span class="p">,</span>
                    <span class="n">d_ff</span><span class="p">,</span>
                    <span class="n">dropouth</span><span class="o">=</span><span class="n">dropouth</span><span class="p">,</span>
                    <span class="n">dropouta</span><span class="o">=</span><span class="n">dropouta</span><span class="p">,</span>
                <span class="p">)</span>
            <span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">layers</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ModuleList</span><span class="p">(</span><span class="n">layers</span><span class="p">)</span>

<div class="viewcode-block" id="SetTransformerDecoder.forward">
<a class="viewcode-back" href="../../../../generated/dgl.nn.pytorch.glob.SetTransformerDecoder.html#dgl.nn.pytorch.glob.SetTransformerDecoder.forward">[docs]</a>
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">graph</span><span class="p">,</span> <span class="n">feat</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Compute the decoder part of Set Transformer.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        graph : DGLGraph</span>
<span class="sd">            The input graph.</span>
<span class="sd">        feat : torch.Tensor</span>
<span class="sd">            The input feature with shape :math:`(N, D)`, where :math:`N` is the</span>
<span class="sd">            number of nodes in the graph, and :math:`D` means the size of features.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        torch.Tensor</span>
<span class="sd">            The output feature with shape :math:`(B, D)`, where :math:`B` refers to</span>
<span class="sd">            the batch size.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">len_pma</span> <span class="o">=</span> <span class="n">graph</span><span class="o">.</span><span class="n">batch_num_nodes</span><span class="p">()</span>
        <span class="n">len_sab</span> <span class="o">=</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">k</span><span class="p">]</span> <span class="o">*</span> <span class="n">graph</span><span class="o">.</span><span class="n">batch_size</span>
        <span class="n">feat</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">pma</span><span class="p">(</span><span class="n">feat</span><span class="p">,</span> <span class="n">len_pma</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">layer</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">layers</span><span class="p">:</span>
            <span class="n">feat</span> <span class="o">=</span> <span class="n">layer</span><span class="p">(</span><span class="n">feat</span><span class="p">,</span> <span class="n">len_sab</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">feat</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">graph</span><span class="o">.</span><span class="n">batch_size</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">k</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">d_model</span><span class="p">)</span></div>
</div>



<div class="viewcode-block" id="WeightAndSum">
<a class="viewcode-back" href="../../../../generated/dgl.nn.pytorch.glob.WeightAndSum.html#dgl.nn.pytorch.glob.WeightAndSum">[docs]</a>
<span class="k">class</span> <span class="nc">WeightAndSum</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Compute importance weights for atoms and perform a weighted sum.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    in_feats : int</span>
<span class="sd">        Input atom feature size</span>

<span class="sd">    Examples</span>
<span class="sd">    --------</span>
<span class="sd">    The following example uses PyTorch backend.</span>

<span class="sd">    &gt;&gt;&gt; import dgl</span>
<span class="sd">    &gt;&gt;&gt; import torch as th</span>
<span class="sd">    &gt;&gt;&gt; from dgl.nn import WeightAndSum</span>
<span class="sd">    &gt;&gt;&gt;</span>
<span class="sd">    &gt;&gt;&gt; g1 = dgl.rand_graph(3, 4)  # g1 is a random graph with 3 nodes and 4 edges</span>
<span class="sd">    &gt;&gt;&gt; g1_node_feats = th.rand(3, 5)  # feature size is 5</span>
<span class="sd">    &gt;&gt;&gt; g1_node_feats</span>
<span class="sd">    tensor([[0.8948, 0.0699, 0.9137, 0.7567, 0.3637],</span>
<span class="sd">            [0.8137, 0.8938, 0.8377, 0.4249, 0.6118],</span>
<span class="sd">            [0.5197, 0.9030, 0.6825, 0.5725, 0.4755]])</span>
<span class="sd">    &gt;&gt;&gt;</span>
<span class="sd">    &gt;&gt;&gt; g2 = dgl.rand_graph(4, 6)  # g2 is a random graph with 4 nodes and 6 edges</span>
<span class="sd">    &gt;&gt;&gt; g2_node_feats = th.rand(4, 5)  # feature size is 5</span>
<span class="sd">    &gt;&gt;&gt; g2_node_feats</span>
<span class="sd">    tensor([[0.2053, 0.2426, 0.4111, 0.9028, 0.5658],</span>
<span class="sd">            [0.5278, 0.6365, 0.9990, 0.2351, 0.8945],</span>
<span class="sd">            [0.3134, 0.0580, 0.4349, 0.7949, 0.3891],</span>
<span class="sd">            [0.0142, 0.2709, 0.3330, 0.8521, 0.6925]])</span>
<span class="sd">    &gt;&gt;&gt;</span>
<span class="sd">    &gt;&gt;&gt; weight_and_sum = WeightAndSum(5)  # create a weight and sum layer(in_feats=16)</span>

<span class="sd">    Case 1: Input a single graph</span>

<span class="sd">    &gt;&gt;&gt; weight_and_sum(g1, g1_node_feats)</span>
<span class="sd">    tensor([[1.2194, 0.9490, 1.3235, 0.9609, 0.7710]],</span>
<span class="sd">           grad_fn=&lt;SegmentReduceBackward&gt;)</span>

<span class="sd">    Case 2: Input a batch of graphs</span>

<span class="sd">    Build a batch of DGL graphs and concatenate all graphs&#39; node features into one tensor.</span>

<span class="sd">    &gt;&gt;&gt; batch_g = dgl.batch([g1, g2])</span>
<span class="sd">    &gt;&gt;&gt; batch_f = th.cat([g1_node_feats, g2_node_feats])</span>
<span class="sd">    &gt;&gt;&gt;</span>
<span class="sd">    &gt;&gt;&gt; weight_and_sum(batch_g, batch_f)</span>
<span class="sd">    tensor([[1.2194, 0.9490, 1.3235, 0.9609, 0.7710],</span>
<span class="sd">            [0.5322, 0.5840, 1.0729, 1.3665, 1.2360]],</span>
<span class="sd">           grad_fn=&lt;SegmentReduceBackward&gt;)</span>

<span class="sd">    Notes</span>
<span class="sd">    -----</span>
<span class="sd">    WeightAndSum module was commonly used in molecular property prediction networks,</span>
<span class="sd">    see the GCN predictor in `dgl-lifesci &lt;https://github.com/awslabs/dgl-lifesci/blob/</span>
<span class="sd">    ae0491431804611ba466ff413f69d435789dbfd5/python/dgllife/model/model_zoo/</span>
<span class="sd">    gcn_predictor.py&gt;`__</span>
<span class="sd">    to understand how to use WeightAndSum layer to get the graph readout output.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">in_feats</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">WeightAndSum</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">in_feats</span> <span class="o">=</span> <span class="n">in_feats</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">atom_weighting</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">in_feats</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sigmoid</span><span class="p">()</span>
        <span class="p">)</span>

<div class="viewcode-block" id="WeightAndSum.forward">
<a class="viewcode-back" href="../../../../generated/dgl.nn.pytorch.glob.WeightAndSum.html#dgl.nn.pytorch.glob.WeightAndSum.forward">[docs]</a>
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">g</span><span class="p">,</span> <span class="n">feats</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Compute molecule representations out of atom representations</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        g : DGLGraph</span>
<span class="sd">            DGLGraph with batch size B for processing multiple molecules in parallel</span>
<span class="sd">        feats : FloatTensor of shape (N, self.in_feats)</span>
<span class="sd">            Representations for all atoms in the molecules</span>
<span class="sd">            * N is the total number of atoms in all molecules</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        FloatTensor of shape (B, self.in_feats)</span>
<span class="sd">            Representations for B molecules</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">with</span> <span class="n">g</span><span class="o">.</span><span class="n">local_scope</span><span class="p">():</span>
            <span class="n">g</span><span class="o">.</span><span class="n">ndata</span><span class="p">[</span><span class="s2">&quot;h&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">feats</span>
            <span class="n">g</span><span class="o">.</span><span class="n">ndata</span><span class="p">[</span><span class="s2">&quot;w&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">atom_weighting</span><span class="p">(</span><span class="n">g</span><span class="o">.</span><span class="n">ndata</span><span class="p">[</span><span class="s2">&quot;h&quot;</span><span class="p">])</span>
            <span class="n">h_g_sum</span> <span class="o">=</span> <span class="n">sum_nodes</span><span class="p">(</span><span class="n">g</span><span class="p">,</span> <span class="s2">&quot;h&quot;</span><span class="p">,</span> <span class="s2">&quot;w&quot;</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">h_g_sum</span></div>
</div>

</pre></div>

           </div>
          </div>
          <footer>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2018, DGL Team.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>