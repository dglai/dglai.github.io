<!DOCTYPE html>
<html class="writer-html5" lang="en" data-content_root="../">
<head>
  <meta charset="utf-8" /><meta name="viewport" content="width=device-width, initial-scale=1" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>7.1 Data Preprocessing &mdash; DGL 2.4 documentation</title>
      <link rel="stylesheet" type="text/css" href="../_static/pygments.css?v=80d5e7a1" />
      <link rel="stylesheet" type="text/css" href="../_static/css/theme.css?v=19f00094" />
      <link rel="stylesheet" type="text/css" href="../_static/graphviz.css?v=fd3f3429" />
      <link rel="stylesheet" type="text/css" href="../_static/copybutton.css?v=76b2166b" />
      <link rel="stylesheet" type="text/css" href="../_static/sg_gallery.css?v=61a4c737" />
      <link rel="stylesheet" type="text/css" href="../_static/sg_gallery-binder.css?v=f4aeca0c" />
      <link rel="stylesheet" type="text/css" href="../_static/sg_gallery-dataframe.css?v=2082cf3c" />
      <link rel="stylesheet" type="text/css" href="../_static/sg_gallery-rendered-html.css?v=1277b6f3" />
      <link rel="stylesheet" type="text/css" href="../_static/css/custom.css?v=0bf289b5" />

  
  <!--[if lt IE 9]>
    <script src="../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script src="../_static/jquery.js?v=5d32c60e"></script>
        <script src="../_static/_sphinx_javascript_frameworks_compat.js?v=2cd50e6c"></script>
        <script src="../_static/documentation_options.js?v=9caaf7ed"></script>
        <script src="../_static/doctools.js?v=9a2dae69"></script>
        <script src="../_static/sphinx_highlight.js?v=dc90522c"></script>
        <script src="../_static/clipboard.min.js?v=a7894cd8"></script>
        <script src="../_static/copybutton.js?v=ccdb6887"></script>
        <script crossorigin="anonymous" integrity="sha256-Ae2Vz/4ePdIu6ZyI/5ZGsYnb+m0JlOmKPjt6XZ9JJkA=" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
        <script src="https://cdn.jsdelivr.net/npm/@jupyter-widgets/html-manager@^1.0.1/dist/embed-amd.js"></script>
        <script>window.MathJax = {"tex": {"inlineMath": [["$", "$"], ["\\(", "\\)"]], "processEscapes": true}, "options": {"ignoreHtmlClass": "tex2jax_ignore|mathjax_ignore|document", "processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
        <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script src="../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="7.2 Tools for launching distributed training/inference" href="distributed-tools.html" />
    <link rel="prev" title="Chapter 7: Distributed Training" href="distributed.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="../index.html" class="icon icon-home">
            DGL
          </a>
              <div class="version">
                2.4
              </div>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">Get Started</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../install/index.html">Install and Setup</a></li>
<li class="toctree-l1"><a class="reference internal" href="../tutorials/blitz/index.html">A Blitz Introduction to DGL</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Advanced Materials</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../stochastic_training/index.html">üÜï Stochastic Training of GNNs with GraphBolt</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="index.html">User Guide</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="graph.html">Chapter 1: Graph</a></li>
<li class="toctree-l2"><a class="reference internal" href="message.html">Chapter 2: Message Passing</a></li>
<li class="toctree-l2"><a class="reference internal" href="nn.html">Chapter 3: Building GNN Modules</a></li>
<li class="toctree-l2"><a class="reference internal" href="data.html">Chapter 4: Graph Data Pipeline</a></li>
<li class="toctree-l2"><a class="reference internal" href="training.html">Chapter 5: Training Graph Neural Networks</a></li>
<li class="toctree-l2"><a class="reference internal" href="minibatch.html">Chapter 6: Stochastic Training on Large Graphs</a></li>
<li class="toctree-l2 current"><a class="reference internal" href="distributed.html">Chapter 7: Distributed Training</a><ul class="current">
<li class="toctree-l3 current"><a class="current reference internal" href="#">7.1 Data Preprocessing</a></li>
<li class="toctree-l3"><a class="reference internal" href="distributed-tools.html">7.2 Tools for launching distributed training/inference</a></li>
<li class="toctree-l3"><a class="reference internal" href="distributed-apis.html">7.3 Programming APIs</a></li>
<li class="toctree-l3"><a class="reference internal" href="distributed-partition.html">7.4 Advanced Graph Partitioning</a></li>
<li class="toctree-l3"><a class="reference internal" href="distributed-hetero.html">7.5 Heterogeneous Graph Under The Hood</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="mixed_precision.html">Chapter 8: Mixed Precision Training</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../guide_cn/index.html">Áî®Êà∑ÊåáÂçó„ÄêÂåÖÂê´ËøáÊó∂‰ø°ÊÅØ„Äë</a></li>
<li class="toctree-l1"><a class="reference internal" href="../guide_ko/index.html">ÏÇ¨Ïö©Ïûê Í∞ÄÏù¥Îìú[ÏãúÎåÄÏóê Îí§Ï≥êÏßÑ]</a></li>
<li class="toctree-l1"><a class="reference internal" href="../graphtransformer/index.html">üÜï Tutorial: Graph Transformer</a></li>
<li class="toctree-l1"><a class="reference internal" href="../notebooks/sparse/index.html">Tutorials: dgl.sparse</a></li>
<li class="toctree-l1"><a class="reference internal" href="../tutorials/cpu/index.html">Training on CPUs</a></li>
<li class="toctree-l1"><a class="reference internal" href="../tutorials/multi/index.html">Training on Multiple GPUs</a></li>
<li class="toctree-l1"><a class="reference internal" href="../tutorials/dist/index.html">Distributed training</a></li>
<li class="toctree-l1"><a class="reference internal" href="../tutorials/models/index.html">Paper Study with DGL</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">API Reference</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../api/python/dgl.html">dgl</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api/python/dgl.data.html">dgl.data</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api/python/dgl.dataloading.html">dgl.dataloading</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api/python/dgl.DGLGraph.html">dgl.DGLGraph</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api/python/dgl.distributed.html">dgl.distributed</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api/python/dgl.function.html">dgl.function</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api/python/dgl.geometry.html">dgl.geometry</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api/python/dgl.graphbolt.html">üÜï dgl.graphbolt</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api/python/nn-pytorch.html">dgl.nn (PyTorch)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api/python/nn.functional.html">dgl.nn.functional</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api/python/dgl.ops.html">dgl.ops</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api/python/dgl.optim.html">dgl.optim</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api/python/dgl.sampling.html">dgl.sampling</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api/python/dgl.sparse_v0.html">dgl.sparse</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api/python/dgl.multiprocessing.html">dgl.multiprocessing</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api/python/transforms.html">dgl.transforms</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api/python/udf.html">User-defined Functions</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Notes</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../contribute.html">Contribute to DGL</a></li>
<li class="toctree-l1"><a class="reference internal" href="../developer/ffi.html">DGL Foreign Function Interface (FFI)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../performance.html">Performance Benchmarks</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Misc</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../faq.html">Frequently Asked Questions (FAQ)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../env_var.html">Environment Variables</a></li>
<li class="toctree-l1"><a class="reference internal" href="../resources.html">Resources</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">DGL</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../index.html" class="icon icon-home" aria-label="Home"></a></li>
          <li class="breadcrumb-item"><a href="index.html">User Guide</a></li>
          <li class="breadcrumb-item"><a href="distributed.html">Chapter 7: Distributed Training</a></li>
      <li class="breadcrumb-item active">7.1 Data Preprocessing</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../_sources/guide/distributed-preprocessing.rst.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="data-preprocessing">
<span id="guide-distributed-preprocessing"></span><h1>7.1 Data Preprocessing<a class="headerlink" href="#data-preprocessing" title="Link to this heading">ÔÉÅ</a></h1>
<p>Before launching training jobs, DGL requires the input data to be partitioned
and distributed to the target machines. In order to handle different scales
of graphs, DGL provides 2 partitioning approaches:</p>
<ul class="simple">
<li><p>A partitioning API for graphs that can fit in a single machine memory.</p></li>
<li><p>A distributed partition pipeline for graphs beyond a single machine capacity.</p></li>
</ul>
<section id="partitioning-api">
<h2>7.1.1 Partitioning API<a class="headerlink" href="#partitioning-api" title="Link to this heading">ÔÉÅ</a></h2>
<p>For relatively small graphs, DGL provides a partitioning API
<a class="reference internal" href="../generated/dgl.distributed.partition_graph.html#dgl.distributed.partition_graph" title="dgl.distributed.partition_graph"><code class="xref py py-func docutils literal notranslate"><span class="pre">partition_graph()</span></code></a> that partitions
an in-memory <a class="reference internal" href="../api/python/dgl.DGLGraph.html#dgl.DGLGraph" title="dgl.DGLGraph"><code class="xref py py-class docutils literal notranslate"><span class="pre">DGLGraph</span></code></a> object. It supports
multiple partitioning algorithms such as random partitioning and
<a class="reference external" href="http://glaros.dtc.umn.edu/gkhome/views/metis">Metis</a>.
The benefit of Metis partitioning is that it can generate partitions with
minimal edge cuts to reduce network communication for distributed training and
inference. DGL uses the latest version of Metis with the options optimized for
the real-world graphs with power-law distribution. After partitioning, the API
constructs the partitioned results in a format that is easy to load during the
training. For example,</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">dgl</span>

<span class="n">g</span> <span class="o">=</span> <span class="o">...</span>  <span class="c1"># create or load a DGLGraph object</span>
<span class="n">dgl</span><span class="o">.</span><span class="n">distributed</span><span class="o">.</span><span class="n">partition_graph</span><span class="p">(</span><span class="n">g</span><span class="p">,</span> <span class="s1">&#39;mygraph&#39;</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="s1">&#39;data_root_dir&#39;</span><span class="p">)</span>
</pre></div>
</div>
<p>will outputs the following data file.</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>data_root_dir/
  |-- mygraph.json          # metadata JSON. File name is the given graph name.
  |-- part0/                # data for partition 0
  |  |-- node_feats.dgl     # node features stored in binary format
  |  |-- edge_feats.dgl     # edge features stored in binary format
  |  |-- graph.dgl          # graph structure of this partition stored in binary format
  |
  |-- part1/                # data for partition 1
     |-- node_feats.dgl
     |-- edge_feats.dgl
     |-- graph.dgl
</pre></div>
</div>
<p>Chapter <a class="reference internal" href="distributed-partition.html#guide-distributed-partition"><span class="std std-ref">7.4 Advanced Graph Partitioning</span></a> covers more details about the
partition format. To distribute the partitions to a cluster, users can either save
the data in some shared folder accessible by all machines, or copy the metadata
JSON as well as the corresponding partition folder <code class="docutils literal notranslate"><span class="pre">partX</span></code> to the X^th machine.</p>
<p>Using <a class="reference internal" href="../generated/dgl.distributed.partition_graph.html#dgl.distributed.partition_graph" title="dgl.distributed.partition_graph"><code class="xref py py-func docutils literal notranslate"><span class="pre">partition_graph()</span></code></a> requires an instance with large enough
CPU RAM to hold the entire graph structure and features, which may not be viable for
graphs with hundreds of billions of edges or large features. We describe how to use
the <em>parallel data preparation pipeline</em> for such cases next.</p>
<section id="load-balancing">
<h3>Load balancing<a class="headerlink" href="#load-balancing" title="Link to this heading">ÔÉÅ</a></h3>
<p>When partitioning a graph, by default, METIS only balances the number of nodes
in each partition.  This can result in suboptimal configuration, depending on
the task at hand. For example, in the case of semi-supervised node
classification, a trainer performs computation on a subset of labeled nodes in
a local partition. A partitioning that only balances nodes in a graph (both
labeled and unlabeled), may end up with computational load imbalance. To get a
balanced workload in each partition, the partition API allows balancing between
partitions with respect to the number of nodes in each node type, by specifying
<code class="docutils literal notranslate"><span class="pre">balance_ntypes</span></code> in <a class="reference internal" href="../generated/dgl.distributed.partition_graph.html#dgl.distributed.partition_graph" title="dgl.distributed.partition_graph"><code class="xref py py-func docutils literal notranslate"><span class="pre">partition_graph()</span></code></a>. Users can take
advantage of this and consider nodes in the training set, validation set and
test set are of different node types.</p>
<p>The following example considers nodes inside the training set and outside the
training set are two types of nodes:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">dgl</span><span class="o">.</span><span class="n">distributed</span><span class="o">.</span><span class="n">partition_graph</span><span class="p">(</span><span class="n">g</span><span class="p">,</span> <span class="s1">&#39;graph_name&#39;</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="s1">&#39;/tmp/test&#39;</span><span class="p">,</span> <span class="n">balance_ntypes</span><span class="o">=</span><span class="n">g</span><span class="o">.</span><span class="n">ndata</span><span class="p">[</span><span class="s1">&#39;train_mask&#39;</span><span class="p">])</span>
</pre></div>
</div>
<p>In addition to balancing the node types,
<a class="reference internal" href="../generated/dgl.distributed.partition_graph.html#dgl.distributed.partition_graph" title="dgl.distributed.partition_graph"><code class="xref py py-func docutils literal notranslate"><span class="pre">dgl.distributed.partition_graph()</span></code></a> also allows balancing between
in-degrees of nodes of different node types by specifying <code class="docutils literal notranslate"><span class="pre">balance_edges</span></code>.
This balances the number of edges incident to the nodes of different types.</p>
</section>
<section id="id-mapping">
<h3>ID mapping<a class="headerlink" href="#id-mapping" title="Link to this heading">ÔÉÅ</a></h3>
<p>After partitioning, <a class="reference internal" href="../generated/dgl.distributed.partition_graph.html#dgl.distributed.partition_graph" title="dgl.distributed.partition_graph"><code class="xref py py-func docutils literal notranslate"><span class="pre">partition_graph()</span></code></a> remap node
and edge IDs so that nodes of the same partition are aranged together
(in a consecutive ID range), making it easier to store partitioned node/edge
features. The API also automatically shuffles the node/edge features
according to the new IDs. However, some downstream tasks may want to
recover the original node/edge IDs (such as extracting the computed node
embeddings for later use). For such cases, pass <code class="docutils literal notranslate"><span class="pre">return_mapping=True</span></code>
to <a class="reference internal" href="../generated/dgl.distributed.partition_graph.html#dgl.distributed.partition_graph" title="dgl.distributed.partition_graph"><code class="xref py py-func docutils literal notranslate"><span class="pre">partition_graph()</span></code></a>, which makes the API returns
the ID mappings between the remapped node/edge IDs and their origianl ones.
For a homogeneous graph, it returns two vectors. The first vector maps every new
node ID to its original ID; the second vector maps every new edge ID to
its original ID. For a heterogeneous graph, it returns two dictionaries of
vectors. The first dictionary contains the mapping for each node type; the
second dictionary contains the mapping for each edge type.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">node_map</span><span class="p">,</span> <span class="n">edge_map</span> <span class="o">=</span> <span class="n">dgl</span><span class="o">.</span><span class="n">distributed</span><span class="o">.</span><span class="n">partition_graph</span><span class="p">(</span><span class="n">g</span><span class="p">,</span> <span class="s1">&#39;graph_name&#39;</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="s1">&#39;/tmp/test&#39;</span><span class="p">,</span>
                                                     <span class="n">balance_ntypes</span><span class="o">=</span><span class="n">g</span><span class="o">.</span><span class="n">ndata</span><span class="p">[</span><span class="s1">&#39;train_mask&#39;</span><span class="p">],</span>
                                                     <span class="n">return_mapping</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="c1"># Let&#39;s assume that node_emb is saved from the distributed training.</span>
<span class="n">orig_node_emb</span> <span class="o">=</span> <span class="n">th</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">node_emb</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">node_emb</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
<span class="n">orig_node_emb</span><span class="p">[</span><span class="n">node_map</span><span class="p">]</span> <span class="o">=</span> <span class="n">node_emb</span>
</pre></div>
</div>
</section>
</section>
<section id="load-partitioned-graphs">
<h2>Load partitioned graphs<a class="headerlink" href="#load-partitioned-graphs" title="Link to this heading">ÔÉÅ</a></h2>
<p>DGL provides a <a class="reference internal" href="../generated/dgl.distributed.load_partition.html#dgl.distributed.load_partition" title="dgl.distributed.load_partition"><code class="xref py py-func docutils literal notranslate"><span class="pre">dgl.distributed.load_partition()</span></code></a> function to load one partition
for inspection.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">dgl</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># load partition 0</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">part_data</span> <span class="o">=</span> <span class="n">dgl</span><span class="o">.</span><span class="n">distributed</span><span class="o">.</span><span class="n">load_partition</span><span class="p">(</span><span class="s1">&#39;data_root_dir/graph_name.json&#39;</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">g</span><span class="p">,</span> <span class="n">nfeat</span><span class="p">,</span> <span class="n">efeat</span><span class="p">,</span> <span class="n">partition_book</span><span class="p">,</span> <span class="n">graph_name</span><span class="p">,</span> <span class="n">ntypes</span><span class="p">,</span> <span class="n">etypes</span> <span class="o">=</span> <span class="n">part_data</span>  <span class="c1"># unpack</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">g</span><span class="p">)</span>
<span class="go">Graph(num_nodes=966043, num_edges=34270118,</span>
<span class="go">      ndata_schemes={&#39;orig_id&#39;: Scheme(shape=(), dtype=torch.int64),</span>
<span class="go">                     &#39;part_id&#39;: Scheme(shape=(), dtype=torch.int64),</span>
<span class="go">                     &#39;_ID&#39;: Scheme(shape=(), dtype=torch.int64),</span>
<span class="go">                     &#39;inner_node&#39;: Scheme(shape=(), dtype=torch.int32)}</span>
<span class="go">      edata_schemes={&#39;_ID&#39;: Scheme(shape=(), dtype=torch.int64),</span>
<span class="go">                     &#39;inner_edge&#39;: Scheme(shape=(), dtype=torch.int8),</span>
<span class="go">                     &#39;orig_id&#39;: Scheme(shape=(), dtype=torch.int64)})</span>
</pre></div>
</div>
<p>As mentioned in the <a class="reference internal" href="#id-mapping">ID mapping</a> section, each partition carries auxiliary information
saved as ndata or edata such as original node/edge IDs, partition IDs, etc. Each partition
not only saves nodes/edges it owns, but also includes node/edges that are adjacent to
the partition (called <strong>HALO</strong> nodes/edges). The <code class="docutils literal notranslate"><span class="pre">inner_node</span></code> and <code class="docutils literal notranslate"><span class="pre">inner_edge</span></code>
indicate whether a node/edge truely belongs to the partition (value is <code class="docutils literal notranslate"><span class="pre">True</span></code>)
or is a HALO node/edge (value is <code class="docutils literal notranslate"><span class="pre">False</span></code>).</p>
<p>The <a class="reference internal" href="../generated/dgl.distributed.load_partition.html#dgl.distributed.load_partition" title="dgl.distributed.load_partition"><code class="xref py py-func docutils literal notranslate"><span class="pre">load_partition()</span></code></a> function loads all data at once. Users can
load features or the partition book using the <a class="reference internal" href="../generated/dgl.distributed.load_partition_feats.html#dgl.distributed.load_partition_feats" title="dgl.distributed.load_partition_feats"><code class="xref py py-func docutils literal notranslate"><span class="pre">dgl.distributed.load_partition_feats()</span></code></a>
and <a class="reference internal" href="../generated/dgl.distributed.load_partition_book.html#dgl.distributed.load_partition_book" title="dgl.distributed.load_partition_book"><code class="xref py py-func docutils literal notranslate"><span class="pre">dgl.distributed.load_partition_book()</span></code></a> APIs respectively.</p>
</section>
<section id="distributed-graph-partitioning-pipeline">
<h2>7.1.2 Distributed Graph Partitioning Pipeline<a class="headerlink" href="#distributed-graph-partitioning-pipeline" title="Link to this heading">ÔÉÅ</a></h2>
<p>To handle massive graph data that cannot fit in the CPU RAM of a
single machine, DGL utilizes data chunking and parallel processing to reduce
memory footprint and running time. The figure below illustrates the
pipeline:</p>
<figure class="align-default">
<img alt="https://data.dgl.ai/asset/image/guide_7_distdataprep.png" src="https://data.dgl.ai/asset/image/guide_7_distdataprep.png" />
</figure>
<ul class="simple">
<li><p>The pipeline takes input data stored in <em>Chunked Graph Format</em> and
produces and dispatches data partitions to the target machines.</p></li>
<li><p><strong>Step.1 Graph Partitioning:</strong> It calculates the ownership of each partition
and saves the results as a set of files called <em>partition assignment</em>.
To speedup the step, some algorithms (e.g., ParMETIS) support parallel computing
using multiple machines.</p></li>
<li><p><strong>Step.2 Data Dispatching:</strong> Given the partition assignment, the step then
physically partitions the graph data and dispatches them to the machines user
specified. It also converts the graph data into formats that are suitable for
distributed training and evaluation.</p></li>
</ul>
<p>The whole pipeline is modularized so that each step can be invoked
individually. For example, users can replace Step.1 with some custom graph partition
algorithm as long as it produces partition assignment files
correctly.</p>
</section>
<section id="chunked-graph-format">
<span id="guide-distributed-prep-chunk"></span><h2>Chunked Graph Format<a class="headerlink" href="#chunked-graph-format" title="Link to this heading">ÔÉÅ</a></h2>
<p>To run the pipeline, DGL requires the input graph to be stored in multiple data
chunks.  Each data chunk is the unit of data preprocessing and thus should fit
into CPU RAM.  In this section, we use the MAG240M-LSC data from <a class="reference external" href="https://ogb.stanford.edu/docs/lsc/mag240m/">Open Graph
Benchmark</a>  as an example to
describe the overall design, followed by a formal specification and
tips for creating data in such format.</p>
<section id="example-mag240m-lsc">
<h3>Example: MAG240M-LSC<a class="headerlink" href="#example-mag240m-lsc" title="Link to this heading">ÔÉÅ</a></h3>
<p>The MAG240M-LSC graph is a heterogeneous academic graph
extracted from the Microsoft Academic Graph (MAG), whose schema diagram is
illustrated below:</p>
<figure class="align-default">
<img alt="https://data.dgl.ai/asset/image/guide_7_mag240m.png" src="https://data.dgl.ai/asset/image/guide_7_mag240m.png" />
</figure>
<p>Its raw data files are organized as follows:</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>/mydata/MAG240M-LSC/
  |-- meta.pt   # # A dictionary of the number of nodes for each type saved by torch.save,
  |             # as well as num_classes
  |-- processed/
    |-- author___affiliated_with___institution/
    |  |-- edge_index.npy            # graph, 713 MB
    |
    |-- paper/
    |  |-- node_feat.npy             # feature, 187 GB, (numpy memmap format)
    |  |-- node_label.npy            # label, 974 MB
    |  |-- node_year.npy             # year, 974 MB
    |
    |-- paper___cites___paper/
    |  |-- edge_index.npy            # graph, 21 GB
    |
    |-- author___writes___paper/
       |-- edge_index.npy            # graph, 6GB
</pre></div>
</div>
<p>The graph has three node types (<code class="docutils literal notranslate"><span class="pre">&quot;paper&quot;</span></code>, <code class="docutils literal notranslate"><span class="pre">&quot;author&quot;</span></code> and <code class="docutils literal notranslate"><span class="pre">&quot;institution&quot;</span></code>),
three edge types/relations (<code class="docutils literal notranslate"><span class="pre">&quot;cites&quot;</span></code>, <code class="docutils literal notranslate"><span class="pre">&quot;writes&quot;</span></code> and <code class="docutils literal notranslate"><span class="pre">&quot;affiliated_with&quot;</span></code>). The
<code class="docutils literal notranslate"><span class="pre">&quot;paper&quot;</span></code> nodes have three attributes (<code class="docutils literal notranslate"><span class="pre">&quot;feat&quot;</span></code>, <code class="docutils literal notranslate"><span class="pre">&quot;label&quot;</span></code>, <code class="docutils literal notranslate"><span class="pre">&quot;year&quot;'</span></code>), while
other types of nodes and edges are featureless. Below shows the data files when
it is stored in DGL Chunked Graph Format:</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>/mydata/MAG240M-LSC_chunked/
  |-- metadata.json            # metadata json file
  |-- edges/                   # stores edge ID data
  |  |-- writes-part1.csv
  |  |-- writes-part2.csv
  |  |-- affiliated_with-part1.csv
  |  |-- affiliated_with-part2.csv
  |  |-- cites-part1.csv
  |  |-- cites-part1.csv
  |
  |-- node_data/               # stores node feature data
     |-- paper-feat-part1.npy
     |-- paper-feat-part2.npy
     |-- paper-label-part1.npy
     |-- paper-label-part2.npy
     |-- paper-year-part1.npy
     |-- paper-year-part2.npy
</pre></div>
</div>
<p>All the data files are chunked into two parts, including the edges of each relation
(e.g., writes, affiliates, cites) and node features. If the graph has edge features,
they will be chunked into multiple files too. All ID data are stored in
CSV (we will illustrate the contents soon) while node features are stored in
numpy arrays.</p>
<p>The <code class="docutils literal notranslate"><span class="pre">metadata.json</span></code> stores all the metadata information such as file names
and chunk sizes (e.g., number of nodes, number of edges).</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="p">{</span>
   <span class="s2">&quot;graph_name&quot;</span> <span class="p">:</span> <span class="s2">&quot;MAG240M-LSC&quot;</span><span class="p">,</span>  <span class="c1"># given graph name</span>
   <span class="s2">&quot;node_type&quot;</span><span class="p">:</span> <span class="p">[</span><span class="s2">&quot;author&quot;</span><span class="p">,</span> <span class="s2">&quot;paper&quot;</span><span class="p">,</span> <span class="s2">&quot;institution&quot;</span><span class="p">],</span>
   <span class="s2">&quot;num_nodes_per_chunk&quot;</span><span class="p">:</span> <span class="p">[</span>
       <span class="p">[</span><span class="mi">61191556</span><span class="p">,</span> <span class="mi">61191556</span><span class="p">],</span>      <span class="c1"># number of author nodes per chunk</span>
       <span class="p">[</span><span class="mi">61191553</span><span class="p">,</span> <span class="mi">61191552</span><span class="p">],</span>      <span class="c1"># number of paper nodes per chunk</span>
       <span class="p">[</span><span class="mi">12861</span><span class="p">,</span> <span class="mi">12860</span><span class="p">]</span>             <span class="c1"># number of institution nodes per chunk</span>
   <span class="p">],</span>
   <span class="c1"># The edge type name is a colon-joined string of source, edge, and destination type.</span>
   <span class="s2">&quot;edge_type&quot;</span><span class="p">:</span> <span class="p">[</span>
       <span class="s2">&quot;author:writes:paper&quot;</span><span class="p">,</span>
       <span class="s2">&quot;author:affiliated_with:institution&quot;</span><span class="p">,</span>
       <span class="s2">&quot;paper:cites:paper&quot;</span>
   <span class="p">],</span>
   <span class="s2">&quot;num_edges_per_chunk&quot;</span><span class="p">:</span> <span class="p">[</span>
       <span class="p">[</span><span class="mi">193011360</span><span class="p">,</span> <span class="mi">193011360</span><span class="p">],</span>    <span class="c1"># number of author:writes:paper edges per chunk</span>
       <span class="p">[</span><span class="mi">22296293</span><span class="p">,</span> <span class="mi">22296293</span><span class="p">],</span>      <span class="c1"># number of author:affiliated_with:institution edges per chunk</span>
       <span class="p">[</span><span class="mi">648874463</span><span class="p">,</span> <span class="mi">648874463</span><span class="p">]</span>     <span class="c1"># number of paper:cites:paper edges per chunk</span>
   <span class="p">],</span>
   <span class="s2">&quot;edges&quot;</span> <span class="p">:</span> <span class="p">{</span>
        <span class="s2">&quot;author:writes:paper&quot;</span> <span class="p">:</span> <span class="p">{</span>  <span class="c1"># edge type</span>
             <span class="s2">&quot;format&quot;</span> <span class="p">:</span> <span class="p">{</span><span class="s2">&quot;name&quot;</span><span class="p">:</span> <span class="s2">&quot;csv&quot;</span><span class="p">,</span> <span class="s2">&quot;delimiter&quot;</span><span class="p">:</span> <span class="s2">&quot; &quot;</span><span class="p">},</span>
             <span class="c1"># The list of paths. Can be relative or absolute.</span>
             <span class="s2">&quot;data&quot;</span> <span class="p">:</span> <span class="p">[</span><span class="s2">&quot;edges/writes-part1.csv&quot;</span><span class="p">,</span> <span class="s2">&quot;edges/writes-part2.csv&quot;</span><span class="p">]</span>
        <span class="p">},</span>
        <span class="s2">&quot;author:affiliated_with:institution&quot;</span> <span class="p">:</span> <span class="p">{</span>
             <span class="s2">&quot;format&quot;</span> <span class="p">:</span> <span class="p">{</span><span class="s2">&quot;name&quot;</span><span class="p">:</span> <span class="s2">&quot;csv&quot;</span><span class="p">,</span> <span class="s2">&quot;delimiter&quot;</span><span class="p">:</span> <span class="s2">&quot; &quot;</span><span class="p">},</span>
             <span class="s2">&quot;data&quot;</span> <span class="p">:</span> <span class="p">[</span><span class="s2">&quot;edges/affiliated_with-part1.csv&quot;</span><span class="p">,</span> <span class="s2">&quot;edges/affiliated_with-part2.csv&quot;</span><span class="p">]</span>
        <span class="p">},</span>
        <span class="s2">&quot;paper:cites:paper&quot;</span> <span class="p">:</span> <span class="p">{</span>
             <span class="s2">&quot;format&quot;</span> <span class="p">:</span> <span class="p">{</span><span class="s2">&quot;name&quot;</span><span class="p">:</span> <span class="s2">&quot;csv&quot;</span><span class="p">,</span> <span class="s2">&quot;delimiter&quot;</span><span class="p">:</span> <span class="s2">&quot; &quot;</span><span class="p">},</span>
             <span class="s2">&quot;data&quot;</span> <span class="p">:</span> <span class="p">[</span><span class="s2">&quot;edges/cites-part1.csv&quot;</span><span class="p">,</span> <span class="s2">&quot;edges/cites-part2.csv&quot;</span><span class="p">]</span>
        <span class="p">}</span>
   <span class="p">},</span>
   <span class="s2">&quot;node_data&quot;</span> <span class="p">:</span> <span class="p">{</span>
        <span class="s2">&quot;paper&quot;</span><span class="p">:</span> <span class="p">{</span>       <span class="c1"># node type</span>
             <span class="s2">&quot;feat&quot;</span><span class="p">:</span> <span class="p">{</span>   <span class="c1"># feature key</span>
                 <span class="s2">&quot;format&quot;</span><span class="p">:</span> <span class="p">{</span><span class="s2">&quot;name&quot;</span><span class="p">:</span> <span class="s2">&quot;numpy&quot;</span><span class="p">},</span>
                 <span class="s2">&quot;data&quot;</span><span class="p">:</span> <span class="p">[</span><span class="s2">&quot;node_data/paper-feat-part1.npy&quot;</span><span class="p">,</span> <span class="s2">&quot;node_data/paper-feat-part2.npy&quot;</span><span class="p">]</span>
             <span class="p">},</span>
             <span class="s2">&quot;label&quot;</span><span class="p">:</span> <span class="p">{</span>   <span class="c1"># feature key</span>
                 <span class="s2">&quot;format&quot;</span><span class="p">:</span> <span class="p">{</span><span class="s2">&quot;name&quot;</span><span class="p">:</span> <span class="s2">&quot;numpy&quot;</span><span class="p">},</span>
                 <span class="s2">&quot;data&quot;</span><span class="p">:</span> <span class="p">[</span><span class="s2">&quot;node_data/paper-label-part1.npy&quot;</span><span class="p">,</span> <span class="s2">&quot;node_data/paper-label-part2.npy&quot;</span><span class="p">]</span>
             <span class="p">},</span>
             <span class="s2">&quot;year&quot;</span><span class="p">:</span> <span class="p">{</span>   <span class="c1"># feature key</span>
                 <span class="s2">&quot;format&quot;</span><span class="p">:</span> <span class="p">{</span><span class="s2">&quot;name&quot;</span><span class="p">:</span> <span class="s2">&quot;numpy&quot;</span><span class="p">},</span>
                 <span class="s2">&quot;data&quot;</span><span class="p">:</span> <span class="p">[</span><span class="s2">&quot;node_data/paper-year-part1.npy&quot;</span><span class="p">,</span> <span class="s2">&quot;node_data/paper-year-part2.npy&quot;</span><span class="p">]</span>
             <span class="p">}</span>
        <span class="p">}</span>
   <span class="p">},</span>
   <span class="s2">&quot;edge_data&quot;</span> <span class="p">:</span> <span class="p">{}</span>  <span class="c1"># MAG240M-LSC does not have edge features</span>
<span class="p">}</span>
</pre></div>
</div>
<p>There are three parts in <code class="docutils literal notranslate"><span class="pre">metadata.json</span></code>:</p>
<ul class="simple">
<li><p>Graph schema information and chunk sizes, e.g., <code class="docutils literal notranslate"><span class="pre">&quot;node_type&quot;</span></code> , <code class="docutils literal notranslate"><span class="pre">&quot;num_nodes_per_chunk&quot;</span></code>, etc.</p></li>
<li><p>Edge index data under key <code class="docutils literal notranslate"><span class="pre">&quot;edges&quot;</span></code>.</p></li>
<li><p>Node/edge feature data under keys <code class="docutils literal notranslate"><span class="pre">&quot;node_data&quot;</span></code> and <code class="docutils literal notranslate"><span class="pre">&quot;edge_data&quot;</span></code>.</p></li>
</ul>
<p>The edge index files contain edges in the form of node ID pairs:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="c1"># writes-part1.csv</span>
<span class="m">0</span><span class="w"> </span><span class="m">0</span>
<span class="m">0</span><span class="w"> </span><span class="m">1</span>
<span class="m">0</span><span class="w"> </span><span class="m">20</span>
<span class="m">0</span><span class="w"> </span><span class="m">29</span>
<span class="m">0</span><span class="w"> </span><span class="m">1203</span>
...
</pre></div>
</div>
</section>
<section id="specification">
<h3>Specification<a class="headerlink" href="#specification" title="Link to this heading">ÔÉÅ</a></h3>
<p>In general, a chunked graph data folder just needs a <code class="docutils literal notranslate"><span class="pre">metadata.json</span></code> and a
bunch of data files. The folder structure in the MAG240M-LSC example is not a
strict requirement as long as <code class="docutils literal notranslate"><span class="pre">metadata.json</span></code> contains valid file paths.</p>
<p><code class="docutils literal notranslate"><span class="pre">metadata.json</span></code> top-level keys:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">graph_name</span></code>: String. Unique name used by <a class="reference internal" href="../api/python/dgl.distributed.html#dgl.distributed.DistGraph" title="dgl.distributed.DistGraph"><code class="xref py py-class docutils literal notranslate"><span class="pre">dgl.distributed.DistGraph</span></code></a>
to load graph.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">node_type</span></code>: List of string. Node type names.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">num_nodes_per_chunk</span></code>: List of list of integer. For graphs with <span class="math notranslate nohighlight">\(T\)</span> node
types stored in <span class="math notranslate nohighlight">\(P\)</span> chunks, the value contains <span class="math notranslate nohighlight">\(T\)</span> integer lists.
Each list contains <span class="math notranslate nohighlight">\(P\)</span> integers, which specify the number of nodes
in each chunk.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">edge_type</span></code>: List of string. Edge type names in the form of
<code class="docutils literal notranslate"><span class="pre">&lt;source</span> <span class="pre">node</span> <span class="pre">type&gt;:&lt;relation&gt;:&lt;destination</span> <span class="pre">node</span> <span class="pre">type&gt;</span></code>.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">num_edges_per_chunk</span></code>: List of list of integer. For graphs with <span class="math notranslate nohighlight">\(R\)</span> edge
types stored in <span class="math notranslate nohighlight">\(P\)</span> chunks, the value contains <span class="math notranslate nohighlight">\(R\)</span> integer lists.
Each list contains <span class="math notranslate nohighlight">\(P\)</span> integers, which specify the number of edges
in each chunk.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">edges</span></code>: Dict of <code class="docutils literal notranslate"><span class="pre">ChunkFileSpec</span></code>. Edge index files.
Dictionary keys are edge type names in the form of
<code class="docutils literal notranslate"><span class="pre">&lt;source</span> <span class="pre">node</span> <span class="pre">type&gt;:&lt;relation&gt;:&lt;destination</span> <span class="pre">node</span> <span class="pre">type&gt;</span></code>.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">node_data</span></code>: Dict of <code class="docutils literal notranslate"><span class="pre">ChunkFileSpec</span></code>. Data files that store node attributes
could have arbitrary number of files regardless of <code class="docutils literal notranslate"><span class="pre">num_parts</span></code>. Dictionary
keys are node type names.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">edge_data</span></code>: Dict of <code class="docutils literal notranslate"><span class="pre">ChunkFileSpec</span></code>. Data files that store edge attributes
could have arbitrary number of files regardless of <code class="docutils literal notranslate"><span class="pre">num_parts</span></code>. Dictionary
keys are edge type names in the form of
<code class="docutils literal notranslate"><span class="pre">&lt;source</span> <span class="pre">node</span> <span class="pre">type&gt;:&lt;relation&gt;:&lt;destination</span> <span class="pre">node</span> <span class="pre">type&gt;</span></code>.</p></li>
</ul>
<p><code class="docutils literal notranslate"><span class="pre">ChunkFileSpec</span></code> has two keys:</p>
<ul>
<li><p><code class="docutils literal notranslate"><span class="pre">format</span></code>: File format. Depending on the format <code class="docutils literal notranslate"><span class="pre">name</span></code>, users can configure more
details about how to parse each data file.</p>
<blockquote>
<div><ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">&quot;csv&quot;</span></code>: CSV file. Use the <code class="docutils literal notranslate"><span class="pre">delimiter</span></code> key to specify delimiter in use.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">&quot;numpy&quot;</span></code>: NumPy array binary file created by <a class="reference external" href="https://numpy.org/doc/stable/reference/generated/numpy.save.html#numpy.save" title="(in NumPy v2.0)"><code class="xref py py-func docutils literal notranslate"><span class="pre">numpy.save()</span></code></a>.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">&quot;parquet&quot;</span></code>: parquet table binary file created by <code class="xref py py-func docutils literal notranslate"><span class="pre">pyarrow.parquet.write_table()</span></code>.</p></li>
</ul>
</div></blockquote>
</li>
<li><p><code class="docutils literal notranslate"><span class="pre">data</span></code>: List of string. File path to each data chunk. Support absolute path.</p></li>
</ul>
</section>
<section id="tips-for-making-chunked-graph-data">
<h3>Tips for making chunked graph data<a class="headerlink" href="#tips-for-making-chunked-graph-data" title="Link to this heading">ÔÉÅ</a></h3>
<p>Depending on the raw data, the implementation could include:</p>
<ul class="simple">
<li><p>Construct graphs out of non-structured data such as texts or tabular data.</p></li>
<li><p>Augment or transform the input graph struture or features. E.g., adding reverse
or self-loop edges, normalizing features, etc.</p></li>
<li><p>Chunk the input graph structure and features into multiple data files so that
each one can fit in CPU RAM for subsequent preprocessing steps.</p></li>
</ul>
<p>To avoid running into out-of-memory error, it is recommended to process graph
structures and feature data separately. Processing one chunk at a time can also
reduce the maximal runtime memory footprint. As an example, DGL provides a
<a class="reference external" href="https://github.com/dmlc/dgl/blob/master/tools/chunk_graph.py">tools/chunk_graph.py</a> script that
chunks an in-memory feature-less <a class="reference internal" href="../api/python/dgl.DGLGraph.html#dgl.DGLGraph" title="dgl.DGLGraph"><code class="xref py py-class docutils literal notranslate"><span class="pre">DGLGraph</span></code></a> and feature tensors
stored in <a class="reference external" href="https://numpy.org/doc/stable/reference/generated/numpy.memmap.html#numpy.memmap" title="(in NumPy v2.0)"><code class="xref py py-class docutils literal notranslate"><span class="pre">numpy.memmap</span></code></a>.</p>
</section>
</section>
<section id="step-1-graph-partitioning">
<span id="guide-distributed-prep-partition"></span><h2>Step.1 Graph Partitioning<a class="headerlink" href="#step-1-graph-partitioning" title="Link to this heading">ÔÉÅ</a></h2>
<p>This step reads the chunked graph data and calculates which partition each node
should belong to. The results are saved in a set of <em>partition assignment files</em>.
For example, to randomly partition MAG240M-LSC to two parts, run the
<code class="docutils literal notranslate"><span class="pre">partition_algo/random_partition.py</span></code> script in the <code class="docutils literal notranslate"><span class="pre">tools</span></code> folder:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>python<span class="w"> </span>/my/repo/dgl/tools/partition_algo/random_partition.py
<span class="w">    </span>--in_dir<span class="w"> </span>/mydata/MAG240M-LSC_chunked
<span class="w">    </span>--out_dir<span class="w"> </span>/mydata/MAG240M-LSC_2parts
<span class="w">    </span>--num_partitions<span class="w"> </span><span class="m">2</span>
</pre></div>
</div>
<p>, which outputs files as follows:</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>MAG240M-LSC_2parts/
  |-- paper.txt
  |-- author.txt
  |-- institution.txt
</pre></div>
</div>
<p>Each file stores the partition assignment of the corresponding node type.
The contents are the partition ID of each node stored in lines, i.e., line i is
the partition ID of node i.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="c1"># paper.txt</span>
<span class="m">0</span>
<span class="m">1</span>
<span class="m">1</span>
<span class="m">0</span>
<span class="m">0</span>
<span class="m">1</span>
<span class="m">0</span>
...
</pre></div>
</div>
<p>Despite its simplicity, random partitioning may result in frequent
cross-machine communication.  Check out chapter
<a class="reference internal" href="distributed-partition.html#guide-distributed-partition"><span class="std std-ref">7.4 Advanced Graph Partitioning</span></a> for more advanced options.</p>
</section>
<section id="step-2-data-dispatching">
<h2>Step.2 Data Dispatching<a class="headerlink" href="#step-2-data-dispatching" title="Link to this heading">ÔÉÅ</a></h2>
<p>DGL provides a <code class="docutils literal notranslate"><span class="pre">dispatch_data.py</span></code> script to physically partition the data and
dispatch partitions to each training machines. It will also convert the data
once again to data objects that can be loaded by DGL training processes
efficiently. The entire step can be further accelerated using multi-processing.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>python<span class="w"> </span>/myrepo/dgl/tools/dispatch_data.py<span class="w">         </span><span class="se">\</span>
<span class="w">   </span>--in-dir<span class="w"> </span>/mydata/MAG240M-LSC_chunked/<span class="w">          </span><span class="se">\</span>
<span class="w">   </span>--partitions-dir<span class="w"> </span>/mydata/MAG240M-LSC_2parts/<span class="w">   </span><span class="se">\</span>
<span class="w">   </span>--out-dir<span class="w"> </span>data/MAG_LSC_partitioned<span class="w">            </span><span class="se">\</span>
<span class="w">   </span>--ip-config<span class="w"> </span>ip_config.txt
</pre></div>
</div>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">--in-dir</span></code> specifies the path to the folder of the input chunked graph data produced</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">--partitions-dir</span></code> specifies the path to the partition assignment folder produced by Step.1.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">--out-dir</span></code> specifies the path to stored the data partition on each machine.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">--ip-config</span></code> specifies the IP configuration file of the cluster.</p></li>
</ul>
<p>An example IP configuration file is as follows:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="m">172</span>.31.19.1
<span class="m">172</span>.31.23.205
</pre></div>
</div>
<p>As a counterpart of <code class="docutils literal notranslate"><span class="pre">return_mapping=True</span></code> in <a class="reference internal" href="../generated/dgl.distributed.partition_graph.html#dgl.distributed.partition_graph" title="dgl.distributed.partition_graph"><code class="xref py py-func docutils literal notranslate"><span class="pre">partition_graph()</span></code></a>, the
<a class="reference internal" href="#guide-distributed-preprocessing"><span class="std std-ref">distributed partitioning pipeline</span></a>
provides two arguments in <code class="docutils literal notranslate"><span class="pre">dispatch_data.py</span></code> to save the original node/edge IDs to disk.</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">--save-orig-nids</span></code> save original node IDs into files.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">--save-orig-eids</span></code> save original edge IDs into files.</p></li>
</ul>
<p>Specifying the two options will create two files <code class="docutils literal notranslate"><span class="pre">orig_nids.dgl</span></code> and <code class="docutils literal notranslate"><span class="pre">orig_eids.dgl</span></code>
under each partition folder.</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>data_root_dir/
  |-- graph_name.json       # partition configuration file in JSON
  |-- part0/                # data for partition 0
  |  |-- orig_nids.dgl      # original node IDs
  |  |-- orig_eids.dgl      # original edge IDs
  |  |-- ...                # other data such as graph and node/edge feats
  |
  |-- part1/                # data for partition 1
  |  |-- orig_nids.dgl
  |  |-- orig_eids.dgl
  |  |-- ...
  |
  |-- ...                   # data for other partitions
</pre></div>
</div>
<p>The two files store the original IDs as a dictionary of tensors, where keys are node/edge
type names and values are ID tensors. Users can use the <code class="xref py py-func docutils literal notranslate"><span class="pre">dgl.data.load_tensors()</span></code>
utility to load them:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Load the original IDs for the nodes in partition 0.</span>
<span class="n">orig_nids_0</span> <span class="o">=</span> <span class="n">dgl</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">load_tensors</span><span class="p">(</span><span class="s1">&#39;/path/to/data/part0/orig_nids.dgl&#39;</span><span class="p">)</span>
<span class="c1"># Get the original node IDs for node type &#39;user&#39;</span>
<span class="n">user_orig_nids_0</span> <span class="o">=</span> <span class="n">orig_nids_0</span><span class="p">[</span><span class="s1">&#39;user&#39;</span><span class="p">]</span>

<span class="c1"># Load the original IDs for the edges in partition 0.</span>
<span class="n">orig_eids_0</span> <span class="o">=</span> <span class="n">dgl</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">load_tensors</span><span class="p">(</span><span class="s1">&#39;/path/to/data/part0/orig_eids.dgl&#39;</span><span class="p">)</span>
<span class="c1"># Get the original edge IDs for edge type &#39;like&#39;</span>
<span class="n">like_orig_eids_0</span> <span class="o">=</span> <span class="n">orig_nids_0</span><span class="p">[</span><span class="s1">&#39;like&#39;</span><span class="p">]</span>
</pre></div>
</div>
<p>During data dispatching, DGL assumes that the combined CPU RAM of the cluster
is able to hold the entire graph data. Node ownership is determined by the result
of partitioning algorithm where as for edges the owner of the destination node
also owns the edge as well.</p>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="distributed.html" class="btn btn-neutral float-left" title="Chapter 7: Distributed Training" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="distributed-tools.html" class="btn btn-neutral float-right" title="7.2 Tools for launching distributed training/inference" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2018, DGL Team.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>