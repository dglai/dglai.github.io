<!DOCTYPE html>
<html class="writer-html5" lang="en" data-content_root="../">
<head>
  <meta charset="utf-8" /><meta name="viewport" content="width=device-width, initial-scale=1" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Build Model &mdash; DGL 2.4 documentation</title>
      <link rel="stylesheet" type="text/css" href="../_static/pygments.css?v=80d5e7a1" />
      <link rel="stylesheet" type="text/css" href="../_static/css/theme.css?v=19f00094" />
      <link rel="stylesheet" type="text/css" href="../_static/graphviz.css?v=fd3f3429" />
      <link rel="stylesheet" type="text/css" href="../_static/copybutton.css?v=76b2166b" />
      <link rel="stylesheet" type="text/css" href="../_static/sg_gallery.css?v=61a4c737" />
      <link rel="stylesheet" type="text/css" href="../_static/sg_gallery-binder.css?v=f4aeca0c" />
      <link rel="stylesheet" type="text/css" href="../_static/sg_gallery-dataframe.css?v=2082cf3c" />
      <link rel="stylesheet" type="text/css" href="../_static/sg_gallery-rendered-html.css?v=1277b6f3" />
      <link rel="stylesheet" type="text/css" href="../_static/css/custom.css?v=0bf289b5" />

  
  <!--[if lt IE 9]>
    <script src="../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script src="../_static/jquery.js?v=5d32c60e"></script>
        <script src="../_static/_sphinx_javascript_frameworks_compat.js?v=2cd50e6c"></script>
        <script src="../_static/documentation_options.js?v=9caaf7ed"></script>
        <script src="../_static/doctools.js?v=9a2dae69"></script>
        <script src="../_static/sphinx_highlight.js?v=dc90522c"></script>
        <script src="../_static/clipboard.min.js?v=a7894cd8"></script>
        <script src="../_static/copybutton.js?v=ccdb6887"></script>
        <script crossorigin="anonymous" integrity="sha256-Ae2Vz/4ePdIu6ZyI/5ZGsYnb+m0JlOmKPjt6XZ9JJkA=" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
        <script src="https://cdn.jsdelivr.net/npm/@jupyter-widgets/html-manager@^1.0.1/dist/embed-amd.js"></script>
    <script src="../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Prepare Data" href="data.html" />
    <link rel="prev" title="🆕 Tutorial: Graph Transformer" href="index.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="../index.html" class="icon icon-home">
            DGL
          </a>
              <div class="version">
                2.4
              </div>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">Get Started</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../install/index.html">Install and Setup</a></li>
<li class="toctree-l1"><a class="reference internal" href="../tutorials/blitz/index.html">A Blitz Introduction to DGL</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Advanced Materials</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../stochastic_training/index.html">🆕 Stochastic Training of GNNs with GraphBolt</a></li>
<li class="toctree-l1"><a class="reference internal" href="../guide/index.html">User Guide</a></li>
<li class="toctree-l1"><a class="reference internal" href="../guide_cn/index.html">用户指南【包含过时信息】</a></li>
<li class="toctree-l1"><a class="reference internal" href="../guide_ko/index.html">사용자 가이드[시대에 뒤쳐진]</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="index.html">🆕 Tutorial: Graph Transformer</a><ul class="current">
<li class="toctree-l2 current"><a class="current reference internal" href="#">Build Model</a></li>
<li class="toctree-l2"><a class="reference internal" href="data.html">Prepare Data</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../notebooks/sparse/index.html">Tutorials: dgl.sparse</a></li>
<li class="toctree-l1"><a class="reference internal" href="../tutorials/cpu/index.html">Training on CPUs</a></li>
<li class="toctree-l1"><a class="reference internal" href="../tutorials/multi/index.html">Training on Multiple GPUs</a></li>
<li class="toctree-l1"><a class="reference internal" href="../tutorials/dist/index.html">Distributed training</a></li>
<li class="toctree-l1"><a class="reference internal" href="../tutorials/models/index.html">Paper Study with DGL</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">API Reference</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../api/python/dgl.html">dgl</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api/python/dgl.data.html">dgl.data</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api/python/dgl.dataloading.html">dgl.dataloading</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api/python/dgl.DGLGraph.html">dgl.DGLGraph</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api/python/dgl.distributed.html">dgl.distributed</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api/python/dgl.function.html">dgl.function</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api/python/dgl.geometry.html">dgl.geometry</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api/python/dgl.graphbolt.html">🆕 dgl.graphbolt</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api/python/nn-pytorch.html">dgl.nn (PyTorch)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api/python/nn.functional.html">dgl.nn.functional</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api/python/dgl.ops.html">dgl.ops</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api/python/dgl.optim.html">dgl.optim</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api/python/dgl.sampling.html">dgl.sampling</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api/python/dgl.sparse_v0.html">dgl.sparse</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api/python/dgl.multiprocessing.html">dgl.multiprocessing</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api/python/transforms.html">dgl.transforms</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api/python/udf.html">User-defined Functions</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Notes</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../contribute.html">Contribute to DGL</a></li>
<li class="toctree-l1"><a class="reference internal" href="../developer/ffi.html">DGL Foreign Function Interface (FFI)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../performance.html">Performance Benchmarks</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Misc</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../faq.html">Frequently Asked Questions (FAQ)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../env_var.html">Environment Variables</a></li>
<li class="toctree-l1"><a class="reference internal" href="../resources.html">Resources</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">DGL</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../index.html" class="icon icon-home" aria-label="Home"></a></li>
          <li class="breadcrumb-item"><a href="index.html">🆕 Tutorial: Graph Transformer</a></li>
      <li class="breadcrumb-item active">Build Model</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../_sources/graphtransformer/model.rst.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="build-model">
<h1>Build Model<a class="headerlink" href="#build-model" title="Link to this heading"></a></h1>
<p><strong>GraphTransformer</strong> is a graph neural network that uses multi-head self-attention (sparse or dense) to encode the graph structure and node features. It is a generalization of the <a class="reference external" href="https://arxiv.org/abs/1706.03762">Transformer</a> architecture to arbitrary graphs.</p>
<p>In this tutorial, we will show how to build a graph transformer model with DGL using the <a class="reference external" href="https://arxiv.org/abs/2106.05234">Graphormer</a> model as an example.</p>
<p>Graphormer is a Transformer model designed for graph-structured data, which encodes the structural information of a graph into the standard Transformer. Specifically, Graphormer utilizes degree encoding to measure the importance of nodes, spatial and path Encoding to measure the relation between node pairs. The degree encoding and the node features serve as input to Graphormer, while the spatial and path encoding act as bias terms in the self-attention module.</p>
<section id="degree-encoding">
<h2>Degree Encoding<a class="headerlink" href="#degree-encoding" title="Link to this heading"></a></h2>
<p>The degree encoder is a learnable embedding layer that encodes the degree of each node into a vector. It takes as input the batched input and output degrees of graph nodes, and outputs the degree embeddings of the nodes.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">degree_encoder</span> <span class="o">=</span> <span class="n">dgl</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">DegreeEncoder</span><span class="p">(</span>
    <span class="n">max_degree</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span>  <span class="c1"># the maximum degree to cut off</span>
    <span class="n">embedding_dim</span><span class="o">=</span><span class="mi">512</span>  <span class="c1"># the dimension of the degree embedding</span>
<span class="p">)</span>
</pre></div>
</div>
</section>
<section id="path-encoding">
<h2>Path Encoding<a class="headerlink" href="#path-encoding" title="Link to this heading"></a></h2>
<p>The path encoder encodes the edge features on the shortest path between two nodes to get attention bias for the self-attention module. It takes as input the batched edge features in shape  and outputs the attention bias based on path encoding.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">path_encoder</span> <span class="o">=</span> <span class="n">PathEncoder</span><span class="p">(</span>
    <span class="n">max_len</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span>  <span class="c1"># the maximum length of the shortest path</span>
    <span class="n">feat_dim</span><span class="o">=</span><span class="mi">512</span><span class="p">,</span>  <span class="c1"># the dimension of the edge feature</span>
    <span class="n">num_heads</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span>  <span class="c1"># the number of attention heads</span>
<span class="p">)</span>
</pre></div>
</div>
</section>
<section id="spatial-encoding">
<h2>Spatial Encoding<a class="headerlink" href="#spatial-encoding" title="Link to this heading"></a></h2>
<p>The spatial encoder encodes the shortest distance between two nodes to get attention bias for the self-attention module. It takes as input the shortest distance between two nodes and outputs the attention bias based on spatial encoding.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">spatial_encoder</span> <span class="o">=</span> <span class="n">SpatialEncoder</span><span class="p">(</span>
    <span class="n">max_dist</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span>  <span class="c1"># the maximum distance between two nodes</span>
    <span class="n">num_heads</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span>  <span class="c1"># the number of attention heads</span>
<span class="p">)</span>
</pre></div>
</div>
</section>
<section id="graphormer-layer">
<h2>Graphormer Layer<a class="headerlink" href="#graphormer-layer" title="Link to this heading"></a></h2>
<p>The Graphormer layer is like a Transformer encoder layer with the Multi-head Attention part replaced with <code class="xref py py-class docutils literal notranslate"><span class="pre">BiasedMHA</span></code>. It takes in not only the input node features, but also the attention bias computed computed above, and outputs the updated node features.</p>
<p>We can stack multiple Graphormer layers as a list just like implementing a Transformer encoder in PyTorch.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">layers</span> <span class="o">=</span> <span class="n">th</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">ModuleList</span><span class="p">([</span>
    <span class="n">GraphormerLayer</span><span class="p">(</span>
        <span class="n">feat_size</span><span class="o">=</span><span class="mi">512</span><span class="p">,</span>  <span class="c1"># the dimension of the input node features</span>
        <span class="n">hidden_size</span><span class="o">=</span><span class="mi">1024</span><span class="p">,</span>  <span class="c1"># the dimension of the hidden layer</span>
        <span class="n">num_heads</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span>  <span class="c1"># the number of attention heads</span>
        <span class="n">dropout</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span>  <span class="c1"># the dropout rate</span>
        <span class="n">activation</span><span class="o">=</span><span class="n">th</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">(),</span>  <span class="c1"># the activation function</span>
        <span class="n">norm_first</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>  <span class="c1"># whether to put the normalization before attention and feedforward</span>
    <span class="p">)</span>
    <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">6</span><span class="p">)</span>
<span class="p">])</span>
</pre></div>
</div>
</section>
<section id="model-forward">
<h2>Model Forward<a class="headerlink" href="#model-forward" title="Link to this heading"></a></h2>
<p>Grouping the modules above defines the primary components of the Graphormer model. We then can define the forward process as follows:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">node_feat</span><span class="p">,</span> <span class="n">in_degree</span><span class="p">,</span> <span class="n">out_degree</span><span class="p">,</span> <span class="n">attn_mask</span><span class="p">,</span> <span class="n">path_data</span><span class="p">,</span> <span class="n">dist</span> <span class="o">=</span> \
    <span class="nb">next</span><span class="p">(</span><span class="nb">iter</span><span class="p">(</span><span class="n">dataloader</span><span class="p">))</span>  <span class="c1">#  we will use the first batch as an example</span>
<span class="n">num_graphs</span><span class="p">,</span> <span class="n">max_num_nodes</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">node_feat</span><span class="o">.</span><span class="n">shape</span>
<span class="n">deg_emb</span> <span class="o">=</span> <span class="n">degree_encoder</span><span class="p">(</span><span class="n">th</span><span class="o">.</span><span class="n">stack</span><span class="p">((</span><span class="n">in_degree</span><span class="p">,</span> <span class="n">out_degree</span><span class="p">)))</span>

<span class="c1"># node feature + degree encoding as input</span>
<span class="n">node_feat</span> <span class="o">=</span> <span class="n">node_feat</span> <span class="o">+</span> <span class="n">deg_emb</span>

<span class="c1"># spatial encoding and path encoding serve as attention bias</span>
<span class="n">path_encoding</span> <span class="o">=</span> <span class="n">path_encoder</span><span class="p">(</span><span class="n">dist</span><span class="p">,</span> <span class="n">path_data</span><span class="p">)</span>
<span class="n">spatial_encoding</span> <span class="o">=</span> <span class="n">spatial_encoder</span><span class="p">(</span><span class="n">dist</span><span class="p">)</span>
<span class="n">attn_bias</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">:,</span> <span class="mi">1</span><span class="p">:,</span> <span class="p">:]</span> <span class="o">=</span> <span class="n">path_encoding</span> <span class="o">+</span> <span class="n">spatial_encoding</span>

<span class="c1"># graphormer layers</span>
<span class="k">for</span> <span class="n">layer</span> <span class="ow">in</span> <span class="n">layers</span><span class="p">:</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">layer</span><span class="p">(</span>
        <span class="n">x</span><span class="p">,</span>
        <span class="n">attn_mask</span><span class="o">=</span><span class="n">attn_mask</span><span class="p">,</span>
        <span class="n">attn_bias</span><span class="o">=</span><span class="n">attn_bias</span><span class="p">,</span>
    <span class="p">)</span>
</pre></div>
</div>
<p>For simplicity, we omit some details in the forward process. For the complete implementation, please refer to the <a class="reference external" href="https://github.com/dmlc/dgl/tree/master/examples/core/Graphormer">Graphormer example</a>.</p>
<p>You can also explore other <a class="reference external" href="https://docs.dgl.ai/api/python/nn-pytorch.html#utility-modules-for-graph-transformer">utility modules</a> to customize your own graph transformer model. In the next section, we will show how to prepare the data for training.</p>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="index.html" class="btn btn-neutral float-left" title="🆕 Tutorial: Graph Transformer" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="data.html" class="btn btn-neutral float-right" title="Prepare Data" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2018, DGL Team.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>