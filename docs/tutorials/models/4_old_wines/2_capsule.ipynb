{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "# Capsule Network\n",
    "\n",
    "**Author**: Jinjing Zhou, [Jake Zhao](https://cs.nyu.edu/~jakezhao/), Zheng Zhang, Jinyang Li\n",
    "\n",
    "In this tutorial, you learn how to describe one of the more classical models in terms of graphs. The approach\n",
    "offers a different perspective. The tutorial describes how to implement a Capsule model for the\n",
    "[capsule network](http://arxiv.org/abs/1710.09829)_.\n",
    "\n",
    "<div class=\"alert alert-danger\"><h4>Warning</h4><p>The tutorial aims at gaining insights into the paper, with code as a mean\n",
    "    of explanation. The implementation thus is NOT optimized for running\n",
    "    efficiency. For recommended implementation, please refer to the [official\n",
    "    examples](https://github.com/dmlc/dgl/tree/master/examples).</p></div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key ideas of Capsule\n",
    "\n",
    "The Capsule model offers two key ideas: Richer representation and dynamic routing.\n",
    "\n",
    "**Richer representation** -- In classic convolutional networks, a scalar\n",
    "value represents the activation of a given feature. By contrast, a\n",
    "capsule outputs a vector. The vector's length represents the probability\n",
    "of a feature being present. The vector's orientation represents the\n",
    "various properties of the feature (such as pose, deformation, texture\n",
    "etc.).\n",
    "\n",
    "|image0|\n",
    "\n",
    "**Dynamic routing** -- The output of a capsule is sent to\n",
    "certain parents in the layer above based on how well the capsule's\n",
    "prediction agrees with that of a parent. Such dynamic\n",
    "routing-by-agreement generalizes the static routing of max-pooling.\n",
    "\n",
    "During training, routing is accomplished iteratively. Each iteration adjusts\n",
    "routing weights between capsules based on their observed agreements.\n",
    "It's a manner similar to a k-means algorithm or [competitive\n",
    "learning](https://en.wikipedia.org/wiki/Competitive_learning)_.\n",
    "\n",
    "In this tutorial, you see how a capsule's dynamic routing algorithm can be\n",
    "naturally expressed as a graph algorithm. The implementation is adapted\n",
    "from [Cedric\n",
    "Chee](https://github.com/cedrickchee/capsule-net-pytorch)_, replacing\n",
    "only the routing layer. This version achieves similar speed and accuracy.\n",
    "\n",
    "## Model implementation\n",
    "Step 1: Setup and graph initialization\n",
    "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "\n",
    "The connectivity between two layers of capsules form a directed,\n",
    "bipartite graph, as shown in the Figure below.\n",
    "\n",
    "|image1|\n",
    "\n",
    "Each node $j$ is associated with feature $v_j$,\n",
    "representing its capsuleâ€™s output. Each edge is associated with\n",
    "features $b_{ij}$ and $\\hat{u}_{j|i}$. $b_{ij}$\n",
    "determines routing weights, and $\\hat{u}_{j|i}$ represents the\n",
    "prediction of capsule $i$ for $j$.\n",
    "\n",
    "Here's how we set up the graph and initialize node and edge features.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2024-06-27T04:42:02.797041Z",
     "iopub.status.busy": "2024-06-27T04:42:02.796656Z",
     "iopub.status.idle": "2024-06-27T04:42:05.540482Z",
     "shell.execute_reply": "2024-06-27T04:42:05.539854Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"DGLBACKEND\"] = \"pytorch\"\n",
    "import dgl\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch as th\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "def init_graph(in_nodes, out_nodes, f_size):\n",
    "    u = np.repeat(np.arange(in_nodes), out_nodes)\n",
    "    v = np.tile(np.arange(in_nodes, in_nodes + out_nodes), in_nodes)\n",
    "    g = dgl.DGLGraph((u, v))\n",
    "    # init states\n",
    "    g.ndata[\"v\"] = th.zeros(in_nodes + out_nodes, f_size)\n",
    "    g.edata[\"b\"] = th.zeros(in_nodes * out_nodes, 1)\n",
    "    return g"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Define message passing functions\n",
    "\n",
    "This is the pseudocode for Capsule's routing algorithm.\n",
    "\n",
    "|image2|\n",
    "Implement pseudocode lines 4-7 in the class `DGLRoutingLayer` as the following steps:\n",
    "\n",
    "1. Calculate coupling coefficients.\n",
    "\n",
    "   -  Coefficients are the softmax over all out-edge of in-capsules.\n",
    "      $\\textbf{c}_{i,j} = \\text{softmax}(\\textbf{b}_{i,j})$.\n",
    "\n",
    "2. Calculate weighted sum over all in-capsules.\n",
    "\n",
    "   -  Output of a capsule is equal to the weighted sum of its in-capsules\n",
    "      $s_j=\\sum_i c_{ij}\\hat{u}_{j|i}$\n",
    "\n",
    "3. Squash outputs.\n",
    "\n",
    "   -  Squash the length of a Capsule's output vector to range (0,1), so it can represent the probability (of some feature being present).\n",
    "   -  $v_j=\\text{squash}(s_j)=\\frac{||s_j||^2}{1+||s_j||^2}\\frac{s_j}{||s_j||}$\n",
    "\n",
    "4. Update weights by the amount of agreement.\n",
    "\n",
    "   -  The scalar product $\\hat{u}_{j|i}\\cdot v_j$ can be considered as how well capsule $i$ agrees with $j$. It is used to update\n",
    "      $b_{ij}=b_{ij}+\\hat{u}_{j|i}\\cdot v_j$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2024-06-27T04:42:05.543516Z",
     "iopub.status.busy": "2024-06-27T04:42:05.543065Z",
     "iopub.status.idle": "2024-06-27T04:42:05.551572Z",
     "shell.execute_reply": "2024-06-27T04:42:05.551034Z"
    }
   },
   "outputs": [],
   "source": [
    "import dgl.function as fn\n",
    "\n",
    "\n",
    "class DGLRoutingLayer(nn.Module):\n",
    "    def __init__(self, in_nodes, out_nodes, f_size):\n",
    "        super(DGLRoutingLayer, self).__init__()\n",
    "        self.g = init_graph(in_nodes, out_nodes, f_size)\n",
    "        self.in_nodes = in_nodes\n",
    "        self.out_nodes = out_nodes\n",
    "        self.in_indx = list(range(in_nodes))\n",
    "        self.out_indx = list(range(in_nodes, in_nodes + out_nodes))\n",
    "\n",
    "    def forward(self, u_hat, routing_num=1):\n",
    "        self.g.edata[\"u_hat\"] = u_hat\n",
    "\n",
    "        for r in range(routing_num):\n",
    "            # step 1 (line 4): normalize over out edges\n",
    "            edges_b = self.g.edata[\"b\"].view(self.in_nodes, self.out_nodes)\n",
    "            self.g.edata[\"c\"] = F.softmax(edges_b, dim=1).view(-1, 1)\n",
    "            self.g.edata[\"c u_hat\"] = self.g.edata[\"c\"] * self.g.edata[\"u_hat\"]\n",
    "\n",
    "            # Execute step 1 & 2\n",
    "            self.g.update_all(fn.copy_e(\"c u_hat\", \"m\"), fn.sum(\"m\", \"s\"))\n",
    "\n",
    "            # step 3 (line 6)\n",
    "            self.g.nodes[self.out_indx].data[\"v\"] = self.squash(\n",
    "                self.g.nodes[self.out_indx].data[\"s\"], dim=1\n",
    "            )\n",
    "\n",
    "            # step 4 (line 7)\n",
    "            v = th.cat(\n",
    "                [self.g.nodes[self.out_indx].data[\"v\"]] * self.in_nodes, dim=0\n",
    "            )\n",
    "            self.g.edata[\"b\"] = self.g.edata[\"b\"] + (\n",
    "                self.g.edata[\"u_hat\"] * v\n",
    "            ).sum(dim=1, keepdim=True)\n",
    "\n",
    "    @staticmethod\n",
    "    def squash(s, dim=1):\n",
    "        sq = th.sum(s**2, dim=dim, keepdim=True)\n",
    "        s_norm = th.sqrt(sq)\n",
    "        s = (sq / (1.0 + sq)) * (s / s_norm)\n",
    "        return s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Testing\n",
    "\n",
    "Make a simple 20x10 capsule layer.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2024-06-27T04:42:05.553892Z",
     "iopub.status.busy": "2024-06-27T04:42:05.553570Z",
     "iopub.status.idle": "2024-06-27T04:42:05.580015Z",
     "shell.execute_reply": "2024-06-27T04:42:05.579457Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/dgl/python/dgl/heterograph.py:92: DGLWarning: Recommend creating graphs by `dgl.graph(data)` instead of `dgl.DGLGraph(data)`.\n",
      "  dgl_warning(\n"
     ]
    }
   ],
   "source": [
    "in_nodes = 20\n",
    "out_nodes = 10\n",
    "f_size = 4\n",
    "u_hat = th.randn(in_nodes * out_nodes, f_size)\n",
    "routing = DGLRoutingLayer(in_nodes, out_nodes, f_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can visualize a Capsule network's behavior by monitoring the entropy\n",
    "of coupling coefficients. They should start high and then drop, as the\n",
    "weights gradually concentrate on fewer edges.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2024-06-27T04:42:05.582501Z",
     "iopub.status.busy": "2024-06-27T04:42:05.582108Z",
     "iopub.status.idle": "2024-06-27T04:42:05.635384Z",
     "shell.execute_reply": "2024-06-27T04:42:05.634807Z"
    }
   },
   "outputs": [],
   "source": [
    "entropy_list = []\n",
    "dist_list = []\n",
    "\n",
    "for i in range(10):\n",
    "    routing(u_hat)\n",
    "    dist_matrix = routing.g.edata[\"c\"].view(in_nodes, out_nodes)\n",
    "    entropy = (-dist_matrix * th.log(dist_matrix)).sum(dim=1)\n",
    "    entropy_list.append(entropy.data.numpy())\n",
    "    dist_list.append(dist_matrix.data.numpy())\n",
    "stds = np.std(entropy_list, axis=1)\n",
    "means = np.mean(entropy_list, axis=1)\n",
    "plt.errorbar(np.arange(len(entropy_list)), means, stds, marker=\"o\")\n",
    "plt.ylabel(\"Entropy of Weight Distribution\")\n",
    "plt.xlabel(\"Number of Routing\")\n",
    "plt.xticks(np.arange(len(entropy_list)))\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|image3|\n",
    "\n",
    "Alternatively, we can also watch the evolution of histograms.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2024-06-27T04:42:05.638053Z",
     "iopub.status.busy": "2024-06-27T04:42:05.637651Z",
     "iopub.status.idle": "2024-06-27T04:42:05.886932Z",
     "shell.execute_reply": "2024-06-27T04:42:05.886290Z"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.animation as animation\n",
    "import seaborn as sns\n",
    "\n",
    "fig = plt.figure(dpi=150)\n",
    "fig.clf()\n",
    "ax = fig.subplots()\n",
    "\n",
    "\n",
    "def dist_animate(i):\n",
    "    ax.cla()\n",
    "    sns.distplot(dist_list[i].reshape(-1), kde=False, ax=ax)\n",
    "    ax.set_xlabel(\"Weight Distribution Histogram\")\n",
    "    ax.set_title(\"Routing: %d\" % (i))\n",
    "\n",
    "\n",
    "ani = animation.FuncAnimation(\n",
    "    fig, dist_animate, frames=len(entropy_list), interval=500\n",
    ")\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|image4|\n",
    "\n",
    "You can monitor the how lower-level Capsules gradually attach to one of the\n",
    "higher level ones.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2024-06-27T04:42:05.889746Z",
     "iopub.status.busy": "2024-06-27T04:42:05.889308Z",
     "iopub.status.idle": "2024-06-27T04:42:05.907955Z",
     "shell.execute_reply": "2024-06-27T04:42:05.907430Z"
    }
   },
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "from networkx.algorithms import bipartite\n",
    "\n",
    "g = routing.g.to_networkx()\n",
    "X, Y = bipartite.sets(g)\n",
    "height_in = 10\n",
    "height_out = height_in * 0.8\n",
    "height_in_y = np.linspace(0, height_in, in_nodes)\n",
    "height_out_y = np.linspace((height_in - height_out) / 2, height_out, out_nodes)\n",
    "pos = dict()\n",
    "\n",
    "fig2 = plt.figure(figsize=(8, 3), dpi=150)\n",
    "fig2.clf()\n",
    "ax = fig2.subplots()\n",
    "pos.update(\n",
    "    (n, (i, 1)) for i, n in zip(height_in_y, X)\n",
    ")  # put nodes from X at x=1\n",
    "pos.update(\n",
    "    (n, (i, 2)) for i, n in zip(height_out_y, Y)\n",
    ")  # put nodes from Y at x=2\n",
    "\n",
    "\n",
    "def weight_animate(i):\n",
    "    ax.cla()\n",
    "    ax.axis(\"off\")\n",
    "    ax.set_title(\"Routing: %d  \" % i)\n",
    "    dm = dist_list[i]\n",
    "    nx.draw_networkx_nodes(\n",
    "        g, pos, nodelist=range(in_nodes), node_color=\"r\", node_size=100, ax=ax\n",
    "    )\n",
    "    nx.draw_networkx_nodes(\n",
    "        g,\n",
    "        pos,\n",
    "        nodelist=range(in_nodes, in_nodes + out_nodes),\n",
    "        node_color=\"b\",\n",
    "        node_size=100,\n",
    "        ax=ax,\n",
    "    )\n",
    "    for edge in g.edges():\n",
    "        nx.draw_networkx_edges(\n",
    "            g,\n",
    "            pos,\n",
    "            edgelist=[edge],\n",
    "            width=dm[edge[0], edge[1] - in_nodes] * 1.5,\n",
    "            ax=ax,\n",
    "        )\n",
    "\n",
    "\n",
    "ani2 = animation.FuncAnimation(\n",
    "    fig2, weight_animate, frames=len(dist_list), interval=500\n",
    ")\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|image5|\n",
    "\n",
    "The full code of this visualization is provided on\n",
    "[GitHub](https://github.com/dmlc/dgl/blob/master/examples/pytorch/capsule/simple_routing.py)_. The complete\n",
    "code that trains on MNIST is also on [GitHub](https://github.com/dmlc/dgl/tree/tutorial/examples/pytorch/capsule)_.\n",
    "\n",
    ".. |image0| image:: https://i.imgur.com/55Ovkdh.png\n",
    ".. |image1| image:: https://i.imgur.com/9tc6GLl.png\n",
    ".. |image2| image:: https://i.imgur.com/mv1W9Rv.png\n",
    ".. |image3| image:: https://i.imgur.com/dMvu7p3.png\n",
    ".. |image4| image:: https://github.com/VoVAllen/DGL_Capsule/raw/master/routing_dist.gif\n",
    ".. |image5| image:: https://github.com/VoVAllen/DGL_Capsule/raw/master/routing_vis.gif\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
