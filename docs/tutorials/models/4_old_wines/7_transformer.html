<!DOCTYPE html>
<html class="writer-html5" lang="en" data-content_root="../../../">
<head>
  <meta charset="utf-8" /><meta name="viewport" content="width=device-width, initial-scale=1" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Transformer as a Graph Neural Network &mdash; DGL 2.4 documentation</title>
      <link rel="stylesheet" type="text/css" href="../../../_static/pygments.css?v=80d5e7a1" />
      <link rel="stylesheet" type="text/css" href="../../../_static/css/theme.css?v=19f00094" />
      <link rel="stylesheet" type="text/css" href="../../../_static/graphviz.css?v=fd3f3429" />
      <link rel="stylesheet" type="text/css" href="../../../_static/copybutton.css?v=76b2166b" />
      <link rel="stylesheet" type="text/css" href="../../../_static/sg_gallery.css?v=61a4c737" />
      <link rel="stylesheet" type="text/css" href="../../../_static/sg_gallery-binder.css?v=f4aeca0c" />
      <link rel="stylesheet" type="text/css" href="../../../_static/sg_gallery-dataframe.css?v=2082cf3c" />
      <link rel="stylesheet" type="text/css" href="../../../_static/sg_gallery-rendered-html.css?v=1277b6f3" />
      <link rel="stylesheet" type="text/css" href="../../../_static/css/custom.css?v=0bf289b5" />

  
  <!--[if lt IE 9]>
    <script src="../../../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script src="../../../_static/jquery.js?v=5d32c60e"></script>
        <script src="../../../_static/_sphinx_javascript_frameworks_compat.js?v=2cd50e6c"></script>
        <script src="../../../_static/documentation_options.js?v=9caaf7ed"></script>
        <script src="../../../_static/doctools.js?v=9a2dae69"></script>
        <script src="../../../_static/sphinx_highlight.js?v=dc90522c"></script>
        <script src="../../../_static/clipboard.min.js?v=a7894cd8"></script>
        <script src="../../../_static/copybutton.js?v=ccdb6887"></script>
        <script crossorigin="anonymous" integrity="sha256-Ae2Vz/4ePdIu6ZyI/5ZGsYnb+m0JlOmKPjt6XZ9JJkA=" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
        <script src="https://cdn.jsdelivr.net/npm/@jupyter-widgets/html-manager@^1.0.1/dist/embed-amd.js"></script>
        <script>window.MathJax = {"tex": {"inlineMath": [["$", "$"], ["\\(", "\\)"]], "processEscapes": true}, "options": {"ignoreHtmlClass": "tex2jax_ignore|mathjax_ignore|document", "processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
        <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script src="../../../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../../../genindex.html" />
    <link rel="search" title="Search" href="../../../search.html" />
    <link rel="next" title="dgl" href="../../../api/python/dgl.html" />
    <link rel="prev" title="Capsule Network" href="2_capsule.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="../../../index.html" class="icon icon-home">
            DGL
          </a>
              <div class="version">
                2.4
              </div>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">Get Started</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../install/index.html">Install and Setup</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../blitz/index.html">A Blitz Introduction to DGL</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Advanced Materials</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../../../stochastic_training/index.html">🆕 Stochastic Training of GNNs with GraphBolt</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../guide/index.html">User Guide</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../guide_cn/index.html">用户指南【包含过时信息】</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../guide_ko/index.html">사용자 가이드[시대에 뒤쳐진]</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../graphtransformer/index.html">🆕 Tutorial: Graph Transformer</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../notebooks/sparse/index.html">Tutorials: dgl.sparse</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../cpu/index.html">Training on CPUs</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../multi/index.html">Training on Multiple GPUs</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../dist/index.html">Distributed training</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="../index.html">Paper Study with DGL</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="../1_gnn/index.html">Graph neural networks and its variants</a></li>
<li class="toctree-l2"><a class="reference internal" href="../2_small_graph/index.html">Batching many small graphs</a></li>
<li class="toctree-l2"><a class="reference internal" href="../3_generative_model/index.html">Generative models</a></li>
<li class="toctree-l2 current"><a class="reference internal" href="index.html">Revisit classic models from a graph perspective</a><ul class="current">
<li class="toctree-l3"><a class="reference internal" href="2_capsule.html">Capsule Network</a></li>
<li class="toctree-l3 current"><a class="current reference internal" href="#">Transformer as a Graph Neural Network</a></li>
</ul>
</li>
</ul>
</li>
</ul>
<p class="caption" role="heading"><span class="caption-text">API Reference</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../api/python/dgl.html">dgl</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../api/python/dgl.data.html">dgl.data</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../api/python/dgl.dataloading.html">dgl.dataloading</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../api/python/dgl.DGLGraph.html">dgl.DGLGraph</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../api/python/dgl.distributed.html">dgl.distributed</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../api/python/dgl.function.html">dgl.function</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../api/python/dgl.geometry.html">dgl.geometry</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../api/python/dgl.graphbolt.html">🆕 dgl.graphbolt</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../api/python/nn-pytorch.html">dgl.nn (PyTorch)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../api/python/nn.functional.html">dgl.nn.functional</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../api/python/dgl.ops.html">dgl.ops</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../api/python/dgl.optim.html">dgl.optim</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../api/python/dgl.sampling.html">dgl.sampling</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../api/python/dgl.sparse_v0.html">dgl.sparse</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../api/python/dgl.multiprocessing.html">dgl.multiprocessing</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../api/python/transforms.html">dgl.transforms</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../api/python/udf.html">User-defined Functions</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Notes</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../contribute.html">Contribute to DGL</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../developer/ffi.html">DGL Foreign Function Interface (FFI)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../performance.html">Performance Benchmarks</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Misc</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../faq.html">Frequently Asked Questions (FAQ)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../env_var.html">Environment Variables</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../resources.html">Resources</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../../index.html">DGL</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../../../index.html" class="icon icon-home" aria-label="Home"></a></li>
          <li class="breadcrumb-item"><a href="../index.html">Paper Study with DGL</a></li>
          <li class="breadcrumb-item"><a href="index.html">Revisit classic models from a graph perspective</a></li>
      <li class="breadcrumb-item active">Transformer as a Graph Neural Network</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../../../_sources/tutorials/models/4_old_wines/7_transformer.ipynb.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="Transformer-as-a-Graph-Neural-Network">
<h1>Transformer as a Graph Neural Network<a class="headerlink" href="#Transformer-as-a-Graph-Neural-Network" title="Link to this heading"></a></h1>
<p><strong>Author</strong>: Zihao Ye, Jinjing Zhou, Qipeng Guo, Quan Gan, Zheng Zhang</p>
<div class="alert alert-danger"><h4><p>Warning</p>
</h4><p><p>The tutorial aims at gaining insights into the paper, with code as a mean of explanation. The implementation thus is NOT optimized for running efficiency. For recommended implementation, please refer to the <a class="reference external" href="https://github.com/dmlc/dgl/tree/master/examples">official examples</a>.</p>
</p></div><p>In this tutorial, you learn about a simplified implementation of the Transformer model. You can see highlights of the most important design points. For instance, there is only single-head attention. The complete code can be found <a class="reference external" href="https://github.com/dmlc/dgl/tree/master/examples/pytorch/transformer">here</a>_.</p>
<p>The overall structure is similar to the one from the research papaer <a class="reference external" href="http://nlp.seas.harvard.edu/2018/04/03/attention.html">Annotated Transformer</a>_.</p>
<p>The Transformer model, as a replacement of CNN/RNN architecture for sequence modeling, was introduced in the research paper: <a class="reference external" href="https://arxiv.org/pdf/1706.03762.pdf">Attention is All You Need</a><em>. It improved the state of the art for machine translation as well as natural language inference task (</em><a class="reference external" href="https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf">GPT</a>). Recent work on pre-training Transformer with large scale corpus
(<a class="reference external" href="https://arxiv.org/pdf/1810.04805.pdf">BERT</a>_) supports that it is capable of learning high-quality semantic representation.</p>
<p>The interesting part of Transformer is its extensive employment of attention. The classic use of attention comes from machine translation model, where the output token attends to all input tokens.</p>
<p>Transformer additionally applies <em>self-attention</em> in both decoder and encoder. This process forces words relate to each other to combine together, irrespective of their positions in the sequence. This is different from RNN-based model, where words (in the source sentence) are combined along the chain, which is thought to be too constrained.</p>
<section id="Attention-layer-of-Transformer">
<h2>Attention layer of Transformer<a class="headerlink" href="#Attention-layer-of-Transformer" title="Link to this heading"></a></h2>
<p>In the attention layer of Transformer, for each node the module learns to assign weights on its in-coming edges. For node pair <span class="math notranslate nohighlight">\((i, j)\)</span> (from <span class="math notranslate nohighlight">\(i\)</span> to <span class="math notranslate nohighlight">\(j\)</span>) with node <span class="math notranslate nohighlight">\(x_i, x_j \in \mathbb{R}^n\)</span>, the score of their connection is defined as follows:</p>
<dl class="simple">
<dt>:nbsphinx-math:<a href="#id1"><span class="problematic" id="id2">`</span></a>begin{align}q_j = W_qcdot x_j \</dt><dd><p>k_i = W_kcdot x_i\
v_i = W_vcdot x_i\
textrm{score} = q_j^T k_iend{align}`</p>
</dd>
</dl>
<p>where <span class="math notranslate nohighlight">\(W_q, W_k, W_v \in \mathbb{R}^{n\times d_k}\)</span> map the representations <span class="math notranslate nohighlight">\(x\)</span> to “query”, “key”, and “value” space respectively.</p>
<p>There are other possibilities to implement the score function. The dot product measures the similarity of a given query <span class="math notranslate nohighlight">\(q_j\)</span> and a key <span class="math notranslate nohighlight">\(k_i\)</span>: if <span class="math notranslate nohighlight">\(j\)</span> needs the information stored in <span class="math notranslate nohighlight">\(i\)</span>, the query vector at position <span class="math notranslate nohighlight">\(j\)</span> (<span class="math notranslate nohighlight">\(q_j\)</span>) is supposed to be close to key vector at position <span class="math notranslate nohighlight">\(i\)</span> (<span class="math notranslate nohighlight">\(k_i\)</span>).</p>
<p>The score is then used to compute the sum of the incoming values, normalized over the weights of edges, stored in <span class="math notranslate nohighlight">\(\textrm{wv}\)</span>. Then apply an affine layer to <span class="math notranslate nohighlight">\(\textrm{wv}\)</span> to get the output <span class="math notranslate nohighlight">\(o\)</span>:</p>
<dl class="simple">
<dt>:nbsphinx-math:<a href="#id3"><span class="problematic" id="id4">`</span></a>begin{align}w_{ji} = frac{exp{textrm{score}_{ji} }}{sumlimits_{(k, i)in E}exp{textrm{score}_{ki} }} \</dt><dd><p>textrm{wv}_i = sum_{(k, i)in E} w_{ki} v_k \
o = W_ocdot textrm{wv} \end{align}`</p>
</dd>
</dl>
<section id="Multi-head-attention-layer">
<h3>Multi-head attention layer<a class="headerlink" href="#Multi-head-attention-layer" title="Link to this heading"></a></h3>
<p>In Transformer, attention is <em>multi-headed</em>. A head is very much like a channel in a convolutional network. The multi-head attention consists of multiple attention heads, in which each head refers to a single attention module. <span class="math notranslate nohighlight">\(\textrm{wv}^{(i)}\)</span> for all the heads are concatenated and mapped to output <span class="math notranslate nohighlight">\(o\)</span> with an affine layer:</p>
<p><span class="math">\begin{align}o = W_o \cdot \textrm{concat}\left(\left[\textrm{wv}^{(0)}, \textrm{wv}^{(1)}, \cdots, \textrm{wv}^{(h)}\right]\right)\end{align}</span></p>
<p>The code below wraps necessary components for multi-head attention, and provides two interfaces.</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">get</span></code> maps state ‘x’, to query, key and value, which is required by following steps( <code class="docutils literal notranslate"><span class="pre">propagate_attention</span></code>).</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">get_o</span></code> maps the updated value after attention to the output <span class="math notranslate nohighlight">\(o\)</span> for post-processing.</p></li>
</ul>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>
</pre></div>
</div>
<p>class MultiHeadAttention(nn.Module): “Multi-Head Attention” def <strong>init</strong>(self, h, dim_model): “h: number of heads; dim_model: hidden dimension” super(MultiHeadAttention, self).__init__() self.d_k = dim_model // h self.h = h # W_q, W_k, W_v, W_o self.linears = clones(nn.Linear(dim_model, dim_model), 4)</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>def get(self, x, fields=&#39;qkv&#39;):
    &quot;Return a dict of queries / keys / values.&quot;
    batch_size = x.shape[0]
    ret = {}
    if &#39;q&#39; in fields:
        ret[&#39;q&#39;] = self.linears[0](x).view(batch_size, self.h, self.d_k)
    if &#39;k&#39; in fields:
        ret[&#39;k&#39;] = self.linears[1](x).view(batch_size, self.h, self.d_k)
    if &#39;v&#39; in fields:
        ret[&#39;v&#39;] = self.linears[2](x).view(batch_size, self.h, self.d_k)
    return ret

def get_o(self, x):
    &quot;get output of the multi-head attention&quot;
    batch_size = x.shape[0]
    return self.linears[3](x.view(batch_size, -1))
</pre></div>
</div>
</section>
</section>
<section id="How-DGL-implements-Transformer-with-a-graph-neural-network">
<h2>How DGL implements Transformer with a graph neural network<a class="headerlink" href="#How-DGL-implements-Transformer-with-a-graph-neural-network" title="Link to this heading"></a></h2>
<p>You get a different perspective of Transformer by treating the attention as edges in a graph and adopt message passing on the edges to induce the appropriate processing.</p>
<section id="Graph-structure">
<h3>Graph structure<a class="headerlink" href="#Graph-structure" title="Link to this heading"></a></h3>
<p>Construct the graph by mapping tokens of the source and target sentence to nodes. The complete Transformer graph is made up of three subgraphs:</p>
<p><strong>Source language graph</strong>. This is a complete graph, each token <span class="math notranslate nohighlight">\(s_i\)</span> can attend to any other token <span class="math notranslate nohighlight">\(s_j\)</span> (including self-loops). |image0| <strong>Target language graph</strong>. The graph is half-complete, in that <span class="math notranslate nohighlight">\(t_i\)</span> attends only to <span class="math notranslate nohighlight">\(t_j\)</span> if <span class="math notranslate nohighlight">\(i &gt; j\)</span> (an output token can not depend on future words). |image1| <strong>Cross-language graph</strong>. This is a bi-partitie graph, where there is an edge from every source token <span class="math notranslate nohighlight">\(s_i\)</span> to every target token <span class="math notranslate nohighlight">\(t_j\)</span>, meaning every
target token can attend on source tokens. |image2|</p>
<p>The full picture looks like this: |image3|</p>
<p>Pre-build the graphs in dataset preparation stage.</p>
</section>
<section id="Message-passing">
<h3>Message passing<a class="headerlink" href="#Message-passing" title="Link to this heading"></a></h3>
<p>Once you define the graph structure, move on to defining the computation for message passing.</p>
<p>Assuming that you have already computed all the queries <span class="math notranslate nohighlight">\(q_i\)</span>, keys <span class="math notranslate nohighlight">\(k_i\)</span> and values <span class="math notranslate nohighlight">\(v_i\)</span>. For each node <span class="math notranslate nohighlight">\(i\)</span> (no matter whether it is a source token or target token), you can decompose the attention computation into two steps:</p>
<ol class="arabic simple">
<li><p><strong>Message computation:</strong> Compute attention score <span class="math notranslate nohighlight">\(\mathrm{score}_{ij}\)</span> between <span class="math notranslate nohighlight">\(i\)</span> and all nodes <span class="math notranslate nohighlight">\(j\)</span> to be attended over, by taking the scaled-dot product between <span class="math notranslate nohighlight">\(q_i\)</span> and <span class="math notranslate nohighlight">\(k_j\)</span>. The message sent from <span class="math notranslate nohighlight">\(j\)</span> to <span class="math notranslate nohighlight">\(i\)</span> will consist of the score <span class="math notranslate nohighlight">\(\mathrm{score}_{ij}\)</span> and the value <span class="math notranslate nohighlight">\(v_j\)</span>.</p></li>
<li><p><strong>Message aggregation:</strong> Aggregate the values <span class="math notranslate nohighlight">\(v_j\)</span> from all <span class="math notranslate nohighlight">\(j\)</span> according to the scores <span class="math notranslate nohighlight">\(\mathrm{score}_{ij}\)</span>.</p></li>
</ol>
<section id="Simple-implementation">
<h4>Simple implementation<a class="headerlink" href="#Simple-implementation" title="Link to this heading"></a></h4>
<section id="Message-computation">
<h5>Message computation<a class="headerlink" href="#Message-computation" title="Link to this heading"></a></h5>
<p>Compute <code class="docutils literal notranslate"><span class="pre">score</span></code> and send source node’s <code class="docutils literal notranslate"><span class="pre">v</span></code> to destination’s mailbox</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>
</pre></div>
</div>
<p>def message_func(edges): return {‘score’: ((edges.src[‘k’] * edges.dst[‘q’]) .sum(-1, keepdim=True)), ‘v’: edges.src[‘v’]}</p>
</section>
<section id="Message-aggregation">
<h5>Message aggregation<a class="headerlink" href="#Message-aggregation" title="Link to this heading"></a></h5>
<p>Normalize over all in-edges and weighted sum to get output</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>
</pre></div>
</div>
<p>import torch as th import torch.nn.functional as F</p>
<p>def reduce_func(nodes, d_k=64): v = nodes.mailbox[‘v’] att = F.softmax(nodes.mailbox[‘score’] / th.sqrt(d_k), 1) return {‘dx’: (att * v).sum(1)}</p>
</section>
<section id="Execute-on-specific-edges">
<h5>Execute on specific edges<a class="headerlink" href="#Execute-on-specific-edges" title="Link to this heading"></a></h5>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>
</pre></div>
</div>
<p>import functools.partial as partial def naive_propagate_attention(self, g, eids): g.send_and_recv(eids, message_func, partial(reduce_func, d_k=self.d_k))</p>
</section>
</section>
<section id="Speeding-up-with-built-in-functions">
<h4>Speeding up with built-in functions<a class="headerlink" href="#Speeding-up-with-built-in-functions" title="Link to this heading"></a></h4>
<p>To speed up the message passing process, use DGL’s built-in functions, including:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">fn.src_mul_egdes(src_field,</span> <span class="pre">edges_field,</span> <span class="pre">out_field)</span></code> multiplies source’s attribute and edges attribute, and send the result to the destination node’s mailbox keyed by <code class="docutils literal notranslate"><span class="pre">out_field</span></code>.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">fn.copy_e(edges_field,</span> <span class="pre">out_field)</span></code> copies edge’s attribute to destination node’s mailbox.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">fn.sum(edges_field,</span> <span class="pre">out_field)</span></code> sums up edge’s attribute and sends aggregation to destination node’s mailbox.</p></li>
</ul>
<p>Here, you assemble those built-in functions into <code class="docutils literal notranslate"><span class="pre">propagate_attention</span></code>, which is also the main graph operation function in the final implementation. To accelerate it, break the <code class="docutils literal notranslate"><span class="pre">softmax</span></code> operation into the following steps. Recall that for each head there are two phases.</p>
<ol class="arabic">
<li><p>Compute attention score by multiply src node’s <code class="docutils literal notranslate"><span class="pre">k</span></code> and dst node’s <code class="docutils literal notranslate"><span class="pre">q</span></code></p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">g.apply_edges(src_dot_dst('k',</span> <span class="pre">'q',</span> <span class="pre">'score'),</span> <span class="pre">eids)</span></code></p></li>
</ul>
</li>
<li><p>Scaled Softmax over all dst nodes’ in-coming edges</p>
<ul>
<li><p>Step 1: Exponentialize score with scale normalize constant</p>
<ul>
<li><p><code class="docutils literal notranslate"><span class="pre">g.apply_edges(scaled_exp('score',</span> <span class="pre">np.sqrt(self.d_k)))</span></code></p>
<div class="math notranslate nohighlight">
\[:nbsphinx-math:`\textrm{score}`\_{ij}:nbsphinx-math:`\leftarrow`:nbsphinx-math:`\exp{\left(\frac{\textrm{score}_{ij}}{ \sqrt{d_k}}\right)}`\]</div>
</li>
</ul>
</li>
<li><p>Step 2: Get the “values” on associated nodes weighted by “scores” on in-coming edges of each node; get the sum of “scores” on in-coming edges of each node for normalization. Note that here <span class="math notranslate nohighlight">\(\textrm{wv}\)</span> is not normalized.</p>
<ul>
<li><p><code class="docutils literal notranslate"><span class="pre">msg:</span> <span class="pre">fn.u_mul_e('v',</span> <span class="pre">'score',</span> <span class="pre">'v'),</span> <span class="pre">reduce:</span> <span class="pre">fn.sum('v',</span> <span class="pre">'wv')</span></code></p>
<div class="math notranslate nohighlight">
\[:nbsphinx-math:`\textrm{wv}`\ *j=:nbsphinx-math:`\sum`*\ {i=1}^{N} :nbsphinx-math:`\textrm{score}`\_{ij} :nbsphinx-math:`\cdot `v_i\]</div>
</li>
<li><p><code class="docutils literal notranslate"><span class="pre">msg:</span> <span class="pre">fn.copy_e('score',</span> <span class="pre">'score'),</span> <span class="pre">reduce:</span> <span class="pre">fn.sum('score',</span> <span class="pre">'z')</span></code></p>
<div class="math notranslate nohighlight">
\[:nbsphinx-math:`\textrm{z}`\ *j=:nbsphinx-math:`\sum`*\ {i=1}^{N} :nbsphinx-math:`\textrm{score}`\_{ij}\]</div>
</li>
</ul>
</li>
</ul>
</li>
</ol>
<p>The normalization of <span class="math notranslate nohighlight">\(\textrm{wv}\)</span> is left to post processing.</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>
</pre></div>
</div>
<p>def src_dot_dst(src_field, dst_field, out_field): def func(edges): return {out_field: (edges.src[src_field] * edges.dst[dst_field]).sum(-1, keepdim=True)}</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>return func
</pre></div>
</div>
<p>def scaled_exp(field, scale_constant): def func(edges): # clamp for softmax numerical stability return {field: th.exp((edges.data[field] / scale_constant).clamp(-5, 5))}</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>return func
</pre></div>
</div>
<p>def propagate_attention(self, g, eids): # Compute attention score g.apply_edges(src_dot_dst(‘k’, ‘q’, ‘score’), eids) g.apply_edges(scaled_exp(‘score’, np.sqrt(self.d_k))) # Update node state g.send_and_recv(eids, [fn.u_mul_e(‘v’, ‘score’, ‘v’), fn.copy_e(‘score’, ‘score’)], [fn.sum(‘v’, ‘wv’), fn.sum(‘score’, ‘z’)])</p>
</section>
</section>
<section id="Preprocessing-and-postprocessing">
<h3>Preprocessing and postprocessing<a class="headerlink" href="#Preprocessing-and-postprocessing" title="Link to this heading"></a></h3>
<p>In Transformer, data needs to be pre- and post-processed before and after the <code class="docutils literal notranslate"><span class="pre">propagate_attention</span></code> function.</p>
<p><strong>Preprocessing</strong> The preprocessing function <code class="docutils literal notranslate"><span class="pre">pre_func</span></code> first normalizes the node representations and then map them to a set of queries, keys and values, using self-attention as an example:</p>
<dl class="simple">
<dt>:nbsphinx-math:<a href="#id5"><span class="problematic" id="id6">`</span></a>begin{align}x leftarrow textrm{LayerNorm}(x) \</dt><dd><p>[q, k, v] leftarrow [W_q, W_k, W_v ]cdot xend{align}`</p>
</dd>
</dl>
<p><strong>Postprocessing</strong> The postprocessing function <code class="docutils literal notranslate"><span class="pre">post_funcs</span></code> completes the whole computation correspond to one layer of the transformer: 1. Normalize <span class="math notranslate nohighlight">\(\textrm{wv}\)</span> and get the output of Multi-Head Attention Layer <span class="math notranslate nohighlight">\(o\)</span>.</p>
<dl class="simple">
<dt>:nbsphinx-math:<a href="#id7"><span class="problematic" id="id8">`</span></a>begin{align}textrm{wv} leftarrow frac{textrm{wv}}{z} \</dt><dd><p>o leftarrow W_ocdot textrm{wv} + b_oend{align}`</p>
</dd>
</dl>
<p>add residual connection:</p>
<p><span class="math">\begin{align}x \leftarrow x + o\end{align}</span></p>
<ol class="arabic" start="2">
<li><p>Applying a two layer position-wise feed forward layer on <span class="math notranslate nohighlight">\(x\)</span> then add residual connection:</p>
<div class="math notranslate nohighlight">
\[\]</div>
<p>x <span class="math">\leftarrow `x + :nbsphinx-math:</span>textrm{LayerNorm}`(:nbsphinx-math:<a href="#id9"><span class="problematic" id="id10">`</span></a>textrm{FFN}`(x))</p>
<p>where <span class="math notranslate nohighlight">\(\textrm{FFN}\)</span> refers to the feed forward function.</p>
</li>
</ol>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>
</pre></div>
</div>
<p>class Encoder(nn.Module): def <strong>init</strong>(self, layer, N): super(Encoder, self).__init__() self.N = N self.layers = clones(layer, N) self.norm = LayerNorm(layer.size)</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>def pre_func(self, i, fields=&#39;qkv&#39;):
    layer = self.layers[i]
    def func(nodes):
        x = nodes.data[&#39;x&#39;]
        norm_x = layer.sublayer[0].norm(x)
        return layer.self_attn.get(norm_x, fields=fields)
    return func

def post_func(self, i):
    layer = self.layers[i]
    def func(nodes):
        x, wv, z = nodes.data[&#39;x&#39;], nodes.data[&#39;wv&#39;], nodes.data[&#39;z&#39;]
        o = layer.self_attn.get_o(wv / z)
        x = x + layer.sublayer[0].dropout(o)
        x = layer.sublayer[1](x, layer.feed_forward)
        return {&#39;x&#39;: x if i &lt; self.N - 1 else self.norm(x)}
    return func
</pre></div>
</div>
<p>class Decoder(nn.Module): def <strong>init</strong>(self, layer, N): super(Decoder, self).__init__() self.N = N self.layers = clones(layer, N) self.norm = LayerNorm(layer.size)</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>def pre_func(self, i, fields=&#39;qkv&#39;, l=0):
    layer = self.layers[i]
    def func(nodes):
        x = nodes.data[&#39;x&#39;]
        if fields == &#39;kv&#39;:
            norm_x = x # In enc-dec attention, x has already been normalized.
        else:
            norm_x = layer.sublayer[l].norm(x)
        return layer.self_attn.get(norm_x, fields)
    return func

def post_func(self, i, l=0):
    layer = self.layers[i]
    def func(nodes):
        x, wv, z = nodes.data[&#39;x&#39;], nodes.data[&#39;wv&#39;], nodes.data[&#39;z&#39;]
        o = layer.self_attn.get_o(wv / z)
        x = x + layer.sublayer[l].dropout(o)
        if l == 1:
            x = layer.sublayer[2](x, layer.feed_forward)
        return {&#39;x&#39;: x if i &lt; self.N - 1 else self.norm(x)}
    return func
</pre></div>
</div>
<p>This completes all procedures of one layer of encoder and decoder in Transformer.</p>
<div class="admonition note">
<div class="admonition-title fa fa-exclamation-circle"><h4></div><p>Note</p>
</h4><p><p>The sublayer connection part is little bit different from the original paper. However, this implementation is the same as <a class="reference external" href="http://nlp.seas.harvard.edu/2018/04/03/attention.html">The Annotated Transformer</a><em>and</em><a class="reference external" href="https://github.com/OpenNMT/OpenNMT-py/blob/cd29c1dbfb35f4a2701ff52a1bf4e5bdcf02802e/onmt/encoders/transformer.py">OpenNMT</a>.</p>
</p></div>
</section>
</section>
<section id="Main-class-of-Transformer-graph">
<h2>Main class of Transformer graph<a class="headerlink" href="#Main-class-of-Transformer-graph" title="Link to this heading"></a></h2>
<p>The processing flow of Transformer can be seen as a 2-stage message-passing within the complete graph (adding pre- and post- processing appropriately): 1) self-attention in encoder, 2) self-attention in decoder followed by cross-attention between encoder and decoder, as shown below. |image4|</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span>
</pre></div>
</div>
<p>class Transformer(nn.Module): def <strong>init</strong>(self, encoder, decoder, src_embed, tgt_embed, pos_enc, generator, h, d_k): super(Transformer, self).__init__() self.encoder, self.decoder = encoder, decoder self.src_embed, self.tgt_embed = src_embed, tgt_embed self.pos_enc = pos_enc self.generator = generator self.h, self.d_k = h, d_k</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>def propagate_attention(self, g, eids):
    # Compute attention score
    g.apply_edges(src_dot_dst(&#39;k&#39;, &#39;q&#39;, &#39;score&#39;), eids)
    g.apply_edges(scaled_exp(&#39;score&#39;, np.sqrt(self.d_k)))
    # Send weighted values to target nodes
    g.send_and_recv(eids,
                    [fn.u_mul_e(&#39;v&#39;, &#39;score&#39;, &#39;v&#39;), fn.copy_e(&#39;score&#39;, &#39;score&#39;)],
                    [fn.sum(&#39;v&#39;, &#39;wv&#39;), fn.sum(&#39;score&#39;, &#39;z&#39;)])

def update_graph(self, g, eids, pre_pairs, post_pairs):
    &quot;Update the node states and edge states of the graph.&quot;

    # Pre-compute queries and key-value pairs.
    for pre_func, nids in pre_pairs:
        g.apply_nodes(pre_func, nids)
    self.propagate_attention(g, eids)
    # Further calculation after attention mechanism
    for post_func, nids in post_pairs:
        g.apply_nodes(post_func, nids)

def forward(self, graph):
    g = graph.g
    nids, eids = graph.nids, graph.eids

    # Word Embedding and Position Embedding
    src_embed, src_pos = self.src_embed(graph.src[0]), self.pos_enc(graph.src[1])
    tgt_embed, tgt_pos = self.tgt_embed(graph.tgt[0]), self.pos_enc(graph.tgt[1])
    g.nodes[nids[&#39;enc&#39;]].data[&#39;x&#39;] = self.pos_enc.dropout(src_embed + src_pos)
    g.nodes[nids[&#39;dec&#39;]].data[&#39;x&#39;] = self.pos_enc.dropout(tgt_embed + tgt_pos)

    for i in range(self.encoder.N):
        # Step 1: Encoder Self-attention
        pre_func = self.encoder.pre_func(i, &#39;qkv&#39;)
        post_func = self.encoder.post_func(i)
        nodes, edges = nids[&#39;enc&#39;], eids[&#39;ee&#39;]
        self.update_graph(g, edges, [(pre_func, nodes)], [(post_func, nodes)])

    for i in range(self.decoder.N):
        # Step 2: Dncoder Self-attention
        pre_func = self.decoder.pre_func(i, &#39;qkv&#39;)
        post_func = self.decoder.post_func(i)
        nodes, edges = nids[&#39;dec&#39;], eids[&#39;dd&#39;]
        self.update_graph(g, edges, [(pre_func, nodes)], [(post_func, nodes)])
        # Step 3: Encoder-Decoder attention
        pre_q = self.decoder.pre_func(i, &#39;q&#39;, 1)
        pre_kv = self.decoder.pre_func(i, &#39;kv&#39;, 1)
        post_func = self.decoder.post_func(i, 1)
        nodes_e, nodes_d, edges = nids[&#39;enc&#39;], nids[&#39;dec&#39;], eids[&#39;ed&#39;]
        self.update_graph(g, edges, [(pre_q, nodes_d), (pre_kv, nodes_e)], [(post_func, nodes_d)])

    return self.generator(g.ndata[&#39;x&#39;][nids[&#39;dec&#39;]])
</pre></div>
</div>
<div class="admonition note">
<div class="admonition-title fa fa-exclamation-circle"><h4></div><p>Note</p>
</h4><p><p>By calling <code class="docutils literal notranslate"><span class="pre">update_graph</span></code> function, you can create your own Transformer on any subgraphs with nearly the same code. This flexibility enables us to discover new, sparse structures (c.f. local attention mentioned <a class="reference external" href="https://arxiv.org/pdf/1508.04025.pdf">here</a>_). Note in this implementation you don’t use mask or padding, which makes the logic more clear and saves memory. The trade-off is that the implementation is slower.</p>
</p></div>
</section>
<section id="Training">
<h2>Training<a class="headerlink" href="#Training" title="Link to this heading"></a></h2>
<p>This tutorial does not cover several other techniques such as Label Smoothing and Noam Optimizations mentioned in the original paper. For detailed description about these modules, read <a class="reference external" href="http://nlp.seas.harvard.edu/2018/04/03/attention.html">The Annotated Transformer</a>_ written by Harvard NLP team.</p>
<section id="Task-and-the-dataset">
<h3>Task and the dataset<a class="headerlink" href="#Task-and-the-dataset" title="Link to this heading"></a></h3>
<p>The Transformer is a general framework for a variety of NLP tasks. This tutorial focuses on the sequence to sequence learning: it’s a typical case to illustrate how it works.</p>
<p>As for the dataset, there are two example tasks: copy and sort, together with two real-world translation tasks: multi30k en-de task and wmt14 en-de task.</p>
<ul class="simple">
<li><p><strong>copy dataset</strong>: copy input sequences to output. (train/valid/test: 9000, 1000, 1000)</p></li>
<li><p><strong>sort dataset</strong>: sort input sequences as output. (train/valid/test: 9000, 1000, 1000)</p></li>
<li><p><strong>Multi30k en-de</strong>, translate sentences from En to De. (train/valid/test: 29000, 1000, 1000)</p></li>
<li><p><strong>WMT14 en-de</strong>, translate sentences from En to De. (Train/Valid/Test: 4500966/3000/3003)</p></li>
</ul>
<div class="admonition note">
<div class="admonition-title fa fa-exclamation-circle"><h4></div><p>Note</p>
</h4><p><p>Training with wmt14 requires multi-GPU support and is not available. Contributions are welcome!</p>
</p></div>
</section>
<section id="Graph-building">
<h3>Graph building<a class="headerlink" href="#Graph-building" title="Link to this heading"></a></h3>
<p><strong>Batching</strong> This is similar to the way you handle Tree-LSTM. Build a graph pool in advance, including all possible combination of input lengths and output lengths. Then for each sample in a batch, call <code class="docutils literal notranslate"><span class="pre">dgl.batch</span></code> to batch graphs of their sizes together in to a single large graph.</p>
<p>You can wrap the process of creating graph pool and building BatchedGraph in <code class="docutils literal notranslate"><span class="pre">dataset.GraphPool</span></code> and <code class="docutils literal notranslate"><span class="pre">dataset.TranslationDataset</span></code>.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span>
</pre></div>
</div>
<p>graph_pool = GraphPool()</p>
<p>data_iter = dataset(graph_pool, mode=’train’, batch_size=1, devices=devices) for graph in data_iter: print(graph.nids[‘enc’]) # encoder node ids print(graph.nids[‘dec’]) # decoder node ids print(graph.eids[‘ee’]) # encoder-encoder edge ids print(graph.eids[‘ed’]) # encoder-decoder edge ids print(graph.eids[‘dd’]) # decoder-decoder edge ids print(graph.src[0]) # Input word index list print(graph.src[1]) # Input positions print(graph.tgt[0]) # Output word index list print(graph.tgt[1]) # Ouptut
positions break</p>
<p>Output:</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>
</pre></div>
</div>
<p>tensor([0, 1, 2, 3, 4, 5, 6, 7, 8], device=’cuda:0’) tensor([ 9, 10, 11, 12, 13, 14, 15, 16, 17, 18], device=’cuda:0’) tensor([ 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80], device=’cuda:0’) tensor([ 81, 82, 83, 84, 85, 86, 87, 88,
89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170], device=’cuda:0’) tensor([171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185,
186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225], device=’cuda:0’) tensor([28, 25, 7, 26, 6, 4, 5, 9, 18], device=’cuda:0’) tensor([0, 1, 2, 3, 4, 5, 6, 7, 8], device=’cuda:0’) tensor([ 0, 28, 25, 7, 26, 6, 4, 5, 9, 18], device=’cuda:0’) tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9], device=’cuda:0’)</p>
</section>
</section>
<section id="Put-it-all-together">
<h2>Put it all together<a class="headerlink" href="#Put-it-all-together" title="Link to this heading"></a></h2>
<p>Train a one-head transformer with one layer, 128 dimension on copy task. Set other parameters to the default.</p>
<p>Inference module is not included in this tutorial. It requires beam search. For a full implementation, see the <a class="reference external" href="https://github.com/dmlc/dgl/tree/master/examples/pytorch/transformer">GitHub repo</a>_.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span>
</pre></div>
</div>
<p>from tqdm.auto import tqdm import torch as th import numpy as np</p>
<p>from loss import LabelSmoothing, SimpleLossCompute from modules import make_model from optims import NoamOpt from dgl.contrib.transformer import get_dataset, GraphPool</p>
<p>def run_epoch(data_iter, model, loss_compute, is_train=True): for i, g in tqdm(enumerate(data_iter)): with th.set_grad_enabled(is_train): output = model(g) loss = loss_compute(output, g.tgt_y, g.n_tokens) print(‘average loss: {}’.format(loss_compute.avg_loss)) print(‘accuracy: {}’.format(loss_compute.accuracy))</p>
<p>N = 1 batch_size = 128 devices = [‘cuda’ if th.cuda.is_available() else ‘cpu’]</p>
<p>dataset = get_dataset(“copy”) V = dataset.vocab_size criterion = LabelSmoothing(V, padding_idx=dataset.pad_id, smoothing=0.1) dim_model = 128</p>
<p># Create model model = make_model(V, V, N=N, dim_model=128, dim_ff=128, h=1)</p>
<p># Sharing weights between Encoder &amp; Decoder model.src_embed.lut.weight = model.tgt_embed.lut.weight model.generator.proj.weight = model.tgt_embed.lut.weight</p>
<p>model, criterion = model.to(devices[0]), criterion.to(devices[0]) model_opt = NoamOpt(dim_model, 1, 400, th.optim.Adam(model.parameters(), lr=1e-3, betas=(0.9, 0.98), eps=1e-9)) loss_compute = SimpleLossCompute</p>
<p>att_maps = [] for epoch in range(4): train_iter = dataset(graph_pool, mode=’train’, batch_size=batch_size, devices=devices) valid_iter = dataset(graph_pool, mode=’valid’, batch_size=batch_size, devices=devices) print(‘Epoch: {} Training…’.format(epoch)) model.train(True) run_epoch(train_iter, model, loss_compute(criterion, model_opt), is_train=True) print(‘Epoch: {} Evaluating…’.format(epoch)) model.att_weight_map = None model.eval() run_epoch(valid_iter, model, loss_compute(criterion,
None), is_train=False) att_maps.append(model.att_weight_map)</p>
</section>
<section id="Visualization">
<h2>Visualization<a class="headerlink" href="#Visualization" title="Link to this heading"></a></h2>
<p>After training, you can visualize the attention that the Transformer generates on copy task.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span>
</pre></div>
</div>
<p>src_seq = dataset.get_seq_by_id(VIZ_IDX, mode=’valid’, field=’src’) tgt_seq = dataset.get_seq_by_id(VIZ_IDX, mode=’valid’, field=’tgt’)[:-1] # visualize head 0 of encoder-decoder attention att_animation(att_maps, ‘e2d’, src_seq, tgt_seq, 0)</p>
<p>|image5| from the figure you see the decoder nodes gradually learns to attend to corresponding nodes in input sequence, which is the expected behavior.</p>
<section id="Multi-head-attention">
<h3>Multi-head attention<a class="headerlink" href="#Multi-head-attention" title="Link to this heading"></a></h3>
<p>Besides the attention of a one-head attention trained on toy task. We also visualize the attention scores of Encoder’s Self Attention, Decoder’s Self Attention and the Encoder-Decoder attention of an one-Layer Transformer network trained on multi-30k dataset.</p>
<p>From the visualization you see the diversity of different heads, which is what you would expect. Different heads learn different relations between word pairs.</p>
<ul class="simple">
<li><p><strong>Encoder Self-Attention</strong> |image6|</p></li>
<li><p><strong>Encoder-Decoder Attention</strong> Most words in target sequence attend on their related words in source sequence, for example: when generating “See” (in De), several heads attend on “lake”; when generating “Eisfischerhütte”, several heads attend on “ice”. |image7|</p></li>
<li><p><strong>Decoder Self-Attention</strong> Most words attend on their previous few words. |image8|</p></li>
</ul>
</section>
</section>
<section id="Adaptive-Universal-Transformer">
<h2>Adaptive Universal Transformer<a class="headerlink" href="#Adaptive-Universal-Transformer" title="Link to this heading"></a></h2>
<p>A recent research paper by Google, <a class="reference external" href="https://arxiv.org/pdf/1807.03819.pdf">Universal Transformer</a>_, is an example to show how <code class="docutils literal notranslate"><span class="pre">update_graph</span></code> adapts to more complex updating rules.</p>
<p>The Universal Transformer was proposed to address the problem that vanilla Transformer is not computationally universal by introducing recurrence in Transformer:</p>
<ul class="simple">
<li><p>The basic idea of Universal Transformer is to repeatedly revise its representations of all symbols in the sequence with each recurrent step by applying a Transformer layer on the representations.</p></li>
<li><p>Compared to vanilla Transformer, Universal Transformer shares weights among its layers, and it does not fix the recurrence time (which means the number of layers in Transformer).</p></li>
</ul>
<p>A further optimization employs an <a class="reference external" href="https://arxiv.org/pdf/1603.08983.pdf">adaptive computation time (ACT)</a>_ mechanism to allow the model to dynamically adjust the number of times the representation of each position in a sequence is revised (refereed to as <strong>step</strong> hereafter). This model is also known as the Adaptive Universal Transformer (AUT).</p>
<p>In AUT, you maintain an active nodes list. In each step <span class="math notranslate nohighlight">\(t\)</span>, we compute a halting probability: <span class="math notranslate nohighlight">\(h (0&lt;h&lt;1)\)</span> for all nodes in this list by:</p>
<p><span class="math">\begin{align}h^t_i = \sigma(W_h x^t_i + b_h)\end{align}</span></p>
<p>then dynamically decide which nodes are still active. A node is halted at time <span class="math notranslate nohighlight">\(T\)</span> if and only if <span class="math notranslate nohighlight">\(\sum_{t=1}^{T-1} h_t &lt; 1 - \varepsilon \leq \sum_{t=1}^{T}h_t\)</span>. Halted nodes are removed from the list. The procedure proceeds until the list is empty or a pre-defined maximum step is reached. From DGL’s perspective, this means that the “active” graph becomes sparser over time.</p>
<p>The final state of a node <span class="math notranslate nohighlight">\(s_i\)</span> is a weighted average of <span class="math notranslate nohighlight">\(x_i^t\)</span> by <span class="math notranslate nohighlight">\(h_i^t\)</span>:</p>
<p><span class="math">\begin{align}s_i = \sum_{t=1}^{T} h_i^t\cdot x_i^t\end{align}</span></p>
<p>In DGL, implement an algorithm by calling <code class="docutils literal notranslate"><span class="pre">update_graph</span></code> on nodes that are still active and edges associated with this nodes. The following code shows the Universal Transformer class in DGL:</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>
</pre></div>
</div>
<p>class UTransformer(nn.Module): “Universal Transformer(<a class="reference external" href="https://arxiv.org/pdf/1807.03819.pdf">https://arxiv.org/pdf/1807.03819.pdf</a>) with ACT(<a class="reference external" href="https://arxiv.org/pdf/1603.08983.pdf">https://arxiv.org/pdf/1603.08983.pdf</a>).” MAX_DEPTH = 8 thres = 0.99 act_loss_weight = 0.01 def <strong>init</strong>(self, encoder, decoder, src_embed, tgt_embed, pos_enc, time_enc, generator, h, d_k): super(UTransformer, self).__init__() self.encoder, self.decoder = encoder, decoder self.src_embed, self.tgt_embed = src_embed, tgt_embed self.pos_enc, self.time_enc = pos_enc, time_enc self.halt_enc =
HaltingUnit(h * d_k) self.halt_dec = HaltingUnit(h * d_k) self.generator = generator self.h, self.d_k = h, d_k</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>def step_forward(self, nodes):
    # add positional encoding and time encoding, increment step by one
    x = nodes.data[&#39;x&#39;]
    step = nodes.data[&#39;step&#39;]
    pos = nodes.data[&#39;pos&#39;]
    return {&#39;x&#39;: self.pos_enc.dropout(x + self.pos_enc(pos.view(-1)) + self.time_enc(step.view(-1))),
            &#39;step&#39;: step + 1}

def halt_and_accum(self, name, end=False):
    &quot;field: &#39;enc&#39; or &#39;dec&#39;&quot;
    halt = self.halt_enc if name == &#39;enc&#39; else self.halt_dec
    thres = self.thres
    def func(nodes):
        p = halt(nodes.data[&#39;x&#39;])
        sum_p = nodes.data[&#39;sum_p&#39;] + p
        active = (sum_p &lt; thres) &amp; (1 - end)
        _continue = active.float()
        r = nodes.data[&#39;r&#39;] * (1 - _continue) + (1 - sum_p) * _continue
        s = nodes.data[&#39;s&#39;] + ((1 - _continue) * r + _continue * p) * nodes.data[&#39;x&#39;]
        return {&#39;p&#39;: p, &#39;sum_p&#39;: sum_p, &#39;r&#39;: r, &#39;s&#39;: s, &#39;active&#39;: active}
    return func

def propagate_attention(self, g, eids):
    # Compute attention score
    g.apply_edges(src_dot_dst(&#39;k&#39;, &#39;q&#39;, &#39;score&#39;), eids)
    g.apply_edges(scaled_exp(&#39;score&#39;, np.sqrt(self.d_k)), eids)
    # Send weighted values to target nodes
    g.send_and_recv(eids,
                    [fn.u_mul_e(&#39;v&#39;, &#39;score&#39;, &#39;v&#39;), fn.copy_e(&#39;score&#39;, &#39;score&#39;)],
                    [fn.sum(&#39;v&#39;, &#39;wv&#39;), fn.sum(&#39;score&#39;, &#39;z&#39;)])

def update_graph(self, g, eids, pre_pairs, post_pairs):
    &quot;Update the node states and edge states of the graph.&quot;
    # Pre-compute queries and key-value pairs.
    for pre_func, nids in pre_pairs:
        g.apply_nodes(pre_func, nids)
    self.propagate_attention(g, eids)
    # Further calculation after attention mechanism
    for post_func, nids in post_pairs:
        g.apply_nodes(post_func, nids)

def forward(self, graph):
    g = graph.g
    N, E = graph.n_nodes, graph.n_edges
    nids, eids = graph.nids, graph.eids

    # embed &amp; pos
    g.nodes[nids[&#39;enc&#39;]].data[&#39;x&#39;] = self.src_embed(graph.src[0])
    g.nodes[nids[&#39;dec&#39;]].data[&#39;x&#39;] = self.tgt_embed(graph.tgt[0])
    g.nodes[nids[&#39;enc&#39;]].data[&#39;pos&#39;] = graph.src[1]
    g.nodes[nids[&#39;dec&#39;]].data[&#39;pos&#39;] = graph.tgt[1]

    # init step
    device = next(self.parameters()).device
    g.ndata[&#39;s&#39;] = th.zeros(N, self.h * self.d_k, dtype=th.float, device=device)    # accumulated state
    g.ndata[&#39;p&#39;] = th.zeros(N, 1, dtype=th.float, device=device)                    # halting prob
    g.ndata[&#39;r&#39;] = th.ones(N, 1, dtype=th.float, device=device)                     # remainder
    g.ndata[&#39;sum_p&#39;] = th.zeros(N, 1, dtype=th.float, device=device)                # sum of pondering values
    g.ndata[&#39;step&#39;] = th.zeros(N, 1, dtype=th.long, device=device)                  # step
    g.ndata[&#39;active&#39;] = th.ones(N, 1, dtype=th.uint8, device=device)                # active

    for step in range(self.MAX_DEPTH):
        pre_func = self.encoder.pre_func(&#39;qkv&#39;)
        post_func = self.encoder.post_func()
        nodes = g.filter_nodes(lambda v: v.data[&#39;active&#39;].view(-1), nids[&#39;enc&#39;])
        if len(nodes) == 0: break
        edges = g.filter_edges(lambda e: e.dst[&#39;active&#39;].view(-1), eids[&#39;ee&#39;])
        end = step == self.MAX_DEPTH - 1
        self.update_graph(g, edges,
                          [(self.step_forward, nodes), (pre_func, nodes)],
                          [(post_func, nodes), (self.halt_and_accum(&#39;enc&#39;, end), nodes)])

    g.nodes[nids[&#39;enc&#39;]].data[&#39;x&#39;] = self.encoder.norm(g.nodes[nids[&#39;enc&#39;]].data[&#39;s&#39;])

    for step in range(self.MAX_DEPTH):
        pre_func = self.decoder.pre_func(&#39;qkv&#39;)
        post_func = self.decoder.post_func()
        nodes = g.filter_nodes(lambda v: v.data[&#39;active&#39;].view(-1), nids[&#39;dec&#39;])
        if len(nodes) == 0: break
        edges = g.filter_edges(lambda e: e.dst[&#39;active&#39;].view(-1), eids[&#39;dd&#39;])
        self.update_graph(g, edges,
                          [(self.step_forward, nodes), (pre_func, nodes)],
                          [(post_func, nodes)])

        pre_q = self.decoder.pre_func(&#39;q&#39;, 1)
        pre_kv = self.decoder.pre_func(&#39;kv&#39;, 1)
        post_func = self.decoder.post_func(1)
        nodes_e = nids[&#39;enc&#39;]
        edges = g.filter_edges(lambda e: e.dst[&#39;active&#39;].view(-1), eids[&#39;ed&#39;])
        end = step == self.MAX_DEPTH - 1
        self.update_graph(g, edges,
                          [(pre_q, nodes), (pre_kv, nodes_e)],
                          [(post_func, nodes), (self.halt_and_accum(&#39;dec&#39;, end), nodes)])

    g.nodes[nids[&#39;dec&#39;]].data[&#39;x&#39;] = self.decoder.norm(g.nodes[nids[&#39;dec&#39;]].data[&#39;s&#39;])
    act_loss = th.mean(g.ndata[&#39;r&#39;]) # ACT loss

    return self.generator(g.ndata[&#39;x&#39;][nids[&#39;dec&#39;]]), act_loss * self.act_loss_weight
</pre></div>
</div>
<p>Call <code class="docutils literal notranslate"><span class="pre">filter_nodes</span></code> and <code class="docutils literal notranslate"><span class="pre">filter_edge</span></code> to find nodes/edges that are still active:</p>
<div class="alert alert-info"><h4><p>Note</p>
</h4><p><ul>
<li><p>:func:<code class="docutils literal notranslate"><span class="pre">~dgl.DGLGraph.filter_nodes</span></code> takes a predicate and a node ID list/tensor as input, then returns a tensor of node IDs that satisfy the given predicate.</p>
<ul>
<li><p>:func:<code class="docutils literal notranslate"><span class="pre">~dgl.DGLGraph.filter_edges</span></code> takes a predicate and an edge ID list/tensor as input, then returns a tensor of edge IDs that satisfy the given predicate.</p>
</p></div></li>
</ul>
</li>
</ul>
<p>For the full implementation, see the <a class="reference external" href="https://github.com/dmlc/dgl/tree/master/examples/pytorch/transformer/modules/act.py">GitHub repo</a>_.</p>
<p>The figure below shows the effect of Adaptive Computational Time. Different positions of a sentence were revised different times.</p>
<p>|image9|</p>
<p>You can also visualize the dynamics of step distribution on nodes during the training of AUT on sort task(reach 99.7% accuracy), which demonstrates how AUT learns to reduce recurrence steps during training. |image10|</p>
<p>|image9| image:: <a class="reference external" href="https://s1.ax1x.com/2018/12/06/F1sGod.png">https://s1.ax1x.com/2018/12/06/F1sGod.png</a> .. |image10| image:: <a class="reference external" href="https://s1.ax1x.com/2018/12/06/F1r8Cq.gif">https://s1.ax1x.com/2018/12/06/F1r8Cq.gif</a></p>
<div class="admonition note">
<div class="admonition-title fa fa-exclamation-circle"><h4></div><p>Note</p>
</h4><p><p>The notebook itself is not executable due to many dependencies. Download <a class="reference external" href="https://data.dgl.ai/tutorial/7_transformer.py">7_transformer.py</a>_, and copy the python script to directory <code class="docutils literal notranslate"><span class="pre">examples/pytorch/transformer</span></code> then run <code class="docutils literal notranslate"><span class="pre">python</span> <span class="pre">7_transformer.py</span></code> to see how it works.</p>
</p></div>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="2_capsule.html" class="btn btn-neutral float-left" title="Capsule Network" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="../../../api/python/dgl.html" class="btn btn-neutral float-right" title="dgl" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2018, DGL Team.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>