{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "# Transformer as a Graph Neural Network\n",
    "\n",
    "**Author**: Zihao Ye, Jinjing Zhou, Qipeng Guo, Quan Gan, Zheng Zhang\n",
    "\n",
    "<div class=\"alert alert-danger\"><h4>Warning</h4><p>The tutorial aims at gaining insights into the paper, with code as a mean\n",
    "    of explanation. The implementation thus is NOT optimized for running\n",
    "    efficiency. For recommended implementation, please refer to the [official\n",
    "    examples](https://github.com/dmlc/dgl/tree/master/examples).</p></div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial, you learn about a simplified implementation of the Transformer model.\n",
    "You can see highlights of the most important design points. For instance, there is\n",
    "only single-head attention. The complete code can be found\n",
    "[here](https://github.com/dmlc/dgl/tree/master/examples/pytorch/transformer)_.\n",
    "\n",
    "The overall structure is similar to the one from the research papaer [Annotated\n",
    "Transformer](http://nlp.seas.harvard.edu/2018/04/03/attention.html)_.\n",
    "\n",
    "The Transformer model, as a replacement of CNN/RNN architecture for\n",
    "sequence modeling, was introduced in the research paper: [Attention is All\n",
    "You Need](https://arxiv.org/pdf/1706.03762.pdf)_. It improved the\n",
    "state of the art for machine translation as well as natural language\n",
    "inference task\n",
    "([GPT](https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf)_).\n",
    "Recent work on pre-training Transformer with large scale corpus\n",
    "([BERT](https://arxiv.org/pdf/1810.04805.pdf)_) supports that it is\n",
    "capable of learning high-quality semantic representation.\n",
    "\n",
    "The interesting part of Transformer is its extensive employment of\n",
    "attention. The classic use of attention comes from machine translation\n",
    "model, where the output token attends to all input tokens.\n",
    "\n",
    "Transformer additionally applies *self-attention* in both decoder and\n",
    "encoder. This process forces words relate to each other to combine\n",
    "together, irrespective of their positions in the sequence. This is\n",
    "different from RNN-based model, where words (in the source sentence) are\n",
    "combined along the chain, which is thought to be too constrained.\n",
    "\n",
    "## Attention layer of Transformer\n",
    "\n",
    "In the attention layer of Transformer, for each node the module learns to\n",
    "assign weights on its in-coming edges. For node pair $(i, j)$\n",
    "(from $i$ to $j$) with node\n",
    "$x_i, x_j \\in \\mathbb{R}^n$, the score of their connection is\n",
    "defined as follows:\n",
    "\n",
    "\\begin{align}q_j = W_q\\cdot x_j \\\\\n",
    "   k_i = W_k\\cdot x_i\\\\\n",
    "   v_i = W_v\\cdot x_i\\\\\n",
    "   \\textrm{score} = q_j^T k_i\\end{align}\n",
    "\n",
    "where $W_q, W_k, W_v \\in \\mathbb{R}^{n\\times d_k}$ map the\n",
    "representations $x$ to “query”, “key”, and “value” space\n",
    "respectively.\n",
    "\n",
    "There are other possibilities to implement the score function. The dot\n",
    "product measures the similarity of a given query $q_j$ and a key\n",
    "$k_i$: if $j$ needs the information stored in $i$, the\n",
    "query vector at position $j$ ($q_j$) is supposed to be close\n",
    "to key vector at position $i$ ($k_i$).\n",
    "\n",
    "The score is then used to compute the sum of the incoming values,\n",
    "normalized over the weights of edges, stored in $\\textrm{wv}$.\n",
    "Then apply an affine layer to $\\textrm{wv}$ to get the output\n",
    "$o$:\n",
    "\n",
    "\\begin{align}w_{ji} = \\frac{\\exp\\{\\textrm{score}_{ji} \\}}{\\sum\\limits_{(k, i)\\in E}\\exp\\{\\textrm{score}_{ki} \\}} \\\\\n",
    "   \\textrm{wv}_i = \\sum_{(k, i)\\in E} w_{ki} v_k \\\\\n",
    "   o = W_o\\cdot \\textrm{wv} \\\\\\end{align}\n",
    "\n",
    "### Multi-head attention layer\n",
    "\n",
    "In Transformer, attention is *multi-headed*. A head is very much like a\n",
    "channel in a convolutional network. The multi-head attention consists of\n",
    "multiple attention heads, in which each head refers to a single\n",
    "attention module. $\\textrm{wv}^{(i)}$ for all the heads are\n",
    "concatenated and mapped to output $o$ with an affine layer:\n",
    "\n",
    "\\begin{align}o = W_o \\cdot \\textrm{concat}\\left(\\left[\\textrm{wv}^{(0)}, \\textrm{wv}^{(1)}, \\cdots, \\textrm{wv}^{(h)}\\right]\\right)\\end{align}\n",
    "\n",
    "The code below wraps necessary components for multi-head attention, and\n",
    "provides two interfaces.\n",
    "\n",
    "-  ``get`` maps state ‘x’, to query, key and value, which is required by\n",
    "   following steps(\\ ``propagate_attention``).\n",
    "-  ``get_o`` maps the updated value after attention to the output\n",
    "   $o$ for post-processing.\n",
    "\n",
    ".. code::\n",
    "\n",
    "   class MultiHeadAttention(nn.Module):\n",
    "       \"Multi-Head Attention\"\n",
    "       def __init__(self, h, dim_model):\n",
    "           \"h: number of heads; dim_model: hidden dimension\"\n",
    "           super(MultiHeadAttention, self).__init__()\n",
    "           self.d_k = dim_model // h\n",
    "           self.h = h\n",
    "           # W_q, W_k, W_v, W_o\n",
    "           self.linears = clones(nn.Linear(dim_model, dim_model), 4)\n",
    "\n",
    "       def get(self, x, fields='qkv'):\n",
    "           \"Return a dict of queries / keys / values.\"\n",
    "           batch_size = x.shape[0]\n",
    "           ret = {}\n",
    "           if 'q' in fields:\n",
    "               ret['q'] = self.linears[0](x).view(batch_size, self.h, self.d_k)\n",
    "           if 'k' in fields:\n",
    "               ret['k'] = self.linears[1](x).view(batch_size, self.h, self.d_k)\n",
    "           if 'v' in fields:\n",
    "               ret['v'] = self.linears[2](x).view(batch_size, self.h, self.d_k)\n",
    "           return ret\n",
    "\n",
    "       def get_o(self, x):\n",
    "           \"get output of the multi-head attention\"\n",
    "           batch_size = x.shape[0]\n",
    "           return self.linears[3](x.view(batch_size, -1))\n",
    "\n",
    "\n",
    "## How DGL implements Transformer with a graph neural network\n",
    "\n",
    "You get a different perspective of Transformer by treating the\n",
    "attention as edges in a graph and adopt message passing on the edges to\n",
    "induce the appropriate processing.\n",
    "\n",
    "### Graph structure\n",
    "\n",
    "Construct the graph by mapping tokens of the source and target\n",
    "sentence to nodes. The complete Transformer graph is made up of three\n",
    "subgraphs:\n",
    "\n",
    "**Source language graph**. This is a complete graph, each\n",
    "token $s_i$ can attend to any other token $s_j$ (including\n",
    "self-loops). |image0|\n",
    "**Target language graph**. The graph is\n",
    "half-complete, in that $t_i$ attends only to $t_j$ if\n",
    "$i > j$ (an output token can not depend on future words). |image1|\n",
    "**Cross-language graph**. This is a bi-partitie graph, where there is\n",
    "an edge from every source token $s_i$ to every target token\n",
    "$t_j$, meaning every target token can attend on source tokens.\n",
    "|image2|\n",
    "\n",
    "The full picture looks like this: |image3|\n",
    "\n",
    "Pre-build the graphs in dataset preparation stage.\n",
    "\n",
    "### Message passing\n",
    "\n",
    "Once you define the graph structure, move on to defining the\n",
    "computation for message passing.\n",
    "\n",
    "Assuming that you have already computed all the queries $q_i$, keys\n",
    "$k_i$ and values $v_i$. For each node $i$ (no matter\n",
    "whether it is a source token or target token), you can decompose the\n",
    "attention computation into two steps:\n",
    "\n",
    "1. **Message computation:** Compute attention score\n",
    "   $\\mathrm{score}_{ij}$ between $i$ and all nodes $j$\n",
    "   to be attended over, by taking the scaled-dot product between\n",
    "   $q_i$ and $k_j$. The message sent from $j$ to\n",
    "   $i$ will consist of the score $\\mathrm{score}_{ij}$ and\n",
    "   the value $v_j$.\n",
    "2. **Message aggregation:** Aggregate the values $v_j$ from all\n",
    "   $j$ according to the scores $\\mathrm{score}_{ij}$.\n",
    "\n",
    "#### Simple implementation\n",
    "\n",
    "##### Message computation\n",
    "\n",
    "Compute ``score`` and send source node’s ``v`` to destination’s mailbox\n",
    "\n",
    ".. code::\n",
    "\n",
    "   def message_func(edges):\n",
    "       return {'score': ((edges.src['k'] * edges.dst['q'])\n",
    "                         .sum(-1, keepdim=True)),\n",
    "               'v': edges.src['v']}\n",
    "\n",
    "##### Message aggregation\n",
    "\n",
    "Normalize over all in-edges and weighted sum to get output\n",
    "\n",
    ".. code::\n",
    "\n",
    "   import torch as th\n",
    "   import torch.nn.functional as F\n",
    "\n",
    "   def reduce_func(nodes, d_k=64):\n",
    "       v = nodes.mailbox['v']\n",
    "       att = F.softmax(nodes.mailbox['score'] / th.sqrt(d_k), 1)\n",
    "       return {'dx': (att * v).sum(1)}\n",
    "\n",
    "##### Execute on specific edges\n",
    "\n",
    ".. code::\n",
    "\n",
    "   import functools.partial as partial\n",
    "   def naive_propagate_attention(self, g, eids):\n",
    "       g.send_and_recv(eids, message_func, partial(reduce_func, d_k=self.d_k))\n",
    "\n",
    "#### Speeding up with built-in functions\n",
    "\n",
    "To speed up the message passing process, use DGL’s built-in\n",
    "functions, including:\n",
    "\n",
    "- ``fn.src_mul_egdes(src_field, edges_field, out_field)`` multiplies\n",
    "  source’s attribute and edges attribute, and send the result to the\n",
    "  destination node’s mailbox keyed by ``out_field``.\n",
    "- ``fn.copy_e(edges_field, out_field)`` copies edge’s attribute to\n",
    "  destination node’s mailbox.\n",
    "- ``fn.sum(edges_field, out_field)`` sums up\n",
    "  edge’s attribute and sends aggregation to destination node’s mailbox.\n",
    "\n",
    "Here, you assemble those built-in functions into ``propagate_attention``,\n",
    "which is also the main graph operation function in the final\n",
    "implementation. To accelerate it, break the ``softmax`` operation into\n",
    "the following steps. Recall that for each head there are two phases.\n",
    "\n",
    "1. Compute attention score by multiply src node’s ``k`` and dst node’s\n",
    "   ``q``\n",
    "\n",
    "   -  ``g.apply_edges(src_dot_dst('k', 'q', 'score'), eids)``\n",
    "\n",
    "2. Scaled Softmax over all dst nodes’ in-coming edges\n",
    "\n",
    "   -  Step 1: Exponentialize score with scale normalize constant\n",
    "\n",
    "      -  ``g.apply_edges(scaled_exp('score', np.sqrt(self.d_k)))``\n",
    "\n",
    "         .. math:: \\textrm{score}_{ij}\\leftarrow\\exp{\\left(\\frac{\\textrm{score}_{ij}}{ \\sqrt{d_k}}\\right)}\n",
    "\n",
    "   -  Step 2: Get the “values” on associated nodes weighted by “scores”\n",
    "      on in-coming edges of each node; get the sum of “scores” on\n",
    "      in-coming edges of each node for normalization. Note that here\n",
    "      $\\textrm{wv}$ is not normalized.\n",
    "\n",
    "      -  ``msg: fn.u_mul_e('v', 'score', 'v'), reduce: fn.sum('v', 'wv')``\n",
    "\n",
    "         .. math:: \\textrm{wv}_j=\\sum_{i=1}^{N} \\textrm{score}_{ij} \\cdot v_i\n",
    "\n",
    "      -  ``msg: fn.copy_e('score', 'score'), reduce: fn.sum('score', 'z')``\n",
    "\n",
    "         .. math:: \\textrm{z}_j=\\sum_{i=1}^{N} \\textrm{score}_{ij}\n",
    "\n",
    "The normalization of $\\textrm{wv}$ is left to post processing.\n",
    "\n",
    ".. code::\n",
    "\n",
    "   def src_dot_dst(src_field, dst_field, out_field):\n",
    "       def func(edges):\n",
    "           return {out_field: (edges.src[src_field] * edges.dst[dst_field]).sum(-1, keepdim=True)}\n",
    "\n",
    "       return func\n",
    "\n",
    "   def scaled_exp(field, scale_constant):\n",
    "       def func(edges):\n",
    "           # clamp for softmax numerical stability\n",
    "           return {field: th.exp((edges.data[field] / scale_constant).clamp(-5, 5))}\n",
    "\n",
    "       return func\n",
    "\n",
    "\n",
    "   def propagate_attention(self, g, eids):\n",
    "       # Compute attention score\n",
    "       g.apply_edges(src_dot_dst('k', 'q', 'score'), eids)\n",
    "       g.apply_edges(scaled_exp('score', np.sqrt(self.d_k)))\n",
    "       # Update node state\n",
    "       g.send_and_recv(eids,\n",
    "                       [fn.u_mul_e('v', 'score', 'v'), fn.copy_e('score', 'score')],\n",
    "                       [fn.sum('v', 'wv'), fn.sum('score', 'z')])\n",
    "\n",
    "### Preprocessing and postprocessing\n",
    "\n",
    "In Transformer, data needs to be pre- and post-processed before and\n",
    "after the ``propagate_attention`` function.\n",
    "\n",
    "**Preprocessing** The preprocessing function ``pre_func`` first\n",
    "normalizes the node representations and then map them to a set of\n",
    "queries, keys and values, using self-attention as an example:\n",
    "\n",
    "\\begin{align}x \\leftarrow \\textrm{LayerNorm}(x) \\\\\n",
    "   [q, k, v] \\leftarrow [W_q, W_k, W_v ]\\cdot x\\end{align}\n",
    "\n",
    "**Postprocessing** The postprocessing function ``post_funcs`` completes\n",
    "the whole computation correspond to one layer of the transformer: 1.\n",
    "Normalize $\\textrm{wv}$ and get the output of Multi-Head Attention\n",
    "Layer $o$.\n",
    "\n",
    "\\begin{align}\\textrm{wv} \\leftarrow \\frac{\\textrm{wv}}{z} \\\\\n",
    "   o \\leftarrow W_o\\cdot \\textrm{wv} + b_o\\end{align}\n",
    "\n",
    "add residual connection:\n",
    "\n",
    "\\begin{align}x \\leftarrow x + o\\end{align}\n",
    "\n",
    "2. Applying a two layer position-wise feed forward layer on $x$\n",
    "   then add residual connection:\n",
    "\n",
    "   .. math::\n",
    "\n",
    "\n",
    "      x \\leftarrow x + \\textrm{LayerNorm}(\\textrm{FFN}(x))\n",
    "\n",
    "   where $\\textrm{FFN}$ refers to the feed forward function.\n",
    "\n",
    ".. code::\n",
    "\n",
    "   class Encoder(nn.Module):\n",
    "       def __init__(self, layer, N):\n",
    "           super(Encoder, self).__init__()\n",
    "           self.N = N\n",
    "           self.layers = clones(layer, N)\n",
    "           self.norm = LayerNorm(layer.size)\n",
    "\n",
    "       def pre_func(self, i, fields='qkv'):\n",
    "           layer = self.layers[i]\n",
    "           def func(nodes):\n",
    "               x = nodes.data['x']\n",
    "               norm_x = layer.sublayer[0].norm(x)\n",
    "               return layer.self_attn.get(norm_x, fields=fields)\n",
    "           return func\n",
    "\n",
    "       def post_func(self, i):\n",
    "           layer = self.layers[i]\n",
    "           def func(nodes):\n",
    "               x, wv, z = nodes.data['x'], nodes.data['wv'], nodes.data['z']\n",
    "               o = layer.self_attn.get_o(wv / z)\n",
    "               x = x + layer.sublayer[0].dropout(o)\n",
    "               x = layer.sublayer[1](x, layer.feed_forward)\n",
    "               return {'x': x if i < self.N - 1 else self.norm(x)}\n",
    "           return func\n",
    "\n",
    "   class Decoder(nn.Module):\n",
    "       def __init__(self, layer, N):\n",
    "           super(Decoder, self).__init__()\n",
    "           self.N = N\n",
    "           self.layers = clones(layer, N)\n",
    "           self.norm = LayerNorm(layer.size)\n",
    "\n",
    "       def pre_func(self, i, fields='qkv', l=0):\n",
    "           layer = self.layers[i]\n",
    "           def func(nodes):\n",
    "               x = nodes.data['x']\n",
    "               if fields == 'kv':\n",
    "                   norm_x = x # In enc-dec attention, x has already been normalized.\n",
    "               else:\n",
    "                   norm_x = layer.sublayer[l].norm(x)\n",
    "               return layer.self_attn.get(norm_x, fields)\n",
    "           return func\n",
    "\n",
    "       def post_func(self, i, l=0):\n",
    "           layer = self.layers[i]\n",
    "           def func(nodes):\n",
    "               x, wv, z = nodes.data['x'], nodes.data['wv'], nodes.data['z']\n",
    "               o = layer.self_attn.get_o(wv / z)\n",
    "               x = x + layer.sublayer[l].dropout(o)\n",
    "               if l == 1:\n",
    "                   x = layer.sublayer[2](x, layer.feed_forward)\n",
    "               return {'x': x if i < self.N - 1 else self.norm(x)}\n",
    "           return func\n",
    "\n",
    "This completes all procedures of one layer of encoder and decoder in\n",
    "Transformer.\n",
    "\n",
    "<div class=\"alert alert-info\"><h4>Note</h4><p>The sublayer connection part is little bit different from the\n",
    "   original paper. However, this implementation is the same as [The Annotated\n",
    "   Transformer](http://nlp.seas.harvard.edu/2018/04/03/attention.html)_\n",
    "   and\n",
    "   [OpenNMT](https://github.com/OpenNMT/OpenNMT-py/blob/cd29c1dbfb35f4a2701ff52a1bf4e5bdcf02802e/onmt/encoders/transformer.py)_.</p></div>\n",
    "\n",
    "## Main class of Transformer graph\n",
    "\n",
    "The processing flow of Transformer can be seen as a 2-stage\n",
    "message-passing within the complete graph (adding pre- and post-\n",
    "processing appropriately): 1) self-attention in encoder, 2)\n",
    "self-attention in decoder followed by cross-attention between encoder\n",
    "and decoder, as shown below. |image4|\n",
    "\n",
    ".. code:: python\n",
    "\n",
    "   class Transformer(nn.Module):\n",
    "       def __init__(self, encoder, decoder, src_embed, tgt_embed, pos_enc, generator, h, d_k):\n",
    "           super(Transformer, self).__init__()\n",
    "           self.encoder, self.decoder = encoder, decoder\n",
    "           self.src_embed, self.tgt_embed = src_embed, tgt_embed\n",
    "           self.pos_enc = pos_enc\n",
    "           self.generator = generator\n",
    "           self.h, self.d_k = h, d_k\n",
    "\n",
    "       def propagate_attention(self, g, eids):\n",
    "           # Compute attention score\n",
    "           g.apply_edges(src_dot_dst('k', 'q', 'score'), eids)\n",
    "           g.apply_edges(scaled_exp('score', np.sqrt(self.d_k)))\n",
    "           # Send weighted values to target nodes\n",
    "           g.send_and_recv(eids,\n",
    "                           [fn.u_mul_e('v', 'score', 'v'), fn.copy_e('score', 'score')],\n",
    "                           [fn.sum('v', 'wv'), fn.sum('score', 'z')])\n",
    "\n",
    "       def update_graph(self, g, eids, pre_pairs, post_pairs):\n",
    "           \"Update the node states and edge states of the graph.\"\n",
    "\n",
    "           # Pre-compute queries and key-value pairs.\n",
    "           for pre_func, nids in pre_pairs:\n",
    "               g.apply_nodes(pre_func, nids)\n",
    "           self.propagate_attention(g, eids)\n",
    "           # Further calculation after attention mechanism\n",
    "           for post_func, nids in post_pairs:\n",
    "               g.apply_nodes(post_func, nids)\n",
    "\n",
    "       def forward(self, graph):\n",
    "           g = graph.g\n",
    "           nids, eids = graph.nids, graph.eids\n",
    "\n",
    "           # Word Embedding and Position Embedding\n",
    "           src_embed, src_pos = self.src_embed(graph.src[0]), self.pos_enc(graph.src[1])\n",
    "           tgt_embed, tgt_pos = self.tgt_embed(graph.tgt[0]), self.pos_enc(graph.tgt[1])\n",
    "           g.nodes[nids['enc']].data['x'] = self.pos_enc.dropout(src_embed + src_pos)\n",
    "           g.nodes[nids['dec']].data['x'] = self.pos_enc.dropout(tgt_embed + tgt_pos)\n",
    "\n",
    "           for i in range(self.encoder.N):\n",
    "               # Step 1: Encoder Self-attention\n",
    "               pre_func = self.encoder.pre_func(i, 'qkv')\n",
    "               post_func = self.encoder.post_func(i)\n",
    "               nodes, edges = nids['enc'], eids['ee']\n",
    "               self.update_graph(g, edges, [(pre_func, nodes)], [(post_func, nodes)])\n",
    "\n",
    "           for i in range(self.decoder.N):\n",
    "               # Step 2: Dncoder Self-attention\n",
    "               pre_func = self.decoder.pre_func(i, 'qkv')\n",
    "               post_func = self.decoder.post_func(i)\n",
    "               nodes, edges = nids['dec'], eids['dd']\n",
    "               self.update_graph(g, edges, [(pre_func, nodes)], [(post_func, nodes)])\n",
    "               # Step 3: Encoder-Decoder attention\n",
    "               pre_q = self.decoder.pre_func(i, 'q', 1)\n",
    "               pre_kv = self.decoder.pre_func(i, 'kv', 1)\n",
    "               post_func = self.decoder.post_func(i, 1)\n",
    "               nodes_e, nodes_d, edges = nids['enc'], nids['dec'], eids['ed']\n",
    "               self.update_graph(g, edges, [(pre_q, nodes_d), (pre_kv, nodes_e)], [(post_func, nodes_d)])\n",
    "\n",
    "           return self.generator(g.ndata['x'][nids['dec']])\n",
    "\n",
    "\n",
    "<div class=\"alert alert-info\"><h4>Note</h4><p>By calling ``update_graph`` function, you can create your own\n",
    "   Transformer on any subgraphs with nearly the same code. This\n",
    "   flexibility enables us to discover new, sparse structures (c.f. local attention\n",
    "   mentioned [here](https://arxiv.org/pdf/1508.04025.pdf)_). Note in this\n",
    "   implementation you don't use mask or padding, which makes the logic\n",
    "   more clear and saves memory. The trade-off is that the implementation is\n",
    "   slower.</p></div>\n",
    "\n",
    "## Training\n",
    "\n",
    "This tutorial does not cover several other techniques such as Label\n",
    "Smoothing and Noam Optimizations mentioned in the original paper. For\n",
    "detailed description about these modules, read [The\n",
    "Annotated\n",
    "Transformer](http://nlp.seas.harvard.edu/2018/04/03/attention.html)_\n",
    "written by Harvard NLP team.\n",
    "\n",
    "### Task and the dataset\n",
    "\n",
    "The Transformer is a general framework for a variety of NLP tasks. This tutorial focuses\n",
    "on the sequence to sequence learning: it’s a typical case to illustrate how it works.\n",
    "\n",
    "As for the dataset, there are two example tasks: copy and sort, together\n",
    "with two real-world translation tasks: multi30k en-de task and wmt14\n",
    "en-de task.\n",
    "\n",
    "-  **copy dataset**: copy input sequences to output. (train/valid/test:\n",
    "   9000, 1000, 1000)\n",
    "-  **sort dataset**: sort input sequences as output. (train/valid/test:\n",
    "   9000, 1000, 1000)\n",
    "-  **Multi30k en-de**, translate sentences from En to De.\n",
    "   (train/valid/test: 29000, 1000, 1000)\n",
    "-  **WMT14 en-de**, translate sentences from En to De.\n",
    "   (Train/Valid/Test: 4500966/3000/3003)\n",
    "\n",
    "<div class=\"alert alert-info\"><h4>Note</h4><p>Training with wmt14 requires multi-GPU support and is not available. Contributions are welcome!</p></div>\n",
    "\n",
    "### Graph building\n",
    "\n",
    "**Batching** This is similar to the way you handle Tree-LSTM. Build a graph pool in\n",
    "advance, including all possible combination of input lengths and output\n",
    "lengths. Then for each sample in a batch, call ``dgl.batch`` to batch\n",
    "graphs of their sizes together in to a single large graph.\n",
    "\n",
    "You can wrap the process of creating graph pool and building\n",
    "BatchedGraph in ``dataset.GraphPool`` and\n",
    "``dataset.TranslationDataset``.\n",
    "\n",
    ".. code:: python\n",
    "\n",
    "   graph_pool = GraphPool()\n",
    "\n",
    "   data_iter = dataset(graph_pool, mode='train', batch_size=1, devices=devices)\n",
    "   for graph in data_iter:\n",
    "       print(graph.nids['enc']) # encoder node ids\n",
    "       print(graph.nids['dec']) # decoder node ids\n",
    "       print(graph.eids['ee']) # encoder-encoder edge ids\n",
    "       print(graph.eids['ed']) # encoder-decoder edge ids\n",
    "       print(graph.eids['dd']) # decoder-decoder edge ids\n",
    "       print(graph.src[0]) # Input word index list\n",
    "       print(graph.src[1]) # Input positions\n",
    "       print(graph.tgt[0]) # Output word index list\n",
    "       print(graph.tgt[1]) # Ouptut positions\n",
    "       break\n",
    "\n",
    "Output:\n",
    "\n",
    ".. code::\n",
    "\n",
    "   tensor([0, 1, 2, 3, 4, 5, 6, 7, 8], device='cuda:0')\n",
    "   tensor([ 9, 10, 11, 12, 13, 14, 15, 16, 17, 18], device='cuda:0')\n",
    "   tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
    "           18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,\n",
    "           36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,\n",
    "           54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71,\n",
    "           72, 73, 74, 75, 76, 77, 78, 79, 80], device='cuda:0')\n",
    "   tensor([ 81,  82,  83,  84,  85,  86,  87,  88,  89,  90,  91,  92,  93,  94,\n",
    "            95,  96,  97,  98,  99, 100, 101, 102, 103, 104, 105, 106, 107, 108,\n",
    "           109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122,\n",
    "           123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136,\n",
    "           137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150,\n",
    "           151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164,\n",
    "           165, 166, 167, 168, 169, 170], device='cuda:0')\n",
    "   tensor([171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184,\n",
    "           185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198,\n",
    "           199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212,\n",
    "           213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225],\n",
    "          device='cuda:0')\n",
    "   tensor([28, 25,  7, 26,  6,  4,  5,  9, 18], device='cuda:0')\n",
    "   tensor([0, 1, 2, 3, 4, 5, 6, 7, 8], device='cuda:0')\n",
    "   tensor([ 0, 28, 25,  7, 26,  6,  4,  5,  9, 18], device='cuda:0')\n",
    "   tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9], device='cuda:0')\n",
    "\n",
    "## Put it all together\n",
    "\n",
    "Train a one-head transformer with one layer, 128 dimension on copy\n",
    "task. Set other parameters to the default.\n",
    "\n",
    "Inference module is not included in this tutorial. It\n",
    "requires beam search. For a full implementation, see the [GitHub\n",
    "repo](https://github.com/dmlc/dgl/tree/master/examples/pytorch/transformer)_.\n",
    "\n",
    ".. code:: python\n",
    "\n",
    "   from tqdm.auto import tqdm\n",
    "   import torch as th\n",
    "   import numpy as np\n",
    "\n",
    "   from loss import LabelSmoothing, SimpleLossCompute\n",
    "   from modules import make_model\n",
    "   from optims import NoamOpt\n",
    "   from dgl.contrib.transformer import get_dataset, GraphPool\n",
    "\n",
    "   def run_epoch(data_iter, model, loss_compute, is_train=True):\n",
    "       for i, g in tqdm(enumerate(data_iter)):\n",
    "           with th.set_grad_enabled(is_train):\n",
    "               output = model(g)\n",
    "               loss = loss_compute(output, g.tgt_y, g.n_tokens)\n",
    "       print('average loss: {}'.format(loss_compute.avg_loss))\n",
    "       print('accuracy: {}'.format(loss_compute.accuracy))\n",
    "\n",
    "   N = 1\n",
    "   batch_size = 128\n",
    "   devices = ['cuda' if th.cuda.is_available() else 'cpu']\n",
    "\n",
    "   dataset = get_dataset(\"copy\")\n",
    "   V = dataset.vocab_size\n",
    "   criterion = LabelSmoothing(V, padding_idx=dataset.pad_id, smoothing=0.1)\n",
    "   dim_model = 128\n",
    "\n",
    "   # Create model\n",
    "   model = make_model(V, V, N=N, dim_model=128, dim_ff=128, h=1)\n",
    "\n",
    "   # Sharing weights between Encoder & Decoder\n",
    "   model.src_embed.lut.weight = model.tgt_embed.lut.weight\n",
    "   model.generator.proj.weight = model.tgt_embed.lut.weight\n",
    "\n",
    "   model, criterion = model.to(devices[0]), criterion.to(devices[0])\n",
    "   model_opt = NoamOpt(dim_model, 1, 400,\n",
    "                       th.optim.Adam(model.parameters(), lr=1e-3, betas=(0.9, 0.98), eps=1e-9))\n",
    "   loss_compute = SimpleLossCompute\n",
    "\n",
    "   att_maps = []\n",
    "   for epoch in range(4):\n",
    "       train_iter = dataset(graph_pool, mode='train', batch_size=batch_size, devices=devices)\n",
    "       valid_iter = dataset(graph_pool, mode='valid', batch_size=batch_size, devices=devices)\n",
    "       print('Epoch: {} Training...'.format(epoch))\n",
    "       model.train(True)\n",
    "       run_epoch(train_iter, model,\n",
    "                 loss_compute(criterion, model_opt), is_train=True)\n",
    "       print('Epoch: {} Evaluating...'.format(epoch))\n",
    "       model.att_weight_map = None\n",
    "       model.eval()\n",
    "       run_epoch(valid_iter, model,\n",
    "                 loss_compute(criterion, None), is_train=False)\n",
    "       att_maps.append(model.att_weight_map)\n",
    "\n",
    "## Visualization\n",
    "\n",
    "After training, you can visualize the attention that the Transformer generates\n",
    "on copy task.\n",
    "\n",
    ".. code:: python\n",
    "\n",
    "   src_seq = dataset.get_seq_by_id(VIZ_IDX, mode='valid', field='src')\n",
    "   tgt_seq = dataset.get_seq_by_id(VIZ_IDX, mode='valid', field='tgt')[:-1]\n",
    "   # visualize head 0 of encoder-decoder attention\n",
    "   att_animation(att_maps, 'e2d', src_seq, tgt_seq, 0)\n",
    "\n",
    "|image5| from the figure you see the decoder nodes gradually learns to\n",
    "attend to corresponding nodes in input sequence, which is the expected\n",
    "behavior.\n",
    "\n",
    "### Multi-head attention\n",
    "\n",
    "Besides the attention of a one-head attention trained on toy task. We\n",
    "also visualize the attention scores of Encoder’s Self Attention,\n",
    "Decoder’s Self Attention and the Encoder-Decoder attention of an\n",
    "one-Layer Transformer network trained on multi-30k dataset.\n",
    "\n",
    "From the visualization you see the diversity of different heads, which is what you would\n",
    "expect. Different heads learn different relations between word pairs.\n",
    "\n",
    "-  **Encoder Self-Attention** |image6|\n",
    "\n",
    "-  **Encoder-Decoder Attention** Most words in target sequence attend on\n",
    "   their related words in source sequence, for example: when generating\n",
    "   “See” (in De), several heads attend on “lake”; when generating\n",
    "   “Eisfischerhütte”, several heads attend on “ice”. |image7|\n",
    "\n",
    "-  **Decoder Self-Attention** Most words attend on their previous few\n",
    "   words. |image8|\n",
    "\n",
    "## Adaptive Universal Transformer\n",
    "\n",
    "A recent research paper by Google, [Universal\n",
    "Transformer](https://arxiv.org/pdf/1807.03819.pdf)_, is an example to\n",
    "show how ``update_graph`` adapts to more complex updating rules.\n",
    "\n",
    "The Universal Transformer was proposed to address the problem that\n",
    "vanilla Transformer is not computationally universal by introducing\n",
    "recurrence in Transformer:\n",
    "\n",
    "-  The basic idea of Universal Transformer is to repeatedly revise its\n",
    "   representations of all symbols in the sequence with each recurrent\n",
    "   step by applying a Transformer layer on the representations.\n",
    "-  Compared to vanilla Transformer, Universal Transformer shares weights\n",
    "   among its layers, and it does not fix the recurrence time (which\n",
    "   means the number of layers in Transformer).\n",
    "\n",
    "A further optimization employs an [adaptive computation time\n",
    "(ACT)](https://arxiv.org/pdf/1603.08983.pdf)_ mechanism to allow the\n",
    "model to dynamically adjust the number of times the representation of\n",
    "each position in a sequence is revised (refereed to as **step**\n",
    "hereafter). This model is also known as the Adaptive Universal\n",
    "Transformer (AUT).\n",
    "\n",
    "In AUT, you maintain an active nodes list. In each step $t$, we\n",
    "compute a halting probability: $h (0<h<1)$ for all nodes in this\n",
    "list by:\n",
    "\n",
    "\\begin{align}h^t_i = \\sigma(W_h x^t_i + b_h)\\end{align}\n",
    "\n",
    "then dynamically decide which nodes are still active. A node is halted\n",
    "at time $T$ if and only if\n",
    "$\\sum_{t=1}^{T-1} h_t < 1 - \\varepsilon \\leq \\sum_{t=1}^{T}h_t$.\n",
    "Halted nodes are removed from the list. The procedure proceeds until the\n",
    "list is empty or a pre-defined maximum step is reached. From DGL’s\n",
    "perspective, this means that the “active” graph becomes sparser over\n",
    "time.\n",
    "\n",
    "The final state of a node $s_i$ is a weighted average of\n",
    "$x_i^t$ by $h_i^t$:\n",
    "\n",
    "\\begin{align}s_i = \\sum_{t=1}^{T} h_i^t\\cdot x_i^t\\end{align}\n",
    "\n",
    "In DGL, implement an algorithm by calling\n",
    "``update_graph`` on nodes that are still active and edges associated\n",
    "with this nodes. The following code shows the Universal Transformer\n",
    "class in DGL:\n",
    "\n",
    ".. code::\n",
    "\n",
    "   class UTransformer(nn.Module):\n",
    "       \"Universal Transformer(https://arxiv.org/pdf/1807.03819.pdf) with ACT(https://arxiv.org/pdf/1603.08983.pdf).\"\n",
    "       MAX_DEPTH = 8\n",
    "       thres = 0.99\n",
    "       act_loss_weight = 0.01\n",
    "       def __init__(self, encoder, decoder, src_embed, tgt_embed, pos_enc, time_enc, generator, h, d_k):\n",
    "           super(UTransformer, self).__init__()\n",
    "           self.encoder,  self.decoder = encoder, decoder\n",
    "           self.src_embed, self.tgt_embed = src_embed, tgt_embed\n",
    "           self.pos_enc, self.time_enc = pos_enc, time_enc\n",
    "           self.halt_enc = HaltingUnit(h * d_k)\n",
    "           self.halt_dec = HaltingUnit(h * d_k)\n",
    "           self.generator = generator\n",
    "           self.h, self.d_k = h, d_k\n",
    "\n",
    "       def step_forward(self, nodes):\n",
    "           # add positional encoding and time encoding, increment step by one\n",
    "           x = nodes.data['x']\n",
    "           step = nodes.data['step']\n",
    "           pos = nodes.data['pos']\n",
    "           return {'x': self.pos_enc.dropout(x + self.pos_enc(pos.view(-1)) + self.time_enc(step.view(-1))),\n",
    "                   'step': step + 1}\n",
    "\n",
    "       def halt_and_accum(self, name, end=False):\n",
    "           \"field: 'enc' or 'dec'\"\n",
    "           halt = self.halt_enc if name == 'enc' else self.halt_dec\n",
    "           thres = self.thres\n",
    "           def func(nodes):\n",
    "               p = halt(nodes.data['x'])\n",
    "               sum_p = nodes.data['sum_p'] + p\n",
    "               active = (sum_p < thres) & (1 - end)\n",
    "               _continue = active.float()\n",
    "               r = nodes.data['r'] * (1 - _continue) + (1 - sum_p) * _continue\n",
    "               s = nodes.data['s'] + ((1 - _continue) * r + _continue * p) * nodes.data['x']\n",
    "               return {'p': p, 'sum_p': sum_p, 'r': r, 's': s, 'active': active}\n",
    "           return func\n",
    "\n",
    "       def propagate_attention(self, g, eids):\n",
    "           # Compute attention score\n",
    "           g.apply_edges(src_dot_dst('k', 'q', 'score'), eids)\n",
    "           g.apply_edges(scaled_exp('score', np.sqrt(self.d_k)), eids)\n",
    "           # Send weighted values to target nodes\n",
    "           g.send_and_recv(eids,\n",
    "                           [fn.u_mul_e('v', 'score', 'v'), fn.copy_e('score', 'score')],\n",
    "                           [fn.sum('v', 'wv'), fn.sum('score', 'z')])\n",
    "\n",
    "       def update_graph(self, g, eids, pre_pairs, post_pairs):\n",
    "           \"Update the node states and edge states of the graph.\"\n",
    "           # Pre-compute queries and key-value pairs.\n",
    "           for pre_func, nids in pre_pairs:\n",
    "               g.apply_nodes(pre_func, nids)\n",
    "           self.propagate_attention(g, eids)\n",
    "           # Further calculation after attention mechanism\n",
    "           for post_func, nids in post_pairs:\n",
    "               g.apply_nodes(post_func, nids)\n",
    "\n",
    "       def forward(self, graph):\n",
    "           g = graph.g\n",
    "           N, E = graph.n_nodes, graph.n_edges\n",
    "           nids, eids = graph.nids, graph.eids\n",
    "\n",
    "           # embed & pos\n",
    "           g.nodes[nids['enc']].data['x'] = self.src_embed(graph.src[0])\n",
    "           g.nodes[nids['dec']].data['x'] = self.tgt_embed(graph.tgt[0])\n",
    "           g.nodes[nids['enc']].data['pos'] = graph.src[1]\n",
    "           g.nodes[nids['dec']].data['pos'] = graph.tgt[1]\n",
    "\n",
    "           # init step\n",
    "           device = next(self.parameters()).device\n",
    "           g.ndata['s'] = th.zeros(N, self.h * self.d_k, dtype=th.float, device=device)    # accumulated state\n",
    "           g.ndata['p'] = th.zeros(N, 1, dtype=th.float, device=device)                    # halting prob\n",
    "           g.ndata['r'] = th.ones(N, 1, dtype=th.float, device=device)                     # remainder\n",
    "           g.ndata['sum_p'] = th.zeros(N, 1, dtype=th.float, device=device)                # sum of pondering values\n",
    "           g.ndata['step'] = th.zeros(N, 1, dtype=th.long, device=device)                  # step\n",
    "           g.ndata['active'] = th.ones(N, 1, dtype=th.uint8, device=device)                # active\n",
    "\n",
    "           for step in range(self.MAX_DEPTH):\n",
    "               pre_func = self.encoder.pre_func('qkv')\n",
    "               post_func = self.encoder.post_func()\n",
    "               nodes = g.filter_nodes(lambda v: v.data['active'].view(-1), nids['enc'])\n",
    "               if len(nodes) == 0: break\n",
    "               edges = g.filter_edges(lambda e: e.dst['active'].view(-1), eids['ee'])\n",
    "               end = step == self.MAX_DEPTH - 1\n",
    "               self.update_graph(g, edges,\n",
    "                                 [(self.step_forward, nodes), (pre_func, nodes)],\n",
    "                                 [(post_func, nodes), (self.halt_and_accum('enc', end), nodes)])\n",
    "\n",
    "           g.nodes[nids['enc']].data['x'] = self.encoder.norm(g.nodes[nids['enc']].data['s'])\n",
    "\n",
    "           for step in range(self.MAX_DEPTH):\n",
    "               pre_func = self.decoder.pre_func('qkv')\n",
    "               post_func = self.decoder.post_func()\n",
    "               nodes = g.filter_nodes(lambda v: v.data['active'].view(-1), nids['dec'])\n",
    "               if len(nodes) == 0: break\n",
    "               edges = g.filter_edges(lambda e: e.dst['active'].view(-1), eids['dd'])\n",
    "               self.update_graph(g, edges,\n",
    "                                 [(self.step_forward, nodes), (pre_func, nodes)],\n",
    "                                 [(post_func, nodes)])\n",
    "\n",
    "               pre_q = self.decoder.pre_func('q', 1)\n",
    "               pre_kv = self.decoder.pre_func('kv', 1)\n",
    "               post_func = self.decoder.post_func(1)\n",
    "               nodes_e = nids['enc']\n",
    "               edges = g.filter_edges(lambda e: e.dst['active'].view(-1), eids['ed'])\n",
    "               end = step == self.MAX_DEPTH - 1\n",
    "               self.update_graph(g, edges,\n",
    "                                 [(pre_q, nodes), (pre_kv, nodes_e)],\n",
    "                                 [(post_func, nodes), (self.halt_and_accum('dec', end), nodes)])\n",
    "\n",
    "           g.nodes[nids['dec']].data['x'] = self.decoder.norm(g.nodes[nids['dec']].data['s'])\n",
    "           act_loss = th.mean(g.ndata['r']) # ACT loss\n",
    "\n",
    "           return self.generator(g.ndata['x'][nids['dec']]), act_loss * self.act_loss_weight\n",
    "\n",
    "Call ``filter_nodes`` and ``filter_edge`` to find nodes/edges\n",
    "that are still active:\n",
    "\n",
    "<div class=\"alert alert-info\"><h4>Note</h4><p>- :func:`~dgl.DGLGraph.filter_nodes` takes a predicate and a node\n",
    "     ID list/tensor as input, then returns a tensor of node IDs that satisfy\n",
    "     the given predicate.\n",
    "   - :func:`~dgl.DGLGraph.filter_edges` takes a predicate\n",
    "     and an edge ID list/tensor as input, then returns a tensor of edge IDs\n",
    "     that satisfy the given predicate.</p></div>\n",
    "\n",
    "For the full implementation, see the [GitHub\n",
    "repo](https://github.com/dmlc/dgl/tree/master/examples/pytorch/transformer/modules/act.py)_.\n",
    "\n",
    "The figure below shows the effect of Adaptive Computational\n",
    "Time. Different positions of a sentence were revised different times.\n",
    "\n",
    "|image9|\n",
    "\n",
    "You can also visualize the dynamics of step distribution on nodes during the\n",
    "training of AUT on sort task(reach 99.7% accuracy), which demonstrates\n",
    "how AUT learns to reduce recurrence steps during training. |image10|\n",
    "\n",
    ".. |image0| image:: https://i.imgur.com/zV5LmTX.png\n",
    ".. |image1| image:: https://i.imgur.com/dETQMMx.png\n",
    ".. |image2| image:: https://i.imgur.com/hnGP229.png\n",
    ".. |image3| image:: https://i.imgur.com/Hj2rRGT.png\n",
    ".. |image4| image:: https://i.imgur.com/zlUpJ41.png\n",
    ".. |image5| image:: https://s1.ax1x.com/2018/12/06/F126xI.gif\n",
    ".. |image6| image:: https://i.imgur.com/HjYb7F2.png\n",
    ".. |image7| image:: https://i.imgur.com/383J5O5.png\n",
    ".. |image8| image:: https://i.imgur.com/c0UWB1V.png\n",
    ".. |image9| image:: https://s1.ax1x.com/2018/12/06/F1sGod.png\n",
    ".. |image10| image:: https://s1.ax1x.com/2018/12/06/F1r8Cq.gif\n",
    "\n",
    "<div class=\"alert alert-info\"><h4>Note</h4><p>The notebook itself is not executable due to many dependencies.\n",
    "    Download [7_transformer.py](https://data.dgl.ai/tutorial/7_transformer.py)_,\n",
    "    and copy the python script to directory ``examples/pytorch/transformer``\n",
    "    then run ``python 7_transformer.py`` to see how it works.</p></div>\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
