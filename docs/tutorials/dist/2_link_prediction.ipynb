{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Distributed Link Prediction\n",
    "\n",
    "In this tutorial, we will walk through the steps of performing distributed GNN training\n",
    "for a link prediction task. This tutorial assumes that you have read the [Distributed Node Classification](https://docs.dgl.ai/en/latest/tutorials/dist/1_node_classification.html) and [Stochastic Training of GNN for Link Prediction](https://docs.dgl.ai/en/latest/tutorials/large/L2_large_link_prediction.html#sphx-glr-tutorials-large-l2-large-link-prediction-py). The general pipeline is shown below.\n",
    "\n",
    ".. figure:: https://data.dgl.ai/tutorial/link.png\n",
    "   :alt: Imgur\n",
    "\n",
    "\n",
    "## Partition a graph\n",
    "\n",
    "In this tutorial, we will use [OGBL citation2 graph](https://ogb.stanford.edu/docs/linkprop/#ogbl-citation2)\n",
    "as an example to illustrate the graph partitioning. Letâ€™s first load the graph into a DGL graph and convert it \n",
    "into a training graph, validation edges and test edges with :class:`~dgl.data.AsLinkPredDataset`.\n",
    "\n",
    ".. code-block:: python\n",
    "\n",
    "\n",
    "    import os\n",
    "    os.environ['DGLBACKEND'] = 'pytorch'\n",
    "    import dgl\n",
    "    import torch as th\n",
    "    from ogb.linkproppred import DglLinkPropPredDataset\n",
    "    data = DglLinkPropPredDataset(name='ogbl-citation2')\n",
    "    graph = data[0]\n",
    "    data = dgl.data.AsLinkPredDataset(data, [0.8, 0.1, 0.1])\n",
    "    graph_train = data[0]\n",
    "    dgl.distributed.partition_graph(graph_train, graph_name='ogbl-citation2', num_parts=4,\n",
    "                                out_path='4part_data',\n",
    "                                balance_edges=True)\n",
    "\n",
    "\n",
    "\n",
    "Then, we store the validation and test edges with the graph partitions.\n",
    "\n",
    "\n",
    "\n",
    ".. code-block:: python\n",
    "\n",
    "\n",
    "    import pickle\n",
    "    with open('4part_data/val.pkl', 'wb') as f:\n",
    "        pickle.dump(data.val_edges, f)\n",
    "    with open('4part_data/test.pkl', 'wb') as f:\n",
    "        pickle.dump(data.test_edges, f)\n",
    "\n",
    "\n",
    "\n",
    "## Distributed training script\n",
    "\n",
    "The distributed link prediction script is very similar to distributed node classification script with just a few modifications.\n",
    "\n",
    "\n",
    "### Initialize network communication\n",
    "\n",
    "We first initialize the network communication and Pytorch's distributed communication. \n",
    "\n",
    "```python\n",
    "import dgl\n",
    "import torch as th\n",
    "dgl.distributed.initialize(ip_config='ip_config.txt')\n",
    "th.distributed.init_process_group(backend='gloo')\n",
    "```\n",
    "The configuration file `ip_config.txt` has the following format:\n",
    "\n",
    "```shell\n",
    "ip_addr1 [port1]\n",
    "ip_addr2 [port2]\n",
    "```\n",
    "Each row is a machine. The first column is the IP address and the second column is the port for\n",
    "connecting to the DGL server on the machine. The port is optional and the default port is 30050.\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "### Reference to the distributed graph\n",
    "\n",
    "DGL's servers load the graph partitions automatically. After the servers load the partitions,\n",
    "trainers connect to the servers and can start to reference to the distributed graph in the cluster as below.\n",
    "\n",
    "\n",
    "```python\n",
    "g = dgl.distributed.DistGraph('ogbl-citation2')\n",
    "```\n",
    "As shown in the code, we refer to a distributed graph by its name. This name is basically the one passed\n",
    "to the `partition_graph` function as shown in the section above.\n",
    "\n",
    "### Get training and validation node IDs\n",
    "\n",
    "For distributed training, each trainer can run its own set of training nodes. We can get the current graph in the trainer with its node ids and edge ids \n",
    "by invoking `node_split` and `edge_split`. We can also get the valid edges and test edges by loading the pickle files.\n",
    "\n",
    "\n",
    "\n",
    "```python\n",
    "train_eids = dgl.distributed.edge_split(th.ones((g.num_edges(),), dtype=th.bool), g.get_partition_book(), force_even=True)\n",
    "train_nids = dgl.distributed.node_split(th.ones((g.num_nodes(),), dtype=th.bool), g.get_partition_book())\n",
    "with open('4part_data/val.pkl', 'rb') as f:\n",
    "    global_valid_eid = pickle.load(f)\n",
    "with open('4part_data/test.pkl', 'rb') as f:\n",
    "    global_test_eid = pickle.load(f)\n",
    "```\n",
    "### Define a GNN model\n",
    "\n",
    "For distributed training, we define a GNN model exactly in the same way as\n",
    "[mini-batch training](https://doc.dgl.ai/guide/minibatch.html#) or\n",
    "[full-graph training](https://doc.dgl.ai/guide/training-node.html#guide-training-node-classification).\n",
    "The code below defines the GraphSage model.\n",
    "\n",
    "\n",
    "```python\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import dgl.nn as dglnn\n",
    "import torch.optim as optim\n",
    "\n",
    "class SAGE(nn.Module):\n",
    "    def __init__(self, in_feats, n_hidden, n_classes, n_layers):\n",
    "        super().__init__()\n",
    "        self.n_layers = n_layers\n",
    "        self.n_hidden = n_hidden\n",
    "        self.n_classes = n_classes\n",
    "        self.layers = nn.ModuleList()\n",
    "        self.layers.append(dglnn.SAGEConv(in_feats, n_hidden, 'mean'))\n",
    "        for i in range(1, n_layers - 1):\n",
    "            self.layers.append(dglnn.SAGEConv(n_hidden, n_hidden, 'mean'))\n",
    "        self.layers.append(dglnn.SAGEConv(n_hidden, n_classes, 'mean'))\n",
    "\n",
    "    def forward(self, blocks, x):\n",
    "        for l, (layer, block) in enumerate(zip(self.layers, blocks)):\n",
    "            x = layer(block, x)\n",
    "            if l != self.n_layers - 1:\n",
    "                x = F.relu(x)\n",
    "        return x\n",
    "\n",
    "num_hidden = 256\n",
    "num_labels = len(th.unique(g.ndata['labels'][0:g.num_nodes()]))\n",
    "num_layers = 2\n",
    "lr = 0.001\n",
    "model = SAGE(g.ndata['feat'].shape[1], num_hidden, num_labels, num_layers)\n",
    "loss_fcn = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "```\n",
    "For distributed training, we need to convert the model into a distributed model with\n",
    "Pytorch's `DistributedDataParallel`.\n",
    "\n",
    "\n",
    "```python\n",
    "model = th.nn.parallel.DistributedDataParallel(model)\n",
    "```\n",
    "We also define an edge predictor :class:`~dgl.nn.pytorch.link.EdgePredictor` to predict the edge scores of pairs of node representations\n",
    "\n",
    "```python\n",
    "from dgl.nn import EdgePredictor\n",
    "predictor = EdgePredictor('dot')\n",
    "```\n",
    "### Distributed mini-batch sampler\n",
    "\n",
    "We can use :class:`~dgl.dataloading.pytorch.DistEdgeDataLoader`, the distributed counterpart\n",
    "of :class:`~dgl.dataloading.pytorch.EdgeDataLoader`, to create a distributed mini-batch sampler for\n",
    "link prediction. \n",
    "\n",
    "\n",
    "\n",
    ".. code-block:: python\n",
    "    sampler = dgl.dataloading.MultiLayerNeighborSampler([25,10])\n",
    "    dataloader = dgl.dataloading.DistEdgeDataLoader(\n",
    "        g=g,\n",
    "        eids=train_eids.numpy(),\n",
    "        graph_sampler=sampler,\n",
    "        negative_sampler=dgl.dataloading.negative_sampler.Uniform(5),\n",
    "        batch_size=args.batch_size,\n",
    "        shuffle=True,\n",
    "        drop_last=False)\n",
    "\n",
    "\n",
    "### Training loop\n",
    "\n",
    "The training loop for distributed training is also exactly the same as the single-process training.\n",
    "\n",
    "\n",
    "```python\n",
    "import sklearn.metrics\n",
    "import numpy as np\n",
    "\n",
    "epoch = 0\n",
    "for epoch in range(10):\n",
    "    for step, (input_nodes, pos_graph, neg_graph, mfgs) in enumerate(dataloader):\n",
    "        pos_graph = pos_graph\n",
    "        neg_graph = neg_graph\n",
    "        node_inputs = mfgs[0].srcdata[dgl.NID]\n",
    "        batch_inputs = g.ndata['feat'][node_inputs]\n",
    "\n",
    "        batch_pred = model(mfgs, batch_inputs)\n",
    "        pos_feature = batch_pred\n",
    "        pos_graph.ndata['h'] = batch_pred\n",
    "        pos_src, pos_dst = pos_graph.edges()\n",
    "        pos_score = predictor(pos_feature[pos_src], pos_feature[pos_dst])\n",
    "\n",
    "        neg_feature = batch_pred\n",
    "        neg_graph.ndata['h'] = batch_pred\n",
    "        neg_src, neg_dst = neg_graph.edges()\n",
    "        neg_score = predictor(neg_feature[pos_src], neg_feature[pos_dst])\n",
    "\n",
    "        score = th.cat([pos_score, neg_score])\n",
    "        label = th.cat([th.ones_like(pos_score), th.zeros_like(neg_score)])\n",
    "        loss = F.binary_cross_entropy_with_logits(score, label)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "```\n",
    "### Inference\n",
    "\n",
    "In the inference stage, we use the model after training loop to get the embedding of nodes.\n",
    "\n",
    "```python\n",
    "def inference(model, graph, node_features, args):\n",
    "    with th.no_grad():\n",
    "        sampler = dgl.dataloading.MultiLayerNeighborSampler([25,10])\n",
    "        train_dataloader = dgl.dataloading.DistNodeDataLoader(\n",
    "            graph, th.arange(graph.num_nodes()), sampler,\n",
    "            batch_size=1024,\n",
    "            shuffle=False,\n",
    "            drop_last=False)\n",
    "\n",
    "        result = []\n",
    "        for input_nodes, output_nodes, mfgs in train_dataloader:\n",
    "            node_inputs = mfgs[0].srcdata[dgl.NID]\n",
    "            inputs = node_features[node_inputs]\n",
    "            result.append(model(mfgs, inputs))\n",
    "\n",
    "        return th.cat(result)\n",
    "\n",
    "node_reprs = inference(model, g, g.ndata['feat'], args)\n",
    "```\n",
    "The test edges is encoded as ((positive_edge_src, positive_edge_dst), (negative_edge_src, negative_edge_dst)). Therefore, we can get the ground truth with positive pairs and negative pairs. \n",
    "\n",
    "```python\n",
    "test_pos_src = global_test_eid[0][0]\n",
    "test_pos_dst = global_test_eid[0][1]\n",
    "test_neg_src = global_test_eid[1][0]\n",
    "test_neg_dst = global_test_eid[1][1]\n",
    "test_labels = th.cat([th.ones_like(test_pos_src), th.zeros_like(test_neg_src)]).cpu().numpy()\n",
    "```\n",
    "Then, we use the dot product predictor to get the score of positive and negative test pairs to compute metrics such as AUC:\n",
    "\n",
    "```python\n",
    "h_pos_src = node_reprs[test_pos_src]\n",
    "h_pos_dst = node_reprs[test_pos_dst]\n",
    "h_neg_src = node_reprs[test_neg_src]\n",
    "h_neg_dst = node_reprs[test_neg_dst]\n",
    "score_pos = predictor(h_pos_src, h_pos_dst)\n",
    "score_neg = predictor(h_neg_src, h_neg_dst)\n",
    "\n",
    "test_preds = th.cat([score_pos, score_neg]).cpu().numpy()\n",
    "auc = skm.roc_auc_score(test_labels, test_preds)\n",
    "```\n",
    "## Set up distributed training environment\n",
    "\n",
    "The distributed training environment set up is similar to the distributed node classification. Please refer here for more details:\n",
    "[Set up distributed training environment](https://docs.dgl.ai/en/latest/tutorials/dist/1_node_classification.html#set-up-distributed-training-environment)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
