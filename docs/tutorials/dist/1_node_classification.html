<!DOCTYPE html>
<html class="writer-html5" lang="en" data-content_root="../../">
<head>
  <meta charset="utf-8" /><meta name="viewport" content="width=device-width, initial-scale=1" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Distributed Node Classification &mdash; DGL 2.4 documentation</title>
      <link rel="stylesheet" type="text/css" href="../../_static/pygments.css?v=80d5e7a1" />
      <link rel="stylesheet" type="text/css" href="../../_static/css/theme.css?v=19f00094" />
      <link rel="stylesheet" type="text/css" href="../../_static/graphviz.css?v=fd3f3429" />
      <link rel="stylesheet" type="text/css" href="../../_static/copybutton.css?v=76b2166b" />
      <link rel="stylesheet" type="text/css" href="../../_static/sg_gallery.css?v=61a4c737" />
      <link rel="stylesheet" type="text/css" href="../../_static/sg_gallery-binder.css?v=f4aeca0c" />
      <link rel="stylesheet" type="text/css" href="../../_static/sg_gallery-dataframe.css?v=2082cf3c" />
      <link rel="stylesheet" type="text/css" href="../../_static/sg_gallery-rendered-html.css?v=1277b6f3" />
      <link rel="stylesheet" type="text/css" href="../../_static/css/custom.css?v=0bf289b5" />

  
  <!--[if lt IE 9]>
    <script src="../../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script src="../../_static/jquery.js?v=5d32c60e"></script>
        <script src="../../_static/_sphinx_javascript_frameworks_compat.js?v=2cd50e6c"></script>
        <script src="../../_static/documentation_options.js?v=9caaf7ed"></script>
        <script src="../../_static/doctools.js?v=9a2dae69"></script>
        <script src="../../_static/sphinx_highlight.js?v=dc90522c"></script>
        <script src="../../_static/clipboard.min.js?v=a7894cd8"></script>
        <script src="../../_static/copybutton.js?v=ccdb6887"></script>
        <script crossorigin="anonymous" integrity="sha256-Ae2Vz/4ePdIu6ZyI/5ZGsYnb+m0JlOmKPjt6XZ9JJkA=" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
        <script src="https://cdn.jsdelivr.net/npm/@jupyter-widgets/html-manager@^1.0.1/dist/embed-amd.js"></script>
        <script>window.MathJax = {"tex": {"inlineMath": [["$", "$"], ["\\(", "\\)"]], "processEscapes": true}, "options": {"ignoreHtmlClass": "tex2jax_ignore|mathjax_ignore|document", "processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
        <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script src="../../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="next" title="Distributed Link Prediction" href="2_link_prediction.html" />
    <link rel="prev" title="Distributed training" href="index.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="../../index.html" class="icon icon-home">
            DGL
          </a>
              <div class="version">
                2.4
              </div>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">Get Started</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../install/index.html">Install and Setup</a></li>
<li class="toctree-l1"><a class="reference internal" href="../blitz/index.html">A Blitz Introduction to DGL</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Advanced Materials</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../../stochastic_training/index.html">🆕 Stochastic Training of GNNs with GraphBolt</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../guide/index.html">User Guide</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../guide_cn/index.html">用户指南【包含过时信息】</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../guide_ko/index.html">사용자 가이드[시대에 뒤쳐진]</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../graphtransformer/index.html">🆕 Tutorial: Graph Transformer</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../notebooks/sparse/index.html">Tutorials: dgl.sparse</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cpu/index.html">Training on CPUs</a></li>
<li class="toctree-l1"><a class="reference internal" href="../multi/index.html">Training on Multiple GPUs</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="index.html">Distributed training</a><ul class="current">
<li class="toctree-l2 current"><a class="current reference internal" href="#">Distributed Node Classification</a></li>
<li class="toctree-l2"><a class="reference internal" href="2_link_prediction.html">Distributed Link Prediction</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../models/index.html">Paper Study with DGL</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">API Reference</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../api/python/dgl.html">dgl</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../api/python/dgl.data.html">dgl.data</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../api/python/dgl.dataloading.html">dgl.dataloading</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../api/python/dgl.DGLGraph.html">dgl.DGLGraph</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../api/python/dgl.distributed.html">dgl.distributed</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../api/python/dgl.function.html">dgl.function</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../api/python/dgl.geometry.html">dgl.geometry</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../api/python/dgl.graphbolt.html">🆕 dgl.graphbolt</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../api/python/nn-pytorch.html">dgl.nn (PyTorch)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../api/python/nn.functional.html">dgl.nn.functional</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../api/python/dgl.ops.html">dgl.ops</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../api/python/dgl.optim.html">dgl.optim</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../api/python/dgl.sampling.html">dgl.sampling</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../api/python/dgl.sparse_v0.html">dgl.sparse</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../api/python/dgl.multiprocessing.html">dgl.multiprocessing</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../api/python/transforms.html">dgl.transforms</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../api/python/udf.html">User-defined Functions</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Notes</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../contribute.html">Contribute to DGL</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../developer/ffi.html">DGL Foreign Function Interface (FFI)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../performance.html">Performance Benchmarks</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Misc</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../faq.html">Frequently Asked Questions (FAQ)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../env_var.html">Environment Variables</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../resources.html">Resources</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../index.html">DGL</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../../index.html" class="icon icon-home" aria-label="Home"></a></li>
          <li class="breadcrumb-item"><a href="index.html">Distributed training</a></li>
      <li class="breadcrumb-item active">Distributed Node Classification</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../../_sources/tutorials/dist/1_node_classification.ipynb.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="Distributed-Node-Classification">
<h1>Distributed Node Classification<a class="headerlink" href="#Distributed-Node-Classification" title="Link to this heading"></a></h1>
<p>In this tutorial, we will walk through the steps of performing distributed GNN training for a node classification task. To understand distributed GNN training, you need to read the tutorial of multi-GPU training first. This tutorial is developed on top of multi-GPU training by providing extra steps for partitioning a graph, modifying the training script and setting up the environment for distributed training.</p>
<section id="Partition-a-graph">
<h2>Partition a graph<a class="headerlink" href="#Partition-a-graph" title="Link to this heading"></a></h2>
<p>In this tutorial, we will use <a class="reference external" href="https://ogb.stanford.edu/docs/nodeprop/#ogbn-products">OGBN products graph</a> as an example to illustrate the graph partitioning. Let’s first load the graph into a DGL graph. Here we store the node labels as node data in the DGL Graph.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span>
</pre></div>
</div>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>import os
os.environ[&#39;DGLBACKEND&#39;] = &#39;pytorch&#39;
import dgl
import torch as th
from ogb.nodeproppred import DglNodePropPredDataset
data = DglNodePropPredDataset(name=&#39;ogbn-products&#39;)
graph, labels = data[0]
labels = labels[:, 0]
graph.ndata[&#39;labels&#39;] = labels
</pre></div>
</div>
<p>We need to split the data into training/validation/test set during the graph partitioning. Because this is a node classification task, the training/validation/test sets contain node IDs. We recommend users to convert them as boolean arrays, in which True indicates the existence of the node ID in the set. In this way, we can store them as node data. After the partitioning, the boolean arrays will be stored with the graph partitions.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span>
</pre></div>
</div>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>splitted_idx = data.get_idx_split()
train_nid, val_nid, test_nid = splitted_idx[&#39;train&#39;], splitted_idx[&#39;valid&#39;], splitted_idx[&#39;test&#39;]
train_mask = th.zeros((graph.num_nodes(),), dtype=th.bool)
train_mask[train_nid] = True
val_mask = th.zeros((graph.num_nodes(),), dtype=th.bool)
val_mask[val_nid] = True
test_mask = th.zeros((graph.num_nodes(),), dtype=th.bool)
test_mask[test_nid] = True
graph.ndata[&#39;train_mask&#39;] = train_mask
graph.ndata[&#39;val_mask&#39;] = val_mask
graph.ndata[&#39;test_mask&#39;] = test_mask
</pre></div>
</div>
<p>Then we call the <code class="docutils literal notranslate"><span class="pre">partition_graph</span></code> function to partition the graph with <a class="reference external" href="http://glaros.dtc.umn.edu/gkhome/metis/metis/overview">METIS</a> and save the partitioned results in the specified folder. <strong>Note</strong>: <code class="docutils literal notranslate"><span class="pre">partition_graph</span></code> runs on a single machine with a single thread. You can go to <a class="reference external" href="https://docs.dgl.ai/en/latest/guide/distributed-preprocessing.html#distributed-partitioning">our user guide</a> to see more information on distributed graph partitioning.</p>
<p>The code below shows an example of invoking the partitioning algorithm and generate four partitions. The partitioned results are stored in a folder called <code class="docutils literal notranslate"><span class="pre">4part_data</span></code>. While partitioning a graph, we allow users to specify how to balance the partitions. By default, the algorithm balances the number of nodes in each partition as much as possible. However, this balancing strategy is not sufficient for distributed GNN training because some partitions may have many more training nodes than other
partitions or some partitions may have more edges than others. As such, <code class="docutils literal notranslate"><span class="pre">partition_graph</span></code> provides two additional arguments <code class="docutils literal notranslate"><span class="pre">balance_ntypes</span></code> and <code class="docutils literal notranslate"><span class="pre">balance_edges</span></code> to enforce more balancing criteria. For example, we can use the training mask to balance the number of training nodes in each partition, as shown in the example below. We can also turn on the <code class="docutils literal notranslate"><span class="pre">balance_edges</span></code> flag to ensure that all partitions have roughly the same number of edges.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span>
</pre></div>
</div>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>dgl.distributed.partition_graph(graph, graph_name=&#39;ogbn-products&#39;, num_parts=4,
                                out_path=&#39;4part_data&#39;,
                                balance_ntypes=graph.ndata[&#39;train_mask&#39;],
                                balance_edges=True)
</pre></div>
</div>
<p>When partitioning a graph, DGL shuffles node IDs and edge IDs so that nodes/edges assigned to a partition have contiguous IDs. This is necessary for DGL to maintain the mappings of global node/edge IDs and partition IDs. If a user needs to map the shuffled node/edge IDs to their original IDs, they can turn on the <code class="docutils literal notranslate"><span class="pre">return_mapping</span></code> flag of <code class="docutils literal notranslate"><span class="pre">partition_graph</span></code>, which returns a vector for the node ID mapping and edge ID mapping. Below shows an example of using the ID mapping to save the node
embeddings after distributed training. This is a common use case when users want to use the trained node embeddings in their downstream task. Below let’s assume that the trained node embeddings are stored in the <code class="docutils literal notranslate"><span class="pre">node_emb</span></code> tensor, which is indexed by the shuffled node IDs. We shuffle the embeddings again and store them in the <code class="docutils literal notranslate"><span class="pre">orig_node_emb</span></code> tensor, which is indexed by the original node IDs.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span>
</pre></div>
</div>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>nmap, emap = dgl.distributed.partition_graph(graph, graph_name=&#39;ogbn-products&#39;,
                                             num_parts=4,
                                             out_path=&#39;4part_data&#39;,
                                             balance_ntypes=graph.ndata[&#39;train_mask&#39;],
                                             balance_edges=True,
                                             return_mapping=True)
orig_node_emb = th.zeros(node_emb.shape, dtype=node_emb.dtype)
orig_node_emb[nmap] = node_emb
</pre></div>
</div>
</section>
<section id="Distributed-training-script">
<h2>Distributed training script<a class="headerlink" href="#Distributed-training-script" title="Link to this heading"></a></h2>
<p>The distributed training script is very similar to multi-GPU training script with just a few modifications. It also relies on the Pytorch distributed component to exchange gradients and update model parameters. The distributed training script only contains the code of the trainers.</p>
<section id="Initialize-network-communication">
<h3>Initialize network communication<a class="headerlink" href="#Initialize-network-communication" title="Link to this heading"></a></h3>
<p>Distributed GNN training requires to access the partitioned graph structure and node/edge features as well as aggregating the gradients of model parameters from multiple trainers. DGL’s distributed component is responsible for accessing the distributed graph structure and distributed node features and edge features while Pytorch distributed is responsible for exchanging the gradients of model parameters. As such, we need to initialize both DGL and Pytorch distributed components at the beginning
of the training script.</p>
<p>We need to call DGL’s initialize function to initialize the trainers’ network communication and connect with DGL’s servers at the very beginning of the distributed training script. This function has an argument that accepts the path to the cluster configuration file.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">dgl</span>
<span class="kn">import</span> <span class="nn">torch</span> <span class="k">as</span> <span class="nn">th</span>
<span class="n">dgl</span><span class="o">.</span><span class="n">distributed</span><span class="o">.</span><span class="n">initialize</span><span class="p">(</span><span class="n">ip_config</span><span class="o">=</span><span class="s1">&#39;ip_config.txt&#39;</span><span class="p">)</span>
</pre></div>
</div>
<p>The configuration file <code class="docutils literal notranslate"><span class="pre">ip_config.txt</span></code> has the following format:</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>ip_addr1<span class="w"> </span><span class="o">[</span>port1<span class="o">]</span>
ip_addr2<span class="w"> </span><span class="o">[</span>port2<span class="o">]</span>
</pre></div>
</div>
<p>Each row is a machine. The first column is the IP address and the second column is the port for connecting to the DGL server on the machine. The port is optional and the default port is 30050.</p>
<p>After initializing DGL’s network communication, a user can initialize Pytorch’s distributed communication.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">th</span><span class="o">.</span><span class="n">distributed</span><span class="o">.</span><span class="n">init_process_group</span><span class="p">(</span><span class="n">backend</span><span class="o">=</span><span class="s1">&#39;gloo&#39;</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="Reference-to-the-distributed-graph">
<h3>Reference to the distributed graph<a class="headerlink" href="#Reference-to-the-distributed-graph" title="Link to this heading"></a></h3>
<p>DGL’s servers load the graph partitions automatically. After the servers load the partitions, trainers connect to the servers and can start to reference to the distributed graph in the cluster as below.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">g</span> <span class="o">=</span> <span class="n">dgl</span><span class="o">.</span><span class="n">distributed</span><span class="o">.</span><span class="n">DistGraph</span><span class="p">(</span><span class="s1">&#39;ogbn-products&#39;</span><span class="p">)</span>
</pre></div>
</div>
<p>As shown in the code, we refer to a distributed graph by its name. This name is basically the one passed to the <code class="docutils literal notranslate"><span class="pre">partition_graph</span></code> function as shown in the section above.</p>
</section>
<section id="Get-training-and-validation-node-IDs">
<h3>Get training and validation node IDs<a class="headerlink" href="#Get-training-and-validation-node-IDs" title="Link to this heading"></a></h3>
<p>For distributed training, each trainer can run its own set of training nodes. The training nodes of the entire graph are stored in a distributed tensor as the <code class="docutils literal notranslate"><span class="pre">train_mask</span></code> node data, which was constructed before we partitioned the graph. Each trainer can invoke <code class="docutils literal notranslate"><span class="pre">node_split</span></code> to its set of training nodes. The <code class="docutils literal notranslate"><span class="pre">node_split</span></code> function splits the full training set evenly and returns the training nodes, majority of which are stored in the local partition, to ensure good data locality.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">train_nid</span> <span class="o">=</span> <span class="n">dgl</span><span class="o">.</span><span class="n">distributed</span><span class="o">.</span><span class="n">node_split</span><span class="p">(</span><span class="n">g</span><span class="o">.</span><span class="n">ndata</span><span class="p">[</span><span class="s1">&#39;train_mask&#39;</span><span class="p">])</span>
</pre></div>
</div>
<p>We can split the validation nodes in the same way as above. In this case, each trainer gets a different set of validation nodes.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">valid_nid</span> <span class="o">=</span> <span class="n">dgl</span><span class="o">.</span><span class="n">distributed</span><span class="o">.</span><span class="n">node_split</span><span class="p">(</span><span class="n">g</span><span class="o">.</span><span class="n">ndata</span><span class="p">[</span><span class="s1">&#39;val_mask&#39;</span><span class="p">])</span>
</pre></div>
</div>
</section>
<section id="Define-a-GNN-model">
<h3>Define a GNN model<a class="headerlink" href="#Define-a-GNN-model" title="Link to this heading"></a></h3>
<p>For distributed training, we define a GNN model exactly in the same way as <a class="reference external" href="https://doc.dgl.ai/guide/minibatch.html#">mini-batch training</a> or <a class="reference external" href="https://doc.dgl.ai/guide/training-node.html#guide-training-node-classification">full-graph training</a>. The code below defines the GraphSage model.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">torch.nn</span> <span class="k">as</span> <span class="nn">nn</span>
<span class="kn">import</span> <span class="nn">torch.nn.functional</span> <span class="k">as</span> <span class="nn">F</span>
<span class="kn">import</span> <span class="nn">dgl.nn</span> <span class="k">as</span> <span class="nn">dglnn</span>
<span class="kn">import</span> <span class="nn">torch.optim</span> <span class="k">as</span> <span class="nn">optim</span>

<span class="k">class</span> <span class="nc">SAGE</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">in_feats</span><span class="p">,</span> <span class="n">n_hidden</span><span class="p">,</span> <span class="n">n_classes</span><span class="p">,</span> <span class="n">n_layers</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">n_layers</span> <span class="o">=</span> <span class="n">n_layers</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">n_hidden</span> <span class="o">=</span> <span class="n">n_hidden</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">n_classes</span> <span class="o">=</span> <span class="n">n_classes</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">layers</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ModuleList</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">dglnn</span><span class="o">.</span><span class="n">SAGEConv</span><span class="p">(</span><span class="n">in_feats</span><span class="p">,</span> <span class="n">n_hidden</span><span class="p">,</span> <span class="s1">&#39;mean&#39;</span><span class="p">))</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">n_layers</span> <span class="o">-</span> <span class="mi">1</span><span class="p">):</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">dglnn</span><span class="o">.</span><span class="n">SAGEConv</span><span class="p">(</span><span class="n">n_hidden</span><span class="p">,</span> <span class="n">n_hidden</span><span class="p">,</span> <span class="s1">&#39;mean&#39;</span><span class="p">))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">dglnn</span><span class="o">.</span><span class="n">SAGEConv</span><span class="p">(</span><span class="n">n_hidden</span><span class="p">,</span> <span class="n">n_classes</span><span class="p">,</span> <span class="s1">&#39;mean&#39;</span><span class="p">))</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">blocks</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="k">for</span> <span class="n">l</span><span class="p">,</span> <span class="p">(</span><span class="n">layer</span><span class="p">,</span> <span class="n">block</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="nb">zip</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">layers</span><span class="p">,</span> <span class="n">blocks</span><span class="p">)):</span>
            <span class="n">x</span> <span class="o">=</span> <span class="n">layer</span><span class="p">(</span><span class="n">block</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">l</span> <span class="o">!=</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_layers</span> <span class="o">-</span> <span class="mi">1</span><span class="p">:</span>
                <span class="n">x</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">x</span>

<span class="n">num_hidden</span> <span class="o">=</span> <span class="mi">256</span>
<span class="n">num_labels</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">th</span><span class="o">.</span><span class="n">unique</span><span class="p">(</span><span class="n">g</span><span class="o">.</span><span class="n">ndata</span><span class="p">[</span><span class="s1">&#39;labels&#39;</span><span class="p">][</span><span class="mi">0</span><span class="p">:</span><span class="n">g</span><span class="o">.</span><span class="n">num_nodes</span><span class="p">()]))</span>
<span class="n">num_layers</span> <span class="o">=</span> <span class="mi">2</span>
<span class="n">lr</span> <span class="o">=</span> <span class="mf">0.001</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">SAGE</span><span class="p">(</span><span class="n">g</span><span class="o">.</span><span class="n">ndata</span><span class="p">[</span><span class="s1">&#39;feat&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">num_hidden</span><span class="p">,</span> <span class="n">num_labels</span><span class="p">,</span> <span class="n">num_layers</span><span class="p">)</span>
<span class="n">loss_fcn</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">CrossEntropyLoss</span><span class="p">()</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="n">lr</span><span class="p">)</span>
</pre></div>
</div>
<p>For distributed training, we need to convert the model into a distributed model with Pytorch’s <code class="docutils literal notranslate"><span class="pre">DistributedDataParallel</span></code>.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">model</span> <span class="o">=</span> <span class="n">th</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">parallel</span><span class="o">.</span><span class="n">DistributedDataParallel</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="Distributed-mini-batch-sampler">
<h3>Distributed mini-batch sampler<a class="headerlink" href="#Distributed-mini-batch-sampler" title="Link to this heading"></a></h3>
<p>We can use the same :class:<code class="docutils literal notranslate"><span class="pre">~dgl.dataloading.pytorch.DistNodeDataLoader</span></code>, the distributed counterpart of :class:<code class="docutils literal notranslate"><span class="pre">~dgl.dataloading.pytorch.NodeDataLoader</span></code>, to create a distributed mini-batch sampler for node classification.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">sampler</span> <span class="o">=</span> <span class="n">dgl</span><span class="o">.</span><span class="n">dataloading</span><span class="o">.</span><span class="n">MultiLayerNeighborSampler</span><span class="p">([</span><span class="mi">25</span><span class="p">,</span><span class="mi">10</span><span class="p">])</span>
<span class="n">train_dataloader</span> <span class="o">=</span> <span class="n">dgl</span><span class="o">.</span><span class="n">dataloading</span><span class="o">.</span><span class="n">DistNodeDataLoader</span><span class="p">(</span>
                             <span class="n">g</span><span class="p">,</span> <span class="n">train_nid</span><span class="p">,</span> <span class="n">sampler</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">1024</span><span class="p">,</span>
                             <span class="n">shuffle</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">drop_last</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="n">valid_dataloader</span> <span class="o">=</span> <span class="n">dgl</span><span class="o">.</span><span class="n">dataloading</span><span class="o">.</span><span class="n">DistNodeDataLoader</span><span class="p">(</span>
                             <span class="n">g</span><span class="p">,</span> <span class="n">valid_nid</span><span class="p">,</span> <span class="n">sampler</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">1024</span><span class="p">,</span>
                             <span class="n">shuffle</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">drop_last</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="Training-loop">
<h3>Training loop<a class="headerlink" href="#Training-loop" title="Link to this heading"></a></h3>
<p>The training loop for distributed training is also exactly the same as the single-process training.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">sklearn.metrics</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>

<span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">10</span><span class="p">):</span>
    <span class="c1"># Loop over the dataloader to sample mini-batches.</span>
    <span class="n">losses</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">with</span> <span class="n">model</span><span class="o">.</span><span class="n">join</span><span class="p">():</span>
        <span class="k">for</span> <span class="n">step</span><span class="p">,</span> <span class="p">(</span><span class="n">input_nodes</span><span class="p">,</span> <span class="n">seeds</span><span class="p">,</span> <span class="n">blocks</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">train_dataloader</span><span class="p">):</span>
            <span class="c1"># Load the input features as well as output labels</span>
            <span class="n">batch_inputs</span> <span class="o">=</span> <span class="n">g</span><span class="o">.</span><span class="n">ndata</span><span class="p">[</span><span class="s1">&#39;feat&#39;</span><span class="p">][</span><span class="n">input_nodes</span><span class="p">]</span>
            <span class="n">batch_labels</span> <span class="o">=</span> <span class="n">g</span><span class="o">.</span><span class="n">ndata</span><span class="p">[</span><span class="s1">&#39;labels&#39;</span><span class="p">][</span><span class="n">seeds</span><span class="p">]</span>

            <span class="c1"># Compute loss and prediction</span>
            <span class="n">batch_pred</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">blocks</span><span class="p">,</span> <span class="n">batch_inputs</span><span class="p">)</span>
            <span class="n">loss</span> <span class="o">=</span> <span class="n">loss_fcn</span><span class="p">(</span><span class="n">batch_pred</span><span class="p">,</span> <span class="n">batch_labels</span><span class="p">)</span>
            <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
            <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
            <span class="n">losses</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">loss</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">())</span>
            <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>

    <span class="c1"># validation</span>
    <span class="n">predictions</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">labels</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">with</span> <span class="n">th</span><span class="o">.</span><span class="n">no_grad</span><span class="p">(),</span> <span class="n">model</span><span class="o">.</span><span class="n">join</span><span class="p">():</span>
        <span class="k">for</span> <span class="n">step</span><span class="p">,</span> <span class="p">(</span><span class="n">input_nodes</span><span class="p">,</span> <span class="n">seeds</span><span class="p">,</span> <span class="n">blocks</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">valid_dataloader</span><span class="p">):</span>
            <span class="n">inputs</span> <span class="o">=</span> <span class="n">g</span><span class="o">.</span><span class="n">ndata</span><span class="p">[</span><span class="s1">&#39;feat&#39;</span><span class="p">][</span><span class="n">input_nodes</span><span class="p">]</span>
            <span class="n">labels</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">g</span><span class="o">.</span><span class="n">ndata</span><span class="p">[</span><span class="s1">&#39;labels&#39;</span><span class="p">][</span><span class="n">seeds</span><span class="p">]</span><span class="o">.</span><span class="n">numpy</span><span class="p">())</span>
            <span class="n">predictions</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">model</span><span class="p">(</span><span class="n">blocks</span><span class="p">,</span> <span class="n">inputs</span><span class="p">)</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">numpy</span><span class="p">())</span>
        <span class="n">predictions</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">concatenate</span><span class="p">(</span><span class="n">predictions</span><span class="p">)</span>
        <span class="n">labels</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">concatenate</span><span class="p">(</span><span class="n">labels</span><span class="p">)</span>
        <span class="n">accuracy</span> <span class="o">=</span> <span class="n">sklearn</span><span class="o">.</span><span class="n">metrics</span><span class="o">.</span><span class="n">accuracy_score</span><span class="p">(</span><span class="n">labels</span><span class="p">,</span> <span class="n">predictions</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Epoch </span><span class="si">{}</span><span class="s1">: Validation Accuracy </span><span class="si">{}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">epoch</span><span class="p">,</span> <span class="n">accuracy</span><span class="p">))</span>
</pre></div>
</div>
</section>
</section>
<section id="Set-up-distributed-training-environment">
<h2>Set up distributed training environment<a class="headerlink" href="#Set-up-distributed-training-environment" title="Link to this heading"></a></h2>
<p>After partitioning a graph and preparing the training script, we now need to set up the distributed training environment and launch the training job. Basically, we need to create a cluster of machines and upload both the training script and the partitioned data to each machine in the cluster. A recommended solution of sharing the training script and the partitioned data in the cluster is to use NFS (Network File System).</p>
<p>For any users who are not familiar with NFS, below is a small tutorial of setting up NFS in an existing cluster.</p>
<section id="NFS-server-side-setup-(ubuntu-only)">
<h3>NFS server side setup (ubuntu only)<a class="headerlink" href="#NFS-server-side-setup-(ubuntu-only)" title="Link to this heading"></a></h3>
<p>First, install essential libs on the storage server</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>sudo<span class="w"> </span>apt-get<span class="w"> </span>install<span class="w"> </span>nfs-kernel-server
</pre></div>
</div>
<p>Below we assume the user account is ubuntu and we create a directory of workspace in the home directory.</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>mkdir<span class="w"> </span>-p<span class="w"> </span>/home/ubuntu/workspace
</pre></div>
</div>
<p>We assume that the all servers are under a subnet with ip range 192.168.0.0 to 192.168.255.255. We need to add the following line to <code class="docutils literal notranslate"><span class="pre">/etc/exports</span></code></p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>/home/ubuntu/workspace<span class="w">  </span><span class="m">192</span>.168.0.0/16<span class="o">(</span>rw,sync,no_subtree_check<span class="o">)</span>
</pre></div>
</div>
<p>Then restart NFS, the setup on server side is finished.</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>sudo<span class="w"> </span>systemctl<span class="w"> </span>restart<span class="w"> </span>nfs-kernel-server
</pre></div>
</div>
<p>For configuration details, please refer to NFS ArchWiki (<a class="reference external" href="https://wiki.archlinux.org/index.php/NFS">https://wiki.archlinux.org/index.php/NFS</a>).</p>
</section>
<section id="NFS-client-side-setup-(ubuntu-only)">
<h3>NFS client side setup (ubuntu only)<a class="headerlink" href="#NFS-client-side-setup-(ubuntu-only)" title="Link to this heading"></a></h3>
<p>To use NFS, clients also require to install essential packages</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>sudo<span class="w"> </span>apt-get<span class="w"> </span>install<span class="w"> </span>nfs-common
</pre></div>
</div>
<p>You can either mount the NFS manually</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>mkdir<span class="w"> </span>-p<span class="w"> </span>/home/ubuntu/workspace
sudo<span class="w"> </span>mount<span class="w"> </span>-t<span class="w"> </span>nfs<span class="w"> </span>&lt;nfs-server-ip&gt;:/home/ubuntu/workspace<span class="w"> </span>/home/ubuntu/workspace
</pre></div>
</div>
<p>or add the following line to <code class="docutils literal notranslate"><span class="pre">/etc/fstab</span></code> so the folder will be mounted automatically</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>&lt;nfs-server-ip&gt;:/home/ubuntu/workspace<span class="w">   </span>/home/ubuntu/workspace<span class="w">   </span>nfs<span class="w">   </span>defaults<span class="w">    </span><span class="m">0</span><span class="w"> </span><span class="m">0</span>
</pre></div>
</div>
<p>Then run</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>mount<span class="w"> </span>-a
</pre></div>
</div>
<p>Now go to <code class="docutils literal notranslate"><span class="pre">/home/ubuntu/workspace</span></code> and save the training script and the partitioned data in the folder.</p>
</section>
<section id="SSH-Access">
<h3>SSH Access<a class="headerlink" href="#SSH-Access" title="Link to this heading"></a></h3>
<p>The launch script accesses the machines in the cluster via SSH. Users should follow the instruction in <a class="reference external" href="https://linuxize.com/post/how-to-setup-passwordless-ssh-login/">this document</a> to set up the passwordless SSH login on every machine in the cluster. After setting up the passwordless SSH, users need to authenticate the connection to each machine and add their key fingerprints to <code class="docutils literal notranslate"><span class="pre">~/.ssh/known_hosts</span></code>. This can be done automatically when we ssh to a machine for the first time.</p>
</section>
<section id="Launch-the-distributed-training-job">
<h3>Launch the distributed training job<a class="headerlink" href="#Launch-the-distributed-training-job" title="Link to this heading"></a></h3>
<p>After everything is ready, we can now use the launch script provided by DGL to launch the distributed training job in the cluster. We can run the launch script on any machine in the cluster.</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>python3<span class="w"> </span>~/workspace/dgl/tools/launch.py<span class="w">   </span>--workspace<span class="w"> </span>~/workspace/<span class="w">   </span>--num_trainers<span class="w"> </span><span class="m">1</span><span class="w">   </span>--num_samplers<span class="w"> </span><span class="m">0</span><span class="w">   </span>--num_servers<span class="w"> </span><span class="m">1</span><span class="w">   </span>--part_config<span class="w"> </span>4part_data/ogbn-products.json<span class="w">   </span>--ip_config<span class="w"> </span>ip_config.txt<span class="w">   </span><span class="s2">&quot;python3 train_dist.py&quot;</span>
</pre></div>
</div>
<p>If we split the graph into four partitions as demonstrated at the beginning of the tutorial, the cluster has to have four machines. The command above launches one trainer and one server on each machine in the cluster. <code class="docutils literal notranslate"><span class="pre">ip_config.txt</span></code> lists the IP addresses of all machines in the cluster as follows:</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>ip_addr1
ip_addr2
ip_addr3
ip_addr4
</pre></div>
</div>
</section>
</section>
<section id="Sample-neighbors-with-GraphBolt">
<h2>Sample neighbors with <code class="docutils literal notranslate"><span class="pre">GraphBolt</span></code><a class="headerlink" href="#Sample-neighbors-with-GraphBolt" title="Link to this heading"></a></h2>
<p>Since DGL 2.0, we have introduced a new dataloading framework <a class="reference external" href="https://doc.dgl.ai/stochastic_training/index.html">GraphBolt</a> in which sampling is highly improved compared to previous implementations in DGL. As a result, we’ve introduced <code class="docutils literal notranslate"><span class="pre">GraphBolt</span></code> to distributed training to improve the performance of distributed sampling. What’s more, the graph partitions could be much smaller than before, which is beneficial for the loading speed and memory usage during distributed training.</p>
<section id="Graph-partitioning">
<h3>Graph partitioning<a class="headerlink" href="#Graph-partitioning" title="Link to this heading"></a></h3>
<p>In order to benefit from <code class="docutils literal notranslate"><span class="pre">GraphBolt</span></code> for distributed sampling, we need to convert partitions from <code class="docutils literal notranslate"><span class="pre">DGL</span></code> format to <code class="docutils literal notranslate"><span class="pre">GraphBolt</span></code> format. This can be done by <code class="docutils literal notranslate"><span class="pre">dgl.distributed.dgl_partition_to_graphbolt</span></code> function. Alternatively, we can use <code class="docutils literal notranslate"><span class="pre">dgl.distributed.partition_graph</span></code> function to generate partitions in <code class="docutils literal notranslate"><span class="pre">GraphBolt</span></code> format directly.</p>
<ol class="arabic simple">
<li><p>Convert partitions from <code class="docutils literal notranslate"><span class="pre">DGL</span></code> format to <code class="docutils literal notranslate"><span class="pre">GraphBolt</span></code> format.</p></li>
</ol>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">part_config</span> <span class="o">=</span> <span class="s2">&quot;4part_data/ogbn-products.json&quot;</span>
<span class="n">dgl</span><span class="o">.</span><span class="n">distributed</span><span class="o">.</span><span class="n">dgl_partition_to_graphbolt</span><span class="p">(</span><span class="n">part_config</span><span class="p">)</span>
</pre></div>
</div>
<p>The new partitions will be stored in the same directory as the original partitions.</p>
<ol class="arabic simple" start="2">
<li><p>Generate partitions in <code class="docutils literal notranslate"><span class="pre">GraphBolt</span></code> format directly. Just set the <code class="docutils literal notranslate"><span class="pre">use_graphbolt</span></code> flag to <code class="docutils literal notranslate"><span class="pre">True</span></code> in <code class="docutils literal notranslate"><span class="pre">partition_graph</span></code> function.</p></li>
</ol>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">dgl</span><span class="o">.</span><span class="n">distributed</span><span class="o">.</span><span class="n">partition_graph</span><span class="p">(</span><span class="n">graph</span><span class="p">,</span> <span class="n">graph_name</span><span class="o">=</span><span class="s1">&#39;ogbn-products&#39;</span><span class="p">,</span> <span class="n">num_parts</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span>
                                <span class="n">out_path</span><span class="o">=</span><span class="s1">&#39;4part_data&#39;</span><span class="p">,</span>
                                <span class="n">balance_ntypes</span><span class="o">=</span><span class="n">graph</span><span class="o">.</span><span class="n">ndata</span><span class="p">[</span><span class="s1">&#39;train_mask&#39;</span><span class="p">],</span>
                                <span class="n">balance_edges</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                                <span class="n">use_graphbolt</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="Enable-GraphBolt-sampling-in-the-training-script">
<h3>Enable <code class="docutils literal notranslate"><span class="pre">GraphBolt</span></code> sampling in the training script<a class="headerlink" href="#Enable-GraphBolt-sampling-in-the-training-script" title="Link to this heading"></a></h3>
<p>Just set the <code class="docutils literal notranslate"><span class="pre">use_graphbolt</span></code> flag to <code class="docutils literal notranslate"><span class="pre">True</span></code> in <code class="docutils literal notranslate"><span class="pre">dgl.distributed.initialize</span></code> function. This is the only change needed in the training script to enable <code class="docutils literal notranslate"><span class="pre">GraphBolt</span></code> sampling.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">dgl</span><span class="o">.</span><span class="n">distributed</span><span class="o">.</span><span class="n">initialize</span><span class="p">(</span><span class="s1">&#39;ip_config.txt&#39;</span><span class="p">,</span> <span class="n">use_graphbolt</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
</section>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="index.html" class="btn btn-neutral float-left" title="Distributed training" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="2_link_prediction.html" class="btn btn-neutral float-right" title="Distributed Link Prediction" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2018, DGL Team.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>