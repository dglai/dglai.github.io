{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# CPU Best Practices\n",
    "\n",
    "This chapter focus on providing best practises for environment setup\n",
    "to get the best performance during training and inference on the CPU.\n",
    "\n",
    "## Intel\n",
    "\n",
    "### Hyper-threading\n",
    "\n",
    "For specific workloads as GNN’s domain, suggested default setting for having best performance\n",
    "is to turn off hyperthreading.\n",
    "Turning off the hyper threading feature can be done at BIOS [#f1]_ or operating system level [#f2]_ [#f3]_ .\n",
    "\n",
    "### Alternative memory allocators\n",
    "\n",
    "Alternative memory allocators, such as *tcmalloc*, might provide significant performance improvements by more efficient memory usage, reducing overhead on unnecessary memory allocations or deallocations. *tcmalloc* uses thread-local caches to reduce overhead on thread synchronization, locks contention by using spinlocks and per-thread arenas respectively and categorizes memory allocations by sizes to reduce overhead on memory fragmentation.\n",
    "\n",
    "To take advantage of optimizations *tcmalloc* provides, install it on your system (on Ubuntu *tcmalloc* is included in libgoogle-perftools4 package) and add shared library to the LD_PRELOAD environment variable:\n",
    "\n",
    "```shell\n",
    "export LD_PRELOAD=/lib/x86_64-linux-gnu/libtcmalloc.so.4:$LD_PRELOAD\n",
    "```\n",
    "### OpenMP settings\n",
    "\n",
    "As `OpenMP` is the default parallel backend, we could control performance\n",
    "including sampling and training via `dgl.utils.set_num_threads()`.\n",
    "\n",
    "If number of OpenMP threads is not set and `num_workers` in dataloader is set\n",
    "to 0, the OpenMP runtime typically use the number of available CPU cores by\n",
    "default. This works well for most cases, and is also the default behavior in DGL.\n",
    "\n",
    "If `num_workers` in dataloader is set to greater than 0, the number of\n",
    "OpenMP threads will be set to **1** for each worker process. This is the\n",
    "default behavior in PyTorch. In this case, we can set the number of OpenMP\n",
    "threads to the number of CPU cores in the main process.\n",
    "\n",
    "Performance tuning is highly dependent on the workload and hardware\n",
    "configuration. We recommend users to try different settings and choose the\n",
    "best one for their own cases.\n",
    "\n",
    "**Dataloader CPU affinity**\n",
    "\n",
    "<div class=\"alert alert-info\"><h4>Note</h4><p>This feature is available for `dgl.dataloading.DataLoader` only. Not\n",
    "    available for dataloaders in `dgl.graphbolt` yet.</p></div>\n",
    "\n",
    "\n",
    "If number of dataloader workers is more than 0, please consider using **use_cpu_affinity()** method\n",
    "of DGL Dataloader class, it will generally result in significant performance improvement for training.\n",
    "\n",
    "*use_cpu_affinity* will set the proper OpenMP thread count (equal to the number of CPU cores allocated for main process),\n",
    "affinitize dataloader workers for separate CPU cores and restrict the main process to remaining cores\n",
    "\n",
    "In multiple NUMA nodes setups *use_cpu_affinity* will only use cores of NUMA node 0 by default\n",
    "with an assumption, that the workload is scaling poorly across multiple NUMA nodes. If you believe\n",
    "your workload will have better performance utilizing more than one NUMA node, you can pass\n",
    "the list of cores to use for dataloading (loader_cores) and for compute (compute_cores).\n",
    "\n",
    "loader_cores and compute_cores arguments (list of CPU cores) can be passed to *enable_cpu_affinity* for more\n",
    "control over which cores should be used, e.g. in case a workload scales well across multiple NUMA nodes.\n",
    "\n",
    "Usage:\n",
    "    .. code:: python\n",
    "\n",
    "        dataloader = dgl.dataloading.DataLoader(...)\n",
    "        ...\n",
    "        with dataloader.enable_cpu_affinity():\n",
    "            <training loop or inferencing>\n",
    "\n",
    "**Manual control**\n",
    "\n",
    "For advanced and more fine-grained control over OpenMP settings please refer to Maximize Performance of Intel® Optimization for PyTorch* on CPU [#f4]_ article\n",
    "\n",
    ".. rubric:: Footnotes\n",
    "\n",
    ".. [#f1] https://www.intel.com/content/www/us/en/support/articles/000007645/boards-and-kits/desktop-boards.html\n",
    ".. [#f2] https://aws.amazon.com/blogs/compute/disabling-intel-hyper-threading-technology-on-amazon-linux/\n",
    ".. [#f3] https://aws.amazon.com/blogs/compute/disabling-intel-hyper-threading-technology-on-amazon-ec2-windows-instances/\n",
    ".. [#f4] https://software.intel.com/content/www/us/en/develop/articles/how-to-get-better-performance-on-pytorchcaffe2-with-intel-acceleration.html\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
