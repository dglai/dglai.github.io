<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.5.2">Jekyll</generator><link href="https://www.dgl.ai/feed.xml" rel="self" type="application/atom+xml" /><link href="https://www.dgl.ai/" rel="alternate" type="text/html" /><updated>2022-09-20T07:46:45+00:00</updated><id>https://www.dgl.ai/feed.xml</id><title type="html">Deep Graph Library</title><subtitle>Easy Deep Learning on Graphs</subtitle><entry><title type="html">Accelerating Partitioning of Billion-scale Graphs with DGL v0.9.1</title><link href="https://www.dgl.ai/release/2022/09/19/release.html" rel="alternate" type="text/html" title="Accelerating Partitioning of Billion-scale Graphs with DGL v0.9.1" /><published>2022-09-19T00:00:00+00:00</published><updated>2022-09-19T00:00:00+00:00</updated><id>https://www.dgl.ai/release/2022/09/19/release</id><content type="html" xml:base="https://www.dgl.ai/release/2022/09/19/release.html">&lt;p&gt;Graphs is ubiquitous to represent relational data, and many real-world
applications such as recommendation and fraud detection involve learning from
massive graphs. As such, GNNs has emerged as a powerful family of models to
learn their representations. However, training GNNs on massive graphs is
challenging, one of the issues is high resource demand to distribute graph data
to a cluster. For example, partitioning a random graph of 1 billion nodes and 5
billion edges into 8 partitions requires a powerful AWS EC2 x1e.32xlarge
instance (128 vCPU, 3.9TB RAM) running for 10 hours to finish the job.&lt;/p&gt;

&lt;p&gt;In the latest DGL v0.9.1, we released a new pipeline for preprocess, partition
and dispatch graph of billions of nodes or edges for distributed GNN training.
At its core is a new data format called Chunked Graph Data Format (CGDF) which
stores graph data by chunks. The new pipeline processes data chunks in parallel
which not only reduces the memory requirement of each machine but also
significantly accelerates the entire procedure. For the same random graph with
1B nodes/5B edges, using a cluster of 8 AWS EC2 x1e.4xlarge (16 vCPU, 488GB RAM
each), &lt;strong&gt;the new pipeline can reduce the running time to 2.7 hours and cut down
the money cost by 3.7x.&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;In this blog, we will illustrate step-by-step how to partition and distribute a
graph of billions of nodes and edges using this new feature.&lt;/p&gt;

&lt;h2 id=&quot;distributed-gnn-training-101&quot;&gt;Distributed GNN Training 101&lt;/h2&gt;

&lt;p&gt;A graph dataset typically consists of graph structure and the features
associated with nodes/edges. If the graph is heterogeneous (i.e., having
multiple types of nodes or edges), different types of nodes/edges may have
different sets of features. Training a GNN model on a multi-machine cluster
first requires users to partition their input graph, which involves two steps:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Run a graph partition algorithm (e.g., random, METIS) to assign each node to
one partition.&lt;/li&gt;
  &lt;li&gt;Shuffle and dispatch the graph structure and node/edge features to the
target machine that owns the partition.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Once the graph is partitioned and provisioned, users can then launch the
distributed training program using DGL’s launch tool, which will:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Launch one main graph server per machine that loads the local graph
partition into RAM. Graph servers provide remove process calls (RPCs) to
conduct computation like graph sampling. Optionally, users can launch more
backup graph servers that share the in-memory data of the main graph server to
increase service throughput.&lt;/li&gt;
  &lt;li&gt;Launch a key-value store (KVStore) server per machine that loads the local
node/edge features into RAM. KVStore service provides RPCs to fetch/update
node/edge features.&lt;/li&gt;
  &lt;li&gt;Launch one or more trainer processes per machine. Trainer processes are
connected with each other via PyTorch’s &lt;a href=&quot;https://pytorch.org/tutorials/intermediate/ddp_tutorial.html&quot;&gt;DistributedDataParallel&lt;/a&gt;
component. At each training iteration, they issue requests to local or remote graph servers
and KVStore servers to get a mini-batch of samples, perform gradient descent
and synchronize their gradients before the next iteration.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;The figure below depicts the system architecture. For more information, check
out the &lt;a href=&quot;https://docs.dgl.ai/en/0.9.x/guide/distributed.html&quot;&gt;Distributed Training chapter&lt;/a&gt; of DGL User Guide.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/posts/2022-09-19-release/dist_train.png&quot; alt=&quot;dist_train&quot; width=&quot;800x&quot; class=&quot;aligncenter&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Due to the complexity of graph data, graph partitioning is typically run on a
single machine, which demands the machine to have large enough memory to fit
the entire graph and features as well as the runtime usage from the partition
algorithm. For example, a random graph of 1 billion nodes and 5 billions edges
and 50 features per nodes needs 268GB when stored in DGL graph format. Using
the existing &lt;code class=&quot;highlighter-rouge&quot;&gt;dgl.distributed.partition_graph&lt;/code&gt; API to partition this graph
requires a powerful AWS EC2 x1e.32xlarge instance (128 vCPU, 3.9TB RAM) and
runs for 10 hours — a significant bottleneck for users to train GNNs at scale.&lt;/p&gt;

&lt;p&gt;DGL v0.9.1 addressed the issue by a new distributed graph partitioning
pipeline. Specifically,&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;We designed a Chunked Graph Data Format (CGDF) for storing large graph data
in chunks to avoid loading the entire graph into a single machine.&lt;/li&gt;
  &lt;li&gt;We provided scripts to partition and dispatch chunked graph in parallel using
multiple machines to reduce the memory requirement of each machine as well as
to accelerate the procedure.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;chunked-graph-data-format&quot;&gt;Chunked Graph Data Format&lt;/h2&gt;

&lt;p&gt;Chunked graph dataset is organized as a data folder with the following data
files:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;A &lt;code class=&quot;highlighter-rouge&quot;&gt;metadata.json&lt;/code&gt; file that stores the meta information of the graph, e.g.,
graph name, node/edge types, chunk sizes, chunk file paths, etc.&lt;/li&gt;
  &lt;li&gt;A list of &lt;em&gt;edge index chunk files&lt;/em&gt; that store the source and destination node
IDs. They are typically in plain texts.&lt;/li&gt;
  &lt;li&gt;A list of &lt;em&gt;node data chunk files&lt;/em&gt;. They are typically array data stored in
&lt;a href=&quot;https://numpy.org/doc/stable/reference/generated/numpy.save.html#numpy.save&quot;&gt;NumPy array binary&lt;/a&gt;.&lt;/li&gt;
  &lt;li&gt;A list of &lt;em&gt;edge data chunk files&lt;/em&gt;. They are also in NumPy array binary.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Here, we illustrate the folder structure and the data files of a random social
graph (nodes being users, edges being follow relation) where nodes have two
data: “feat” and “label”. Check out the &lt;a href=&quot;https://docs.dgl.ai/en/0.9.x/guide/distributed-preprocessing.html#chunked-graph-format&quot;&gt;doc page&lt;/a&gt;
for the full specification of the format and tips for how to convert your data
to chunks.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;//data/random_graph_chunked/
  |-- metadata.json            # metadata JSON
  |-- edge_index/              # edge index chunks
    |-- user:follow:user0.txt  # user-follow-user edges chunk 0
    |-- user:follow:user1.txt  # user-follow-user edges chunk 1
    |-- user:follow:user2.txt  # user-follow-user edges chunk 2
    |-- ...
  |-- node_data/           # node data chunks
    |-- user/              # user nodes have two data: &quot;feat&quot; and &quot;label&quot;
      |-- feat0.npy        # feat chunk 0
      |-- feat1.npy        # feat chunk 1
      |-- feat2.npy        # feat chunk 2
      |-- ...
      |-- label0.npy       # label chunk 0
      |-- label1.npy       # label chunk 1
      |-- label2.npy       # label chunk 2
      |-- ...
  |-- edge_data/           # edge data chunks
    |-- user:follow:user/
       |-- ...
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;h2 id=&quot;running-distributed-partitioning--dispatching&quot;&gt;Running Distributed Partitioning &amp;amp; Dispatching&lt;/h2&gt;

&lt;p&gt;The first step is to get a cluster of machines to partition the graph. We
recommend the total RAM size of the cluster to be 2-3x larger than the graph
data size to accommodate the runtime memory needed. Next is to setup shared
workspace and software environment.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Setup a shared folder that is accessible by each instance in the cluster
(e.g., using &lt;a href=&quot;https://wiki.archlinux.org/title/NFS&quot;&gt;NFS&lt;/a&gt;). Make sure all
instances can ssh to each other. Here, we suppose the folder is mounted to
&lt;code class=&quot;highlighter-rouge&quot;&gt;/workspace&lt;/code&gt;.&lt;/li&gt;
  &lt;li&gt;Clone and download the scripts from DGL 0.9.x branch to &lt;code class=&quot;highlighter-rouge&quot;&gt;/workspace&lt;/code&gt;:
    &lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;git clone https://github.com/dmlc/dgl.git -b 0.9.x /workspace/dgl
&lt;/code&gt;&lt;/pre&gt;
    &lt;/div&gt;
  &lt;/li&gt;
  &lt;li&gt;Copy/move the chunked graph data to &lt;code class=&quot;highlighter-rouge&quot;&gt;/workspace/&lt;/code&gt;. Here, we suppose the data
folder is at &lt;code class=&quot;highlighter-rouge&quot;&gt;/workspace/random_graph_chunked/&lt;/code&gt;.&lt;/li&gt;
  &lt;li&gt;Create an &lt;code class=&quot;highlighter-rouge&quot;&gt;/workspace/ip_config.txt&lt;/code&gt; file that contains the IP address of each
instance.
    &lt;div class=&quot;language-yaml highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c1&quot;&gt;# example IP config file of a 4 machine cluster&lt;/span&gt;
&lt;span class=&quot;s&quot;&gt;172.31.19.1&lt;/span&gt;
&lt;span class=&quot;s&quot;&gt;172.31.23.205&lt;/span&gt;
&lt;span class=&quot;s&quot;&gt;172.31.29.175&lt;/span&gt;
&lt;span class=&quot;s&quot;&gt;172.31.16.98&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
    &lt;/div&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;We can then run a partition algorithm to assign each node to a partition. Here,
we choose random partitioning algorithm.&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;python /workspace/dgl/tools/partition_algo/random_partition.py &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
    --in_dir&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;/workspace/random_graph_chunked/
    --out_dir&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;/workspace/partition_assign/
    --num_partitions&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;4
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;The above script simply calculates which partition a node belongs to. We then
pass both the chunked graph and the partition assignments to the
&lt;code class=&quot;highlighter-rouge&quot;&gt;dispatch_data.py&lt;/code&gt; script to physically split the graph data into multiple pieces
and distribute them to the entire cluster.&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;python /workspace/dgl/tools/dispatch_data.py &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
    --in-dir&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;/workspace/random_graph_chunked/
    --partitions-dir&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;/workspace/partition_assign/
    --out-dir&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;/workspace/random_graph_dist/
    --ip-config&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;/workspace/ip_config.txt
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;The end result will look like the following. We can then launch distributed
training following the instructions &lt;a href=&quot;https://docs.dgl.ai/en/0.9.x/guide/distributed-tools.html&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;/workspace/random_graph_dist/
  |-- medatdata.json      # metadata JSON file
  |-- part0/              # partition 0
    |-- graph.dgl         # graph structure of partition 0 in DGL binary format
    |-- node_feat.dgl     # node feature of partition 0 in DGL binary format
    |-- edge_feat.dgl     # edge feature of partition 0 in DGL binary format
  |-- part1/              # partition 1
    |-- graph.dgl         # graph structure of partition 1 in DGL binary format
    |-- node_feat.dgl     # node feature of partition 1 in DGL binary format
    |-- edge_feat.dgl     # edge feature of partition 1 in DGL binary format
  |-- part2/
  ...
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;Note that the scripts utilize multiple machines to cooperatively partition and
process the data. Therefore, the new pipeline is significantly faster. For the
same random graph with 1B nodes/5B edges, the new pipeline can finish
partitioning in 2.7 hours (3.7x faster) using an cluster of 8 AWS EC2
x1e.4xlarge (16 vCPU, 488GB RAM).&lt;/p&gt;

&lt;h2 id=&quot;further-readings&quot;&gt;Further Readings&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://docs.dgl.ai/en/0.9.x/guide/distributed-preprocessing.html&quot;&gt;User guide for the new distributed graph partitioning pipeline&lt;/a&gt;.&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/dmlc/dgl/tree/0.9.x/examples/pytorch/graphsage/dist&quot;&gt;Example for training a GraphSAGE model on a cluster of machines&lt;/a&gt;.&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/dmlc/dgl/releases/tag/0.9.1&quot;&gt;Other enhancement in the latest v0.9.1 release&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;</content><author><name>DGLTeam</name></author><category term="release" /><category term="release" /><summary type="html">Graphs is ubiquitous to represent relational data, and many real-world applications such as recommendation and fraud detection involve learning from massive graphs. As such, GNNs has emerged as a powerful family of models to learn their representations. However, training GNNs on massive graphs is challenging, one of the issues is high resource demand to distribute graph data to a cluster. For example, partitioning a random graph of 1 billion nodes and 5 billion edges into 8 partitions requires a powerful AWS EC2 x1e.32xlarge instance (128 vCPU, 3.9TB RAM) running for 10 hours to finish the job.</summary></entry><entry><title type="html">v0.9 Release Highlights</title><link href="https://www.dgl.ai/release/2022/07/25/release.html" rel="alternate" type="text/html" title="v0.9 Release Highlights" /><published>2022-07-25T00:00:00+00:00</published><updated>2022-07-25T00:00:00+00:00</updated><id>https://www.dgl.ai/release/2022/07/25/release</id><content type="html" xml:base="https://www.dgl.ai/release/2022/07/25/release.html">&lt;p&gt;Six years after the first Graph Convolutional Networks paper, researchers are
actively investigating more advanced GNN architecture or training methodology.
As the developer team of DGL, we closely watch those new research trends and
release features to facilitate them. Here, we highlighted some of the new
functionalities of the recent v0.9 release.&lt;/p&gt;

&lt;h2 id=&quot;combining-graph-analytics-with-gnns-using-cugraphdgl&quot;&gt;Combining Graph Analytics with GNNs using cuGraph+DGL&lt;/h2&gt;

&lt;p&gt;Graph neural networks (GNNs) are capable of combining the feature and
structural information of graph data. Its power can be further extended when
synergistically combined with techniques of graph analytics, such as feature
augmentation.&lt;/p&gt;

&lt;p&gt;Graph analytics has been widely used for characterising graph structures, e.g.,
identifying important nodes, leading to interesting feature augmentation
methods. To exploit the synergy, we would want a fast and scalable graph
analytics engine. NVidia’s &lt;a href=&quot;https://github.com/rapidsai/cugraph&quot;&gt;RAPIDS cuGraph
library&lt;/a&gt; provides a collection of GPU
accelerated algorithms for graph analytics, such as centrality computation and
community detection. According to this
&lt;a href=&quot;https://docs.rapids.ai/api/cugraph/stable/basics/cugraph_intro.html&quot;&gt;documentation&lt;/a&gt;,
&lt;em&gt;“the latest NVIDIA GPUs (RAPIDS supports Pascal and later GPU architectures)
make graph analytics 1000x faster on average over NetworkX”&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;With collaboration with NVidia’s engineers, DGL v0.9 now allows conversion
between a DGLGraph object and a cuGraph graph object with two APIs &lt;code class=&quot;highlighter-rouge&quot;&gt;to_cugraph&lt;/code&gt;
and &lt;code class=&quot;highlighter-rouge&quot;&gt;from_cugraph&lt;/code&gt;, making it possible for DGL users to access efficient graph
analytics implementations in cuGraph.&lt;/p&gt;

&lt;h3 id=&quot;installation&quot;&gt;Installation&lt;/h3&gt;

&lt;p&gt;To install cuGraph with PyTorch and DGL, we recommend following the practice
below. Mamba is a multi-threaded version of conda.&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;conda install mamba -n base -c conda-forge

mamba create -n dgl_and_cugraph -c dglteam -c rapidsai-nightly -c nvidia -c pytorch -c conda-forge &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
    cugraph pytorch torchvision torchaudio &lt;span class=&quot;nv&quot;&gt;cudatoolkit&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;11.3 dgl-cuda11.3 tqdm

conda activate dgl_and_cugraph
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;h3 id=&quot;feature-initialization-via-cugraph&quot;&gt;Feature Initialization via cuGraph&lt;/h3&gt;

&lt;p&gt;We showcase an example of node feature initialization using the graph analytics
algorithms provided by cuGraph. Here, we consider two options:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://docs.rapids.ai/api/cugraph/stable/api_docs/api/cugraph.louvain.html&quot;&gt;Louvain algorithm&lt;/a&gt; that detects the community membership of each node based on
modularity optimization.&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://docs.rapids.ai/api/cugraph/stable/api_docs/api/cugraph.core_number.html#cugraph.core_number&quot;&gt;Core number algorithm&lt;/a&gt; that calculates the maximal &lt;a href=&quot;https://en.wikipedia.org/wiki/Degeneracy_(graph_theory)&quot;&gt;k-core&lt;/a&gt; subgraph each node
belongs to. A k-core of a graph is a maximal subgraph that contains nodes of
degree k or more.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The two algorithms capture different structural characteristics of a node.
Louvain groups nodes with close spatial distance with each other, while nodes
with the same core numbers are more structurally similar with each other. The
figures below illustrate the node coloring produced by Louvain communities and
core numbers on &lt;a href=&quot;https://en.wikipedia.org/wiki/Zachary%27s_karate_club&quot;&gt;Zachary’s Karate Club Network&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/posts/2022-07-25-release/node_coloring.png&quot; alt=&quot;node_coloring&quot; width=&quot;800x&quot; class=&quot;aligncenter&quot; /&gt;&lt;/p&gt;

&lt;p&gt;cuGraph offers efficient GPU implementations of these two algorithms. To call
them, we convert a &lt;code class=&quot;highlighter-rouge&quot;&gt;dgl.DGLGraph&lt;/code&gt; to a &lt;code class=&quot;highlighter-rouge&quot;&gt;cugraph.Graph&lt;/code&gt; using the &lt;code class=&quot;highlighter-rouge&quot;&gt;to_cugraph&lt;/code&gt;
API.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;cugraph&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;torch&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;louvain&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dgl_g&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;cugraph_g&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dgl_g&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;to_cugraph&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;to_undirected&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;df&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;_&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;cugraph&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;louvain&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cugraph_g&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;resolution&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;c&quot;&gt;# revert the node ID renumbering by cugraph&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;df&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;cugraph_g&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;unrenumber&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;df&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'vertex'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sort_values&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'vertex'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;utils&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dlpack&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;from_dlpack&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;df&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'partition'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;to_dlpack&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;())&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;long&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;core_number&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dgl_g&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;cugraph_g&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dgl_g&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;to_cugraph&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;to_undirected&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;df&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;cugraph&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;core_number&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cugraph_g&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;c&quot;&gt;# revert the node ID renumbering by cugraph&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;df&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;cugraph_g&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;unrenumber&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;df&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'vertex'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sort_values&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'vertex'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;utils&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dlpack&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;from_dlpack&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;df&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'core_number'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;to_dlpack&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;())&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;long&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;h3 id=&quot;training-gnn-via-dgl&quot;&gt;Training GNN via DGL&lt;/h3&gt;

&lt;p&gt;We then use the above functions to prepare node features for the ogbn-arxiv
dataset. Note that since both algorithms calculate structural categories, we
convert them to one-hot encoding and concatenate them as the initial node
features.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;dgl.transforms&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;T&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;torch.nn&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;nn&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;torch.nn.functional&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;F&lt;/span&gt;

&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;dgl.nn&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;SAGEConv&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;ogb.nodeproppred&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;DglNodePropPredDataset&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Evaluator&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;device&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;device&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'cuda'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;dataset&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;DglNodePropPredDataset&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'ogbn-arxiv'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;g&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;label&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dataset&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;transform&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;T&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Compose&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;T&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;AddReverse&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(),&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;T&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;AddSelfLoop&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(),&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;T&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ToSimple&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;g&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;transform&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;g&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;int&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;to&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;device&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;feat1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;louvain&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;g&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;feat2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;core_number&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;g&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# convert to one-hot&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;feat1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;F&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;one_hot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;feat1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;feat1&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;max&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;feat2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;F&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;one_hot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;feat2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;feat2&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;max&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# concat feat1 and feat2&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cat&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;feat1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;feat2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dim&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;float&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;We then train a simple three layer GraphSAGE model (see complete training code
&lt;a href=&quot;https://gist.github.com/jermainewang/f316587084f73e7dd060fe203417f42a&quot;&gt;here&lt;/a&gt;).
With the help of node features initialized by graph analytics algorithms, we
are able to achieve an accuracy of about 0.6 on the test set using pure
structural information, which even outperforms an MLP model using the original
input node features. With the new DGL release, we are looking forward to seeing
more innovation on GNNs combined with graph analytics.&lt;/p&gt;

&lt;h2 id=&quot;fp16--mixed-precision-support&quot;&gt;FP16 &amp;amp; Mixed Precision Support&lt;/h2&gt;

&lt;p&gt;DGL v0.9 is now fully compatible with the &lt;a href=&quot;https://pytorch.org/docs/stable/amp.html&quot;&gt;PyTorch Automatic Mixed Precision
(AMP) package&lt;/a&gt; for mixed precision training, thus saving both training time and
GPU memory consumption.&lt;/p&gt;

&lt;p&gt;By wrapping the forward pass with torch.cuda.amp.autocast(), PyTorch
automatically selects the appropriate data type for each op and tensor. Half
precision tensors are memory efficient, most operators on half precision
tensors are faster as they leverage GPU tensorcores.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;torch.nn.functional&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;F&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;torch.cuda.amp&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;autocast&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;forward&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;g&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;feat&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;label&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;mask&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;with&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;autocast&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;enabled&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;logit&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;g&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;feat&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;loss&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;F&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cross_entropy&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;logit&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mask&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;label&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mask&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;loss&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;Small gradients in &lt;code class=&quot;highlighter-rouge&quot;&gt;float16&lt;/code&gt; format have underflow problems (flush to zero).
PyTorch AMP provides a &lt;code class=&quot;highlighter-rouge&quot;&gt;GradScaler&lt;/code&gt; module to address this issue. It multiplies
the loss by a factor and invokes backward pass on the scaled loss to prevent
the underflow problem. It then unscales the computed gradients before the
optimizer updates the parameters. The scale factor is determined automatically.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;torch.cuda.amp&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;GradScaler&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;scaler&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;GradScaler&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;backward&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;scaler&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;loss&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;optimizer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;scaler&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;scale&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;loss&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;backward&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;scaler&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;step&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;optimizer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;scaler&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;update&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;Putting everything together, we have the example below.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;torch&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;torch.nn&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;nn&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;dgl.data&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;RedditDataset&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;dgl.nn&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;GATConv&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;dgl.transforms&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;AddSelfLoop&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;class&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;GAT&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Module&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;__init__&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;in_feats&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;num_classes&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;num_hidden&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;256&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;num_heads&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
        &lt;span class=&quot;nb&quot;&gt;super&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;__init__&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
        &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;conv1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;GATConv&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;in_feats&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;num_hidden&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;num_heads&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;activation&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;F&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;elu&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;conv2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;GATConv&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;num_hidden&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;num_heads&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;num_hidden&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;num_heads&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

    &lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;forward&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;g&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;h&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;h&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;conv1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;g&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;h&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;flatten&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;h&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;conv2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;g&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;h&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mean&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;h&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;device&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;device&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'cuda'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;transform&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;AddSelfLoop&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;data&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;RedditDataset&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;transform&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;g&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;g&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;g&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;int&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;to&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;device&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;train_mask&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;g&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ndata&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'train_mask'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;feat&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;g&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ndata&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'feat'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;label&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;g&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ndata&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'label'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;in_feats&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;feat&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;model&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;GAT&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;in_feats&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;num_classes&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;to&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;device&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;optimizer&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;optim&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Adam&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;parameters&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;lr&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;1e-3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;weight_decay&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;5e-4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;epoch&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;100&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;optimizer&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;zero_grad&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;loss&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;forward&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;g&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;feat&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;label&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;train_mask&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;backward&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;scaler&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;loss&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;optimizer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;Training GNNs using low precision or mixed precision is still an active
research topic. We hope the new v0.9 release will facilitate more research on
this topic. Check out the
&lt;a href=&quot;https://docs.dgl.ai/guide/mixed_precision.html&quot;&gt;documentation&lt;/a&gt; to know more.&lt;/p&gt;

&lt;h2 id=&quot;dgl-go-update-model-inference-and-graph-prediction&quot;&gt;DGL-Go Update: Model Inference and Graph Prediction&lt;/h2&gt;

&lt;p&gt;DGL-Go now supports training GNNs for graph property prediction tasks. It
includes two popular GNN models – Graph Isomorphism Network (GIN) and Principal
Neighborhood Aggregation (PNA). For example, to train a GIN model on the
ogbg-molpcba dataset, first generate a YAML configuration file using command:&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;dgl configure graphpred --data ogbg-molpcba --model gin
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;which generates the following configuration file. Users can then manually
adjust the configuration file.&lt;/p&gt;

&lt;div class=&quot;language-yaml highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;s&quot;&gt;version&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;0.0.2&lt;/span&gt;
&lt;span class=&quot;s&quot;&gt;pipeline_name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;graphpred&lt;/span&gt;
&lt;span class=&quot;s&quot;&gt;pipeline_mode&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;train&lt;/span&gt;
&lt;span class=&quot;s&quot;&gt;device&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;cpu&lt;/span&gt;                     &lt;span class=&quot;c1&quot;&gt;# Torch device name, e.g., cpu or cuda or cuda:0&lt;/span&gt;
&lt;span class=&quot;s&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;s&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;ogbg-molpcba&lt;/span&gt;
    &lt;span class=&quot;s&quot;&gt;split_ratio&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;                &lt;span class=&quot;c1&quot;&gt;# Ratio to generate data split, for example set to [0.8, 0.1, 0.1] for 80% train/10% val/10% test. Leave blank to use builtin split in original dataset&lt;/span&gt;
&lt;span class=&quot;s&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
     &lt;span class=&quot;s&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;gin&lt;/span&gt;
     &lt;span class=&quot;s&quot;&gt;embed_size&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;300&lt;/span&gt;            &lt;span class=&quot;c1&quot;&gt;# Embedding size&lt;/span&gt;
     &lt;span class=&quot;s&quot;&gt;num_layers&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;5&lt;/span&gt;              &lt;span class=&quot;c1&quot;&gt;# Number of layers&lt;/span&gt;
     &lt;span class=&quot;s&quot;&gt;dropout&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;0.5&lt;/span&gt;               &lt;span class=&quot;c1&quot;&gt;# Dropout rate&lt;/span&gt;
     &lt;span class=&quot;s&quot;&gt;virtual_node&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;false&lt;/span&gt;        &lt;span class=&quot;c1&quot;&gt;# Whether to use virtual node&lt;/span&gt;
&lt;span class=&quot;s&quot;&gt;general_pipeline&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;s&quot;&gt;num_runs&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;1&lt;/span&gt;                 &lt;span class=&quot;c1&quot;&gt;# Number of experiments to run&lt;/span&gt;
    &lt;span class=&quot;s&quot;&gt;train_batch_size&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;32&lt;/span&gt;        &lt;span class=&quot;c1&quot;&gt;# Graph batch size when training&lt;/span&gt;
    &lt;span class=&quot;s&quot;&gt;eval_batch_size&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;32&lt;/span&gt;         &lt;span class=&quot;c1&quot;&gt;# Graph batch size when evaluating&lt;/span&gt;
    &lt;span class=&quot;s&quot;&gt;num_workers&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;4&lt;/span&gt;              &lt;span class=&quot;c1&quot;&gt;# Number of workers for data loading&lt;/span&gt;
    &lt;span class=&quot;s&quot;&gt;optimizer&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;s&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;Adam&lt;/span&gt;
        &lt;span class=&quot;s&quot;&gt;lr&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;0.001&lt;/span&gt;
        &lt;span class=&quot;s&quot;&gt;weight_decay&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;0&lt;/span&gt;
    &lt;span class=&quot;s&quot;&gt;lr_scheduler&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;s&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;StepLR&lt;/span&gt;
        &lt;span class=&quot;s&quot;&gt;step_size&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;100&lt;/span&gt;
        &lt;span class=&quot;s&quot;&gt;gamma&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;1&lt;/span&gt;
    &lt;span class=&quot;s&quot;&gt;loss&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;BCEWithLogitsLoss&lt;/span&gt;
    &lt;span class=&quot;s&quot;&gt;metric&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;roc_auc_score&lt;/span&gt;
    &lt;span class=&quot;s&quot;&gt;num_epochs&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;100&lt;/span&gt;             &lt;span class=&quot;c1&quot;&gt;# Number of training epochs&lt;/span&gt;
    &lt;span class=&quot;s&quot;&gt;save_path&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;results&lt;/span&gt;          &lt;span class=&quot;c1&quot;&gt;# Directory to save the experiment results&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;Alternatively, users can fetch model recipes of pre-defined hyperparameters for
the original experiments.&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;dgl recipe get graphpred_pcba_gin.yaml
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;To launch training:&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;dgl train --cfg graphpred_ogbg-molpcba_gin.yaml
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;Another addition is a new command to conduct inference of a trained model on
some other dataset. For example, the following shows how to apply the GIN model
trained on ogbg-molpcba to ogbg-molhiv:&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c&quot;&gt;# Generate an inference configuration file from a saved experiment checkpoint&lt;/span&gt;
dgl configure-apply graphpred --data ogbg-molhiv --cpt results/run_0.pth

&lt;span class=&quot;c&quot;&gt;# Apply the trained model for inference&lt;/span&gt;
dgl apply --cfg apply_graphpred_ogbg-molhiv_pna.yaml
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;It will save the model prediction in a CSV file like below&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/posts/2022-07-25-release/csv_result.png&quot; alt=&quot;csv_result&quot; width=&quot;800x&quot; class=&quot;aligncenter&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;further-reading&quot;&gt;Further Reading&lt;/h2&gt;

&lt;p&gt;Full release note: &lt;a href=&quot;https://github.com/dmlc/dgl/releases/tag/0.9.0&quot;&gt;https://github.com/dmlc/dgl/releases/tag/0.9.0&lt;/a&gt;&lt;/p&gt;</content><author><name>DGLTeam</name></author><category term="release" /><category term="release" /><summary type="html">Six years after the first Graph Convolutional Networks paper, researchers are actively investigating more advanced GNN architecture or training methodology. As the developer team of DGL, we closely watch those new research trends and release features to facilitate them. Here, we highlighted some of the new functionalities of the recent v0.9 release.</summary></entry><entry><title type="html">May 2022 Update Note</title><link href="https://www.dgl.ai/release/2022/05/31/release.html" rel="alternate" type="text/html" title="May 2022 Update Note" /><published>2022-05-31T00:00:00+00:00</published><updated>2022-05-31T00:00:00+00:00</updated><id>https://www.dgl.ai/release/2022/05/31/release</id><content type="html" xml:base="https://www.dgl.ai/release/2022/05/31/release.html">&lt;h2 id=&quot;synthetic-datasets-for-developing-gnn-explainability-approaches&quot;&gt;Synthetic Datasets for Developing GNN Explainability Approaches&lt;/h2&gt;

&lt;p&gt;We added the following new datasets. &lt;code class=&quot;highlighter-rouge&quot;&gt;BAShapeDataset&lt;/code&gt;, &lt;code class=&quot;highlighter-rouge&quot;&gt;BACommunityDataset&lt;/code&gt;, &lt;code class=&quot;highlighter-rouge&quot;&gt;TreeCycleDataset&lt;/code&gt;, and &lt;code class=&quot;highlighter-rouge&quot;&gt;TreeGridDataset&lt;/code&gt; were first introduced in &lt;a href=&quot;https://arxiv.org/abs/1903.03894v4&quot;&gt;GNNExplainer: Generating Explanations for Graph Neural Networks&lt;/a&gt; for node classification. &lt;code class=&quot;highlighter-rouge&quot;&gt;BA2MotifDataset&lt;/code&gt; was first introduced in &lt;a href=&quot;https://arxiv.org/abs/2011.04573&quot;&gt;Parameterized Explainer for Graph Neural Network&lt;/a&gt; for graph classification.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/posts/2022-05-31-release/syn_data.png&quot; alt=&quot;syn_data&quot; width=&quot;800x&quot; class=&quot;aligncenter&quot; /&gt;&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c&quot;&gt;# A dataset for node classification&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;dgl.data&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;BAShapeDataset&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;dataset&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;BAShapeDataset&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;num_classes&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dataset&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;num_classes&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;graph&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dataset&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;feat&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;graph&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ndata&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'feat'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;label&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;graph&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ndata&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'label'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# A dataset for graph classification&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;dgl.data&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;BA2MotifDataset&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;dataset&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;BA2MotifDataset&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;num_classes&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dataset&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;num_classes&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;graph&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;label&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dataset&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;           &lt;span class=&quot;c&quot;&gt;# Get the first graph data&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;feat&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;graph&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ndata&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'feat'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;blockquote&gt;
  &lt;p&gt;&lt;strong&gt;&lt;em&gt;Developer Recommendation:&lt;/em&gt;&lt;/strong&gt; &lt;em&gt;These synthetic graphs integrate specially designed substructures, motifs, into traditional random graph models, and assign labels based on their existence. Therefore, those substructures act as ground truth explanations for the node/graph labels, making them commonly used benchmarks for evaluating GNN explainability approaches.&lt;/em&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h2 id=&quot;sign-diffusion-transform&quot;&gt;SIGN Diffusion Transform&lt;/h2&gt;

&lt;p&gt;We added a new data transform module &lt;code class=&quot;highlighter-rouge&quot;&gt;SIGNDiffusion&lt;/code&gt; first introduced in &lt;a href=&quot;https://arxiv.org/abs/2004.11198&quot;&gt;SIGN: Scalable Inception Graph Neural Networks&lt;/a&gt;, which diffuses node features for later use. It supports four built-in diffusion matrices, including raw adjacency matrix, random walk adjacency matrix, symmetrically normalized adjacency matrix, and personalized PageRank matrix.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;dgl&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;dgl.transforms&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;T&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;dataset&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dgl&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;CoraGraphDataset&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;transform&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;T&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;SIGNDiffusion&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;k&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;# Diffuse for 1 &amp;amp; 2 hops&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;g&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dataset&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;  &lt;span class=&quot;c&quot;&gt;# diffused node features will be generated as ndata&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;feat1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;g&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ndata&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'out_feat_1'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;       &lt;span class=&quot;c&quot;&gt;# feature diffused for 1 hop&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;feat2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;g&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ndata&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'out_feat_2'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;       &lt;span class=&quot;c&quot;&gt;# feature diffused for 2 hops&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;blockquote&gt;
  &lt;p&gt;&lt;strong&gt;&lt;em&gt;Developer Recommendation:&lt;/em&gt;&lt;/strong&gt; &lt;em&gt;The ability to learn to aggregate neighbor information is one of the key innovation of Message Passing Neural Networks, which also brings scalability challenges due to the exponentially growing receptive field with more hops of neighbors to explore. SIGN proposed a cheap yet efficient solution that decouples model depth and receptive field size by diffusing input node features using various kinds of algorithms. Because the diffusion process is not trainable, we package it as a data transform module so that users can easily plug-in SIGN before running their own model.&lt;/em&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h2 id=&quot;label-propagation&quot;&gt;Label Propagation&lt;/h2&gt;

&lt;p&gt;We added a new NN module &lt;code class=&quot;highlighter-rouge&quot;&gt;LabelPropagation&lt;/code&gt; first introduced in &lt;a href=&quot;http://mlg.eng.cam.ac.uk/zoubin/papers/CMU-CALD-02-107.pdf&quot;&gt;Learning from Labeled and Unlabeled Data with Label Propagation&lt;/a&gt;, which propagates node labels over a graph for inferring the labels of unlabeled nodes.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;dgl&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;lp&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dgl&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;LabelPropagation&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;k&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;alpha&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.9&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;dataset&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dgl&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;CoraGraphDataset&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;g&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dataset&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;labels&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;g&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ndata&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'label'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;train_mask&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;g&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ndata&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'train_mask'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;new_labels&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;lp&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;g&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;labels&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;train_mask&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;blockquote&gt;
  &lt;p&gt;&lt;strong&gt;&lt;em&gt;Developer Recommendation:&lt;/em&gt;&lt;/strong&gt; &lt;em&gt;The classical label propagation is a simple (non-parametric) yet effective algorithm, making it a strong baseline for many node classification datasets.&lt;/em&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h2 id=&quot;directional-graph-network-layer&quot;&gt;Directional Graph Network Layer&lt;/h2&gt;

&lt;p&gt;We added a new NN module &lt;code class=&quot;highlighter-rouge&quot;&gt;DGNConv&lt;/code&gt; first introduced in &lt;a href=&quot;https://arxiv.org/abs/2010.02863&quot;&gt;Directional Graph Networks&lt;/a&gt;, which introduces directional aggregators in message passing based on the gradient of low-frequency eigenvectors of the graph Laplacian matrix.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;dgl&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;torch&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;g&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;...&lt;/span&gt;  &lt;span class=&quot;c&quot;&gt;# some graph&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# Precompute 1 smallest non-trivial eigenvectors&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;transform&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dgl&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;transforms&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;LaplacianPE&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;k&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;g&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;transform&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;g&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;randn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;g&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;num_nodes&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(),&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;10&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;  &lt;span class=&quot;c&quot;&gt;# node features&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;eig&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;g&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ndata&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'PE'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;conv&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dgl&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;DGNConv&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;10&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;10&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                      &lt;span class=&quot;n&quot;&gt;aggregators&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'dir1-av'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'dir1-dx'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
                      &lt;span class=&quot;n&quot;&gt;scalers&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'identity'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'amplification'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
                      &lt;span class=&quot;n&quot;&gt;delta&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;2.5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;h&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;conv&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;g&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;eig_vec&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;eig&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;blockquote&gt;
  &lt;p&gt;&lt;strong&gt;&lt;em&gt;Developer Recommendation:&lt;/em&gt;&lt;/strong&gt; &lt;em&gt;Directional Graph Networks (DGN) allow defining graph convolutions according to topologically-derived directional flows. It is a state-of-the-art baseline for many graph classification tasks.&lt;/em&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h2 id=&quot;graph-isomorphism-network-layer-with-edge-features&quot;&gt;Graph Isomorphism Network Layer with Edge Features&lt;/h2&gt;

&lt;p&gt;We added a new NN module &lt;code class=&quot;highlighter-rouge&quot;&gt;GINEConv&lt;/code&gt; first introduced in &lt;a href=&quot;https://arxiv.org/abs/1905.12265&quot;&gt;Strategies for Pre-training Graph Neural Networks&lt;/a&gt;, which extends Graph Isomorphism Network (GIN) for handling edge features.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;dgl&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;torch&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;g&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;...&lt;/span&gt;  &lt;span class=&quot;c&quot;&gt;# some graph&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;xn&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;randn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;g&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;num_nodes&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(),&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;10&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;  &lt;span class=&quot;c&quot;&gt;# node features&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;xe&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;randn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;g&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;num_edges&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(),&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;10&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;  &lt;span class=&quot;c&quot;&gt;# edge features&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;conv&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dgl&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;GINEConv&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Linear&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;10&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;10&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;hn&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;conv&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;g&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;xn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;xe&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;blockquote&gt;
  &lt;p&gt;&lt;strong&gt;&lt;em&gt;Developer Recommendation:&lt;/em&gt;&lt;/strong&gt; &lt;em&gt;Graph Isomorphism Network with edge features has been an important baseline for many graph classification tasks such as OGB graph property datasets.&lt;/em&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h2 id=&quot;feature-masking&quot;&gt;Feature Masking&lt;/h2&gt;

&lt;p&gt;We added a new data transform module &lt;code class=&quot;highlighter-rouge&quot;&gt;FeatMask&lt;/code&gt; first introduced in &lt;a href=&quot;https://arxiv.org/abs/2010.13902&quot;&gt;Graph Contrastive Learning with Augmentations&lt;/a&gt;, which randomly masks columns of node/edge features.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;dgl&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;dgl.transforms&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;T&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;dataset&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dgl&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;CoraGraphDataset&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;transform&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;T&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;FeatMask&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;p&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;node_feat_names&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'feat'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]))&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;g&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dataset&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;feat&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;g&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ndata&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'feat'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;# The node feature tensor has been randomly masked.&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;blockquote&gt;
  &lt;p&gt;&lt;strong&gt;&lt;em&gt;Developer Recommendation:&lt;/em&gt;&lt;/strong&gt; &lt;em&gt;Randomly masking columns of features is a simple yet useful data augmentation for graph contrastive learning.&lt;/em&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h2 id=&quot;row-normalizer-of-features&quot;&gt;Row-Normalizer of Features&lt;/h2&gt;

&lt;p&gt;We added a new data transform module &lt;code class=&quot;highlighter-rouge&quot;&gt;RowFeatNormalizer&lt;/code&gt;, which performs row-normalization of features.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;dgl&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;dgl.transforms&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;T&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;dataset&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dgl&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;CoraGraphDataset&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;transform&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;T&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;RowFeatNormalizer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;node_feat_names&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'feat'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]))&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;g&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dataset&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;feat&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;g&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ndata&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'feat'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;# The node feature tensor has been row-normalized.&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;blockquote&gt;
  &lt;p&gt;&lt;strong&gt;&lt;em&gt;Developer Recommendation:&lt;/em&gt;&lt;/strong&gt; &lt;em&gt;Row-normalization of raw features is a useful data pre-processing step.&lt;/em&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;For further readings, check out the &lt;a href=&quot;https://github.com/dmlc/dgl/releases/tag/0.8.2&quot;&gt;release note&lt;/a&gt; for a complete list of new additions, improvements and bug fixes. If you have questions about DGL or GNN in general, welcome to join our &lt;a href=&quot;https://deep-graph-library.slack.com/join/shared_invite/zt-eb4ict1g-xcg3PhZAFAB8p6dtKuP6xQ&quot;&gt;Slack channel&lt;/a&gt;. If you have specific requests on what should be included in DGL next, you can submit them on our Github or fill in this &lt;a href=&quot;https://forms.gle/tgKXFuUiGo9PeYBeA&quot;&gt;survey&lt;/a&gt;.&lt;/p&gt;</content><author><name>DGLTeam</name></author><category term="release" /><category term="release" /><summary type="html">Synthetic Datasets for Developing GNN Explainability Approaches</summary></entry><entry><title type="html">April 2022 Update Note</title><link href="https://www.dgl.ai/release/2022/04/18/release.html" rel="alternate" type="text/html" title="April 2022 Update Note" /><published>2022-04-18T00:00:00+00:00</published><updated>2022-04-18T00:00:00+00:00</updated><id>https://www.dgl.ai/release/2022/04/18/release</id><content type="html" xml:base="https://www.dgl.ai/release/2022/04/18/release.html">&lt;h2 id=&quot;grouped-reversible-residual-connection-for-gnns&quot;&gt;Grouped Reversible Residual Connection for GNNs&lt;/h2&gt;

&lt;p&gt;We added a new module &lt;code class=&quot;highlighter-rouge&quot;&gt;GroupRevRes&lt;/code&gt; introduced in &lt;a href=&quot;https://arxiv.org/abs/2106.07476&quot;&gt;Training Graph Neural Networks with 1000 Layers&lt;/a&gt;. It can wrap any GNN module with grouped, reversible and residual connection (example code below).&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;dgl&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;torch&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;class&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;GNNLayer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Module&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;__init__&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;in_size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dropout&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
        &lt;span class=&quot;nb&quot;&gt;super&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;GNNLayer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;__init__&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
        &lt;span class=&quot;c&quot;&gt;# Use BatchNorm and dropout to prevent gradient vanishing&lt;/span&gt;
        &lt;span class=&quot;c&quot;&gt;# In particular if you use a large number of GNN layers&lt;/span&gt;
        &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;norm&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;BatchNorm1d&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;in_size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;conv&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dgl&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;GraphConv&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;in_size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;in_size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dropout&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Dropout&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dropout&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

    &lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;forward&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;g&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;norm&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dropout&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;conv&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;g&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;g&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;...&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;# some graph&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;randn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;g&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;num_nodes&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(),&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;32&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;reversible_conv&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dgl&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;GroupRevRes&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;GNNLayer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;32&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;//&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;     &lt;span class=&quot;c&quot;&gt;# 4 groups&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;reversible_conv&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;g&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;  &lt;span class=&quot;c&quot;&gt;# forward&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;blockquote&gt;
  &lt;p&gt;&lt;strong&gt;&lt;em&gt;Developer Recommendation:&lt;/em&gt;&lt;/strong&gt; &lt;em&gt;The &lt;code class=&quot;highlighter-rouge&quot;&gt;GroupRevRes&lt;/code&gt; module is reversible, meaning the backward propagation does not require storing forward activations. It can significantly reduce memory usage of GNNs, making it possible to train a very deep GNN with up to 1000 layers on a single commodity GPU.&lt;/em&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h2 id=&quot;laplacian-positional-encoding&quot;&gt;Laplacian Positional Encoding&lt;/h2&gt;

&lt;p&gt;We added a new data transform module &lt;code class=&quot;highlighter-rouge&quot;&gt;LaplacianPE&lt;/code&gt; first introduced in &lt;a href=&quot;https://arxiv.org/abs/2003.00982&quot;&gt;Benchmarking Graph Neural Networks&lt;/a&gt;. It computes Laplacian positional encoding for a graph. Besides data transform module, we also provide a functional API. See the example of usage below:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;dgl&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# data transform&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;dataset&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dgl&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;CoraGraphDataset&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;transform&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dgl&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;transforms&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;LaplacianPE&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;k&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;feat_name&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'PE'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;g&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dataset&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;  &lt;span class=&quot;c&quot;&gt;# positional encodings will be generated as an ndata&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;pe&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;g&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ndata&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'PE'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# functional API&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;pe&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dgl&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;laplacian_pe&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;g&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;k&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;blockquote&gt;
  &lt;p&gt;&lt;strong&gt;&lt;em&gt;Developer Recommendation:&lt;/em&gt;&lt;/strong&gt; &lt;em&gt;Laplacian positional encoding improves the expressive power of GNNs by using k-smallest non-trivial Laplacian eigenvectors as additional node features.&lt;/em&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h2 id=&quot;random-walk-positional-encoding&quot;&gt;Random Walk Positional Encoding&lt;/h2&gt;

&lt;p&gt;We added a new data transform module &lt;code class=&quot;highlighter-rouge&quot;&gt;RandomWalkPE&lt;/code&gt; introduced in &lt;a href=&quot;https://arxiv.org/abs/2110.07875&quot;&gt;Graph Neural Networks with Learnable Structural and Positional Representations&lt;/a&gt;. It computes random-walk-based positional encoding for a graph.  Besides data transform module, we also provide a functional API. See the example of usage below:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;dgl&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# data transform&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;dataset&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dgl&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;CoraGraphDataset&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;transform&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dgl&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;transforms&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;RandomWalkPE&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;k&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;feat_name&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'PE'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;g&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dataset&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;  &lt;span class=&quot;c&quot;&gt;# positional encodings will be generated automatically&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;pe&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;g&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ndata&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'PE'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# Functional API&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;pe&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dgl&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;random_walk_pe&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;g&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;k&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;# functional API&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;blockquote&gt;
  &lt;p&gt;&lt;strong&gt;&lt;em&gt;Developer Recommendation:&lt;/em&gt;&lt;/strong&gt; &lt;em&gt;Random walk positional encoding improves the expressive power of GNNs by using the landing probabilities of a node to itself in 1, 2, …, K steps as additional node features.&lt;/em&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h2 id=&quot;graphsaint-samplers&quot;&gt;GraphSAINT Samplers&lt;/h2&gt;

&lt;p&gt;We added a new sampler &lt;code class=&quot;highlighter-rouge&quot;&gt;SAINTSampler&lt;/code&gt; introduced in &lt;a href=&quot;https://arxiv.org/abs/1907.04931v4&quot;&gt;GraphSAINT: Graph Sampling Based Inductive Learning Method&lt;/a&gt;. &lt;code class=&quot;highlighter-rouge&quot;&gt;SAINTSampler&lt;/code&gt; provides three strategies to extract induced subgraphs from a graph — by randomly selected node sets, randomly selected edge sets or nodes reached by random walks. See an example of usage below:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;torch&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;dgl&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;dgl.dataloading&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;SAINTSampler&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;DataLoader&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;sampler&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;SAINTSampler&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;mode&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'node'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;                      &lt;span class=&quot;c&quot;&gt;# Can be 'node', 'edge' or 'walk'&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;budget&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;200&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;prefetch_ndata&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'feat'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'label'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;  &lt;span class=&quot;c&quot;&gt;# optionally, specify data to prefetch&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;data_index&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;arange&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1000&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;  &lt;span class=&quot;c&quot;&gt;# 1000 mini-batches&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;g&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dgl&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;CoraGraphDataset&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;dataloader&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;DataLoader&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;g&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;data_index&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sampler&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;num_workers&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sg&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dataloader&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sg&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;blockquote&gt;
  &lt;p&gt;&lt;strong&gt;&lt;em&gt;Developer Recommendation:&lt;/em&gt;&lt;/strong&gt; &lt;em&gt;GraphSAINT is one of the state-of-the-art sampling methods in the family of subgraph sampling. Compared with neighbor sampling (or node-wise sampling), subgraph sampling avoids the issue of exponential neighborhood expansion, thus saving data transmission cost and enabling mini-batch training of deeper GNNs.&lt;/em&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h2 id=&quot;en-equivariant-convolutional-layer&quot;&gt;E(n) Equivariant Convolutional Layer&lt;/h2&gt;

&lt;p&gt;We added a new GNN module &lt;code class=&quot;highlighter-rouge&quot;&gt;EGNNConv&lt;/code&gt; introduced in &lt;a href=&quot;https://arxiv.org/abs/2102.09844v3&quot;&gt;E(n) Equivariant Graph Neural Networks&lt;/a&gt;. It performs equivariant transformations on node embeddings and coordinate embeddings. See an example of usage below:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;dgl&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;torch&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;g&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;...&lt;/span&gt;  &lt;span class=&quot;c&quot;&gt;# some graph&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;h&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;randn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;g&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;num_nodes&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(),&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;10&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;   &lt;span class=&quot;c&quot;&gt;# node features&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;randn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;g&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;num_nodes&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(),&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;    &lt;span class=&quot;c&quot;&gt;# node coordinates&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;a&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;randn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;g&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;num_edges&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(),&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;    &lt;span class=&quot;c&quot;&gt;# edge features&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;conv&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dgl&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;EGNNConv&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;10&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;10&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;10&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;h&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;conv&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;g&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;h&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;a&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;blockquote&gt;
  &lt;p&gt;&lt;strong&gt;&lt;em&gt;Developer Recommendation:&lt;/em&gt;&lt;/strong&gt; &lt;em&gt;GNNs with the capability of equivariant transformations have wide application in real-world structure data that have coordinates (e.g., molecules, point clouds, etc.). EGNN simplified previous attempts and proposed a design that is equivariant to rotations, translations, reflections and permutations on N-dimensional coordinates while considering both node features and node coordinates.&lt;/em&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h2 id=&quot;principal-neighbourhood-aggregation-layer&quot;&gt;Principal Neighbourhood Aggregation Layer&lt;/h2&gt;

&lt;p&gt;We added a new GNN module &lt;code class=&quot;highlighter-rouge&quot;&gt;PNAConv&lt;/code&gt; introduced in &lt;a href=&quot;https://arxiv.org/abs/2004.05718&quot;&gt;Principal Neighbourhood Aggregation for Graph Nets&lt;/a&gt;. The code below shows an example of usage:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;dgl&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;torch&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;g&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;...&lt;/span&gt;  &lt;span class=&quot;c&quot;&gt;# some graph&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;randn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;g&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;num_nodes&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(),&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;10&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;  &lt;span class=&quot;c&quot;&gt;# node features&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;conv&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dgl&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;PNAConv&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;10&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;10&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                      &lt;span class=&quot;n&quot;&gt;aggregators&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'mean'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'max'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'sum'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
                      &lt;span class=&quot;n&quot;&gt;scalers&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'identity'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'amplification'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
                      &lt;span class=&quot;n&quot;&gt;delta&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;2.5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;h&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;conv&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;g&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;blockquote&gt;
  &lt;p&gt;&lt;strong&gt;&lt;em&gt;Developer Recommendation:&lt;/em&gt;&lt;/strong&gt; &lt;em&gt;Principal Neighbourhood Aggregation (PNA) improves the expressive power of a GNN by combining multiple aggregation functions with degree-scalars, thus making it a state-of-the-art baseline for many graph classification tasks.&lt;/em&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h2 id=&quot;survey&quot;&gt;Survey&lt;/h2&gt;

&lt;p&gt;If there are papers for which you want to have DGL implementations or you have other feedback and suggestions, you could fill in &lt;a href=&quot;https://forms.gle/tgKXFuUiGo9PeYBeA&quot;&gt;this survey&lt;/a&gt;.&lt;/p&gt;</content><author><name>DGLTeam</name></author><category term="release" /><category term="release" /><summary type="html">Grouped Reversible Residual Connection for GNNs</summary></entry><entry><title type="html">v0.8 Release Highlights</title><link href="https://www.dgl.ai/release/2022/03/01/release.html" rel="alternate" type="text/html" title="v0.8 Release Highlights" /><published>2022-03-01T00:00:00+00:00</published><updated>2022-03-01T00:00:00+00:00</updated><id>https://www.dgl.ai/release/2022/03/01/release</id><content type="html" xml:base="https://www.dgl.ai/release/2022/03/01/release.html">&lt;p&gt;We are excited to announce the release of DGL v0.8, which brings many new
features as well as improvement on system performance. The highlights are:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;A major update of the mini-batch sampling pipeline, better customizability,
more optimizations; &lt;strong&gt;3.9x&lt;/strong&gt; and &lt;strong&gt;1.5x&lt;/strong&gt; faster for supervised and unsupervised
GraphSAGE on OGBN-Products, with only one line of code change.&lt;/li&gt;
  &lt;li&gt;Significant acceleration and code simplification of popular heterogeneous
graph NN modules (Up to &lt;strong&gt;36x&lt;/strong&gt; for RGCN convolution and &lt;strong&gt;12x&lt;/strong&gt; for HGT
convolution). 11 new off-the-shelf NN modules for building models for link
prediction, heterogeneous graph learning and GNN explanation.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;GNNLens&lt;/strong&gt;: a DGL empowered tool to visualize and understand graph data using
GNN explanation models.&lt;/li&gt;
  &lt;li&gt;New functions to create, transform and augment graph datasets, making it
easier to conduct research on graph contrastive learning or repurposing a
graph for different tasks.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;DGL-Go&lt;/strong&gt;: a new GNN model training command line tool that utilizes a simple
interface so that users can quickly apply GNNs to their problems and
orchestrate experiments with state-of-the-art GNN models.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;mini-batch-sampling-pipeline-update&quot;&gt;Mini-batch Sampling Pipeline Update&lt;/h2&gt;

&lt;p&gt;In training Neural Networks, minibatch sampling has been used to both improve
model performance and enable scaling to large datasets.  Mini-batch training in
the context of GNNs on graphs introduces new complexities, which can be broken
down into four main steps:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Extract a subgraph from the original graph.&lt;/li&gt;
  &lt;li&gt;Perform transformations on the subgraph.&lt;/li&gt;
  &lt;li&gt;Fetch the node/edge features of the subgraph.&lt;/li&gt;
  &lt;li&gt;Pass the subgraph and its features as the input to your GNN model and update parameters.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Among them, steps 1-3 are unique to GNNs and are quite costly. In v0.7, we have
released the feature to speedup step 2 by transforming subgraphs on GPU, but
the other two may continue to be the bottleneck. In this release, we further
optimized the &lt;em&gt;entire&lt;/em&gt; pipeline to reach an even better performance. We then
briefly describe our technical solutions behind that.&lt;/p&gt;

&lt;p&gt;To speed up subgraph extraction, we utilized CUDA Unified Virtual Addressing(UVA).&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/posts/2022-03-01-release/uva.png&quot; alt=&quot;uva&quot; width=&quot;400x&quot; class=&quot;aligncenter&quot; /&gt;&lt;/p&gt;

&lt;p&gt;(Image courtesy: &lt;a href=&quot;https://developer.download.nvidia.cn/CUDA/training/cuda_webinars_GPUDirect_uva.pdf&quot;&gt;https://developer.download.nvidia.cn/CUDA/training/cuda_webinars_GPUDirect_uva.pdf&lt;/a&gt;)&lt;/p&gt;

&lt;p&gt;CUDA UVA allows users to create in-memory data beyond the size of GPU RAM
capacity while still harnessing GPU kernels for fast computation. Storing the
entire graph structure and its features in UVA enables efficient subgraph
extraction using GPU kernels, which is effective for training large-scale GNNs
[1][2]. In this release, users can turn on the UVA mode by setting the &lt;code class=&quot;highlighter-rouge&quot;&gt;use_uva&lt;/code&gt;
flag in &lt;code class=&quot;highlighter-rouge&quot;&gt;DataLoader&lt;/code&gt;, as shown in the example below:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;g&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;...&lt;/span&gt;                  &lt;span class=&quot;c&quot;&gt;# some DGLGraph data&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;train_nids&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;...&lt;/span&gt;         &lt;span class=&quot;c&quot;&gt;# training node IDs&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;sampler&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dgl&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dataloading&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;MultiLayerNeighborSampler&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;fanout&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;10&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;15&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;dataloader&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dgl&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dataloading&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;DataLoader&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;g&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;train_nids&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sampler&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;device&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'cuda:0'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;     &lt;span class=&quot;c&quot;&gt;# perform sampling on GPU 0&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;batch_size&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1024&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;shuffle&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;use_uva&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;         &lt;span class=&quot;c&quot;&gt;# turn on UVA optimization&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;To speed up feature fetching (step 3), DGL 0.8 supports pre-fetching node/edge
features so that the model computation can happen in parallel with data
movement. Users can specify the features as well as the labels to prefetch in
the sampler object.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;g&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;...&lt;/span&gt;           &lt;span class=&quot;c&quot;&gt;# some DGLGraph data&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;train_nids&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;...&lt;/span&gt;  &lt;span class=&quot;c&quot;&gt;# training node IDs&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;sampler&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dgl&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dataloading&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;MultiLayerNeighborSampler&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;fanout&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;10&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;15&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;prefetch_node_feats&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'feat'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;   &lt;span class=&quot;c&quot;&gt;# prefetch node feature 'feat'&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;prefetch_labels&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'label'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;      &lt;span class=&quot;c&quot;&gt;# prefetch node label 'label'&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;dataloader&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dgl&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dataloading&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;DataLoader&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;g&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;train_nids&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sampler&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;device&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'cuda:0'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;     &lt;span class=&quot;c&quot;&gt;# perform sampling on GPU 0&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;batch_size&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1024&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;shuffle&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;use_uva&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;         &lt;span class=&quot;c&quot;&gt;# turn on UVA optimization&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;These optimizations bring significant speedup for both supervised and
unsupervised mini-batch training. We compared it against the original pipeline
of sampling on CPU but training on GPU for training a two-layer GraphSAGE model
on the ogbn-papers100M graph using A100 GPUs. We observed a speedup of 3.9x and
1.5x for supervised and unsupervised GraphSAGE on a single GPU respectively.
The speedup applies to multi-GPU training as well.&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;&lt;img src=&quot;/assets/images/posts/2022-03-01-release/number1.png&quot; alt=&quot;number1&quot; /&gt;&lt;/th&gt;
      &lt;th&gt;&lt;img src=&quot;/assets/images/posts/2022-03-01-release/number2.png&quot; alt=&quot;number2&quot; /&gt;&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;Speedup of Supervised GraphSAGE&lt;/td&gt;
      &lt;td&gt;Speedup of Unsupervised GraphSAGE&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;Defining a new sampler in DGL v0.8 is also easier, with only one simple
interface &lt;code class=&quot;highlighter-rouge&quot;&gt;sample&lt;/code&gt; to follow. Optionally, users can specify how to prefetch
features for each sample. For example, the cluster sampler used by Cluster-GCN
can be implemented in just a few lines of code.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;class&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;ClusterGCNSampler&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;__init__&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;g&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;k&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;prefetch_ndata&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;None&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;part_ids&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dgl&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;metis_partition_assignment&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;g&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;k&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;c&quot;&gt;# convert partition assignment to bins of nodes&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;part_sizes&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;histogram&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;part_ids&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;float&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;k&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;int&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
        &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;node_bins&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;split&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;argsort&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;part_ids&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;part_sizes&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;c&quot;&gt;# save the node feature names to be prefetched&lt;/span&gt;
        &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;prefetch_ndata&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;prefetch_ndata&lt;/span&gt;

    &lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;sample&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;g&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;part_ids&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
        &lt;span class=&quot;s&quot;&gt;&quot;&quot;&quot;Sample a subgraph given a list of partition IDs.&quot;&quot;&quot;&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;node_ids&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cat&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;node_bins&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pid&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pid&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;part_ids&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;sg&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;g&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;subgraph&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;node_ids&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;  &lt;span class=&quot;c&quot;&gt;# get an induced subgraph&lt;/span&gt;
        &lt;span class=&quot;c&quot;&gt;# tell which feature to pre-fetch&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;dgl&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;set_node_lazy_feature&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sg&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;prefetch_ndata&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sg&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;New samplers in v0.8:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://docs.dgl.ai/generated/dgl.dataloading.ClusterGCNSampler.html#dgl.dataloading.ClusterGCNSampler&quot;&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;dgl.dataloading.ClusterGCNSampler&lt;/code&gt;&lt;/a&gt;: The sampler from &lt;a href=&quot;https://arxiv.org/abs/1905.07953&quot;&gt;Cluster-GCN: An
Efficient Algorithm for Training Deep and Large Graph Convolutional Networks&lt;/a&gt;.&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://docs.dgl.ai/generated/dgl.dataloading.ShaDowKHopSampler.html#dgl.dataloading.ShaDowKHopSampler&quot;&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;dgl.dataloading.ShaDowKHopSampler&lt;/code&gt;&lt;/a&gt;: The sampler from &lt;a href=&quot;https://arxiv.org/abs/2012.01380&quot;&gt;Deep Graph Neural Networks with Shallow Subgraph Samplers&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;This remarkable improvement would not happen without the help from the community.
We want to thank Xin Yao (@yaox12) and Dominique LaSalle (@nv-dlasalle) from
NVIDIA and David Min (@davidmin7) from UIUC for their contributions.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Further reading:&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;User guide chapter for &lt;a href=&quot;https://docs.dgl.ai/guide/minibatch-custom-sampler.html&quot;&gt;customizing graph samplers&lt;/a&gt;.&lt;/li&gt;
  &lt;li&gt;User guide chapter for &lt;a href=&quot;https://docs.dgl.ai/guide/minibatch-prefetching.html&quot;&gt;writing graph samplers with feature prefetching&lt;/a&gt;.&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/dmlc/dgl/tree/master/examples/pytorch/cluster_gcn&quot;&gt;Example implementation&lt;/a&gt; of ClusterGCN.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;nn-module-update&quot;&gt;NN Module Update&lt;/h2&gt;
&lt;p&gt;Heterogeneous GNNs are known to be both difficult to implement as well as
difficult to optimize. In this release, we have significantly improved the
speed of dgl.nn.RelGraphConv and dgl.nn.HGTConv – two state-of-the-art NN
modules for training on heterogeneous graphs, by sometimes an order of
magnitude compared with various baselines[3]:&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;&lt;img src=&quot;/assets/images/posts/2022-03-01-release/number3.png&quot; alt=&quot;number3&quot; /&gt;&lt;/th&gt;
      &lt;th&gt;&lt;img src=&quot;/assets/images/posts/2022-03-01-release/number4.png&quot; alt=&quot;number4&quot; /&gt;&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;Speedup of RGCN convolution&lt;/td&gt;
      &lt;td&gt;Speedup of HGT convolution&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;More importantly, writing an efficient heterogeneous graph convolution is
substantially easier. Here is a minimal implementation of RGCN convolution in
0.8 using the new &lt;a href=&quot;https://docs.dgl.ai/generated/dgl.nn.pytorch.TypedLinear.html#dgl.nn.pytorch.TypedLinear&quot;&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;nn.TypedLinear&lt;/code&gt;&lt;/a&gt; module:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;class&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;RGCNConv&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Module&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;__init__&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;in_size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;out_size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;num_etypes&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
        &lt;span class=&quot;c&quot;&gt;# TypedLinear is a new module in 0.8!&lt;/span&gt;
        &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;linear_r&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dgl&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;TypedLinear&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;in_size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;out_size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;num_etypes&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

    &lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;forward&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;g&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;etype&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;g&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ndata&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'x'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;g&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;edata&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'etype'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;etype&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;g&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;update_all&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;message&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dgl&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;function&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;sum&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'m'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'h'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;g&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ndata&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'h'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;

    &lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;message&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;edges&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;linear_r&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;edges&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;src&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'h'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;edges&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'etype'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;This release also brings 11 new NN modules covering the most requested ones
from the community. They include but are not limited to:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Commonly used &lt;a href=&quot;https://docs.dgl.ai/api/python/nn-pytorch.html#score-modules-for-link-prediction-and-knowledge-graph-completion&quot;&gt;edge score modules&lt;/a&gt; (e.g., TransE, TransR, etc.) for link
prediction.&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://docs.dgl.ai/api/python/nn-pytorch.html#heterogeneous-learning-modules&quot;&gt;Linear projection module and embedding module for heterogeneous graphs&lt;/a&gt;
(&lt;code class=&quot;highlighter-rouge&quot;&gt;nn.HeteroLinear&lt;/code&gt; and &lt;code class=&quot;highlighter-rouge&quot;&gt;nn.HeteroEmbedding&lt;/code&gt;).&lt;/li&gt;
  &lt;li&gt;The &lt;a href=&quot;https://docs.dgl.ai/generated/dgl.nn.pytorch.explain.GNNExplainer.html#dgl.nn.pytorch.explain.GNNExplainer&quot;&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;GNNExplainer&lt;/code&gt;&lt;/a&gt; module.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;understand-graph-via-visualisation-and-gnn-based-explanation&quot;&gt;Understand Graph via Visualisation and GNN-based Explanation&lt;/h2&gt;
&lt;p&gt;Understanding graph data using GNN-based explanation model has become an
important research topic. We partnered with the HKUST VisLab team to release
GNNLens, an interactive visualization tool for graph neural networks.&lt;/p&gt;

&lt;p&gt;To install GNNLens, &lt;code class=&quot;highlighter-rouge&quot;&gt;pip install gnnlens&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;It provides Python APIs for specifying the data to be visualized. For example,
the following shows how to load DGL’s built-in Cora graph dataset and visualize
it:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;dgl.data&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;CoraGraphDataset&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;dataset&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;CoraGraphDataset&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;G&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dataset&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;

&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;gnnlens&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Writer&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# Specify the path to create a new directory for dumping data files.&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;writer&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Writer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'tutorial_graph'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;writer&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;add_graph&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'Cora'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;graph&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cora_graph&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;writer&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;add_graph&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'Citeseer'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;graph&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;citeseer_graph&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# Finish dumping&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;writer&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;close&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;After running the script, you can then launch GNNLens with the following command:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;gnnlens --logdir tutorial_graph
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;And you will see the webpage in your browser:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/posts/2022-03-01-release/gnnlens.png&quot; alt=&quot;gnnlens&quot; width=&quot;800x&quot; class=&quot;aligncenter&quot; /&gt;&lt;/p&gt;

&lt;p&gt;GNNLens is not only capable of visualizing raw graph data, but also designed
for inspecting graph neural networks such as running explanation models to
explain the prediction. Please check out the tutorials in the project README:
&lt;a href=&quot;https://github.com/dmlc/gnnlens2&quot;&gt;https://github.com/dmlc/gnnlens2&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;composable-graph-data-transforms&quot;&gt;Composable Graph Data Transforms&lt;/h2&gt;

&lt;p&gt;Graph data augmentation has become an important component for graph contrastive
learning or structural prediction in general. The new release makes it easier
to compose and apply various graph augmentation and transformation algorithms
to all DGL’s built-in dataset. The new &lt;code class=&quot;highlighter-rouge&quot;&gt;dgl.transforms&lt;/code&gt; package follows the
style of the PyTorch Dataset Transforms. Users can specify the transforms to
use with the &lt;code class=&quot;highlighter-rouge&quot;&gt;transform&lt;/code&gt; keyword argument of all DGL datasets:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;dgl&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;dgl.transforms&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;T&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;t&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;T&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Compose&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;T&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;AddSelfLoop&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(),&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;T&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;GCNNorm&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(),&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;dataset&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dgl&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;CoraGraphDataset&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;transform&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;t&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;g&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dataset&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;  &lt;span class=&quot;c&quot;&gt;# graph and features will be transformed automatically&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;DGL v0.8 provides 16 commonly used data transform APIs. See the &lt;a href=&quot;https://docs.dgl.ai/api/python/transforms.html&quot;&gt;API
doc&lt;/a&gt; for more information.&lt;/p&gt;

&lt;p&gt;Making graph datasets easily accessible for all kinds of research is important. A common scenario is to adapt a dataset for a different task than it was originally designed for (e.g., training a link prediction model on Cora which is originally for node classification). We therefore add two dataset adapters (&lt;a href=&quot;https://docs.dgl.ai/generated/dgl.data.AsNodePredDataset.html#dgl.data.AsNodePredDataset&quot;&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;dgl.data.AsNodePredDataset&lt;/code&gt;&lt;/a&gt; and &lt;a href=&quot;https://docs.dgl.ai/generated/dgl.data.AsLinkPredDataset.html#dgl.data.AsLinkPredDataset&quot;&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;dgl.data.AsLinkPredDataset&lt;/code&gt;&lt;/a&gt;) for this purpose. We also support generating new train/val/test split and save them for later use:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;dgl&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;dataset&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dgl&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;CoraGraphDataset&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# make a Cora dataset suitable for link prediction&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# add train/val/test split and negative samples&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;dataset&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dgl&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;AsLinkPredDataset&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dataset&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;split_ratio&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.8&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;neg_ratio&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;h2 id=&quot;one-more-thing&quot;&gt;One more thing&lt;/h2&gt;

&lt;p&gt;As GNN is still a young and blooming domain, we received many “how to start” questions from our users:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;em&gt;“I’ve heard about GNNs, how to start training a GNN model on my own datasets?”&lt;/em&gt;&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;“I want to learn more about GNNs, how to start experimenting with SOTA baselines?”&lt;/em&gt;&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;“I have some new research ideas, how to start building it upon existing GNN models?”&lt;/em&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;To make those first steps easier, we developed &lt;strong&gt;DGL-Go&lt;/strong&gt;, a command line tool for
users to quickly access the latest GNN research progress.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/posts/2022-03-01-release/dglgo.png&quot; alt=&quot;dglgo&quot; width=&quot;600x&quot; class=&quot;aligncenter&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Using DGL-Go is as easy as three steps:&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;Use &lt;code class=&quot;highlighter-rouge&quot;&gt;dgl configure&lt;/code&gt; to pick the task, dataset and model of your interests.
It generates a configuration file for later use. You could also use &lt;code class=&quot;highlighter-rouge&quot;&gt;dgl
recipe get&lt;/code&gt; to retrieve a configuration file we provided.&lt;/li&gt;
  &lt;li&gt;Use &lt;code class=&quot;highlighter-rouge&quot;&gt;dgl train&lt;/code&gt; to launch training according to the configuration and see
the results.&lt;/li&gt;
  &lt;li&gt;Use &lt;code class=&quot;highlighter-rouge&quot;&gt;dgl export&lt;/code&gt; to generate a &lt;strong&gt;self-contained, reproducible&lt;/strong&gt; Python
script for advanced customization, or try the model on custom data stored in
CSV format.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Install DGL-Go simply by &lt;code class=&quot;highlighter-rouge&quot;&gt;pip install dglgo&lt;/code&gt; and check out the project &lt;a href=&quot;https://github.com/dmlc/dgl/tree/master/dglgo&quot;&gt;README&lt;/a&gt; for more details.&lt;/p&gt;

&lt;h2 id=&quot;further-reading&quot;&gt;Further Reading&lt;/h2&gt;
&lt;p&gt;The full &lt;a href=&quot;https://github.com/dmlc/dgl/releases/tag/0.8.0&quot;&gt;release note&lt;/a&gt; of DGL v0.8.&lt;/p&gt;

&lt;h2 id=&quot;reference&quot;&gt;Reference&lt;/h2&gt;
&lt;p&gt;[1] PyTorch-Direct: Enabling GPU Centric Data Access for Very Large Graph Neural Network Training with Irregular Accesses&lt;/p&gt;

&lt;p&gt;[2] TorchQuiver: &lt;a href=&quot;https://github.com/quiver-team/torch-quiver&quot;&gt;https://github.com/quiver-team/torch-quiver&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;[3] We compared our new &lt;code class=&quot;highlighter-rouge&quot;&gt;nn.RelGraphConv&lt;/code&gt; module with multiple existing baselines from DGL and PyG. For DGL v0.7, Baseline#1 uses the old &lt;code class=&quot;highlighter-rouge&quot;&gt;nn.RelGraphConv&lt;/code&gt; module with &lt;code class=&quot;highlighter-rouge&quot;&gt;low_mem=False&lt;/code&gt;; Baseline#2 uses the old &lt;code class=&quot;highlighter-rouge&quot;&gt;nn.RelGraphConv&lt;/code&gt; with &lt;code class=&quot;highlighter-rouge&quot;&gt;low_mem=True&lt;/code&gt;; Baseline#3 uses &lt;code class=&quot;highlighter-rouge&quot;&gt;nn.HeteroGarphConv&lt;/code&gt;. For PyG, Baseline#1 uses &lt;code class=&quot;highlighter-rouge&quot;&gt;nn.RGCNConv&lt;/code&gt; while Baseline#2 uses &lt;code class=&quot;highlighter-rouge&quot;&gt;nn.FastRGCNConv&lt;/code&gt;. All the benchmarks are tested on one NVIDIA T4 GPU card.&lt;/p&gt;</content><author><name>DGLTeam</name></author><category term="release" /><category term="release" /><summary type="html">We are excited to announce the release of DGL v0.8, which brings many new features as well as improvement on system performance. The highlights are:</summary></entry><entry><title type="html">v0.7 Release Highlights</title><link href="https://www.dgl.ai/release/2021/07/26/release.html" rel="alternate" type="text/html" title="v0.7 Release Highlights" /><published>2021-07-26T00:00:00+00:00</published><updated>2021-07-26T00:00:00+00:00</updated><id>https://www.dgl.ai/release/2021/07/26/release</id><content type="html" xml:base="https://www.dgl.ai/release/2021/07/26/release.html">&lt;p&gt;v0.7 brings improvements on the low-level system infrastructure as well as on
the high-level user-facing utilities. Many of
them involve contributions from the user community. We are grateful to see such
a growing trend and welcome more in the future. Here are the notable
updates.&lt;/p&gt;

&lt;h2 id=&quot;gpu-based-neighbor-sampling&quot;&gt;GPU-based Neighbor Sampling&lt;/h2&gt;

&lt;p&gt;We worked with NVIDIA to make DGL support uniform neighbor sampling and MFG
conversion on GPU. This removes the need to move samples from CPU to GPU in
each iteration and at the same time accelerate the sampling step using GPU
acceleration. As a result, experiment for GraphSAGE on the ogbn-product graph
gets a &lt;strong&gt;&amp;gt;10x speedup&lt;/strong&gt; (reduced from 113s to 11s per epoch) on a g3.16x
instance. To enable the feature, create a NodeDataLoader with a GPU graph and
specify the sampling device to be on GPU:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;g&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;...&lt;/span&gt;             &lt;span class=&quot;c&quot;&gt;# create a graph&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;g&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;g&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;to&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'cuda:0'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;  &lt;span class=&quot;c&quot;&gt;# move the graph to GPU&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# create a data loader&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;dataloader&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dgl&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dataloading&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;NodeDataLoader&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;g&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;                                &lt;span class=&quot;c&quot;&gt;# now accepts graph on GPU&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;train_nid&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;sampler&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;device&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;device&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'cuda:0'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt;    &lt;span class=&quot;c&quot;&gt;# specify the sampling device&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;num_workers&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;                    &lt;span class=&quot;c&quot;&gt;# num_workers must be 0&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;batch_size&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1000&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;drop_last&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;False&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;shuffle&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# training loop&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;input_nodes&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;output_nodes&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sample_graphs&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dataloader&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;c&quot;&gt;# the produced sample_graphs are already on GPU&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;train_on&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;input_nodes&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;output_nodes&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sample_graphs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;The following docs have been updated accordingly:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;A new user guide chapter &lt;a href=&quot;https://docs.dgl.ai/guide/minibatch-gpu-sampling.html&quot;&gt;Using GPU for Neighborhood
Sampling&lt;/a&gt; about when
and how to use this new feature.&lt;/li&gt;
  &lt;li&gt;The API doc of &lt;a href=&quot;https://docs.dgl.ai/api/python/dgl.dataloading.html#dgl.dataloading.pytorch.NodeDataLoader&quot;&gt;NodeDataLoader&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;em&gt;We thank @nv-dlasalle from NVIDIA for contributing the CUDA kernels for
performing neighbor sampling as well as MFG conversion.&lt;/em&gt;&lt;/p&gt;

&lt;h2 id=&quot;improved-cpu-message-passing-kernel&quot;&gt;Improved CPU Message Passing Kernel&lt;/h2&gt;

&lt;p&gt;The core SpMM kernel for GNN message passing on CPU has been re-implemented.
The new kernel performs tiling on CSR matrix and leverages Intel’s LibXSMM for
kernel generation.  Please read the paper
&lt;a href=&quot;https://arxiv.org/abs/2104.06700&quot;&gt;https://arxiv.org/abs/2104.06700&lt;/a&gt; for more
details. The feature is turned on automatically for Xeon CPUs which shows significant
speed boost. &lt;em&gt;We thank @sanchit-misra and Intel for contributing the new CPU kernel.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/posts/2021-07-26-release/cpu_kernel.png&quot; alt=&quot;cpu_kernel&quot; width=&quot;600x&quot; class=&quot;aligncenter&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;better-nodeembedding-for-multi-gpu-training-and-distributed-training&quot;&gt;Better NodeEmbedding for multi-GPU training and distributed training&lt;/h2&gt;

&lt;p&gt;DGL now utilizes &lt;a href=&quot;https://developer.nvidia.com/nccl&quot;&gt;NCCL&lt;/a&gt; to synchronize the
gradients of sparse node embeddings (&lt;code class=&quot;highlighter-rouge&quot;&gt;dgl.nn.NodeEmbedding&lt;/code&gt;) during training. It
is enabled automatically when users specify nccl as the backend for
&lt;code class=&quot;highlighter-rouge&quot;&gt;torch.distributed.init_process_group&lt;/code&gt;. Our experiment shows a &lt;strong&gt;20% speedup&lt;/strong&gt;
(reduced from 47.2s to 39.5s per epoch) on a g4dn.12xlarge (4 T4 GPU) instance
for training RGCN on ogbn-mag graph. &lt;em&gt;We thank the effort from @nv-dlasalle and
NVIDIA.&lt;/em&gt; Distributed node embedding now uses synchronized gradient update,
making the training more stable.&lt;/p&gt;

&lt;h2 id=&quot;dgl-kubernetes-operator&quot;&gt;DGL Kubernetes Operator&lt;/h2&gt;

&lt;p&gt;Qihoo360 built a DGL Operator that makes running graph neural network
distributed or non-distributed training on Kubernetes. Please check out their
repository for usage: &lt;a href=&quot;https://github.com/Qihoo360/dgl-operator&quot;&gt;https://github.com/Qihoo360/dgl-operator&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;other-performance-gains&quot;&gt;Other Performance Gains&lt;/h2&gt;

&lt;p&gt;Apart from the major feature improvement, we have also received helps from
community contributors for fixing performance issues. Notably, DGL’s CPU random
walk sampling is improved by 24x on medium to large size graphs; the memory
consumption of distributed training set splitting drops by ~7x on graphs of
billion scale.&lt;/p&gt;

&lt;h2 id=&quot;more-than-more-models&quot;&gt;More than More Models&lt;/h2&gt;

&lt;p&gt;As usual, the release brings a batch of &lt;strong&gt;19 new model examples&lt;/strong&gt; to the
repository bringing the total number to be &lt;strong&gt;over 90&lt;/strong&gt;. To help users find examples
that fit their needs (e.g. certain topic, datasets), we present a new search
tool on &lt;a href=&quot;https://www.dgl.ai/&quot;&gt;dgl.ai&lt;/a&gt; which supports finding examples by
keywords.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/posts/2021-07-26-release/0.7-high.gif&quot; alt=&quot;search&quot; width=&quot;800x&quot; class=&quot;aligncenter&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Below are the new models added in v0.7.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Interaction Networks for Learning about Objects, Relations, and Physics&lt;/li&gt;
  &lt;li&gt;Multi-GPU RGAT for OGB-LSC Node Classification&lt;/li&gt;
  &lt;li&gt;Network Embedding with Completely-imbalanced Labels&lt;/li&gt;
  &lt;li&gt;Temporal Graph Networks improved&lt;/li&gt;
  &lt;li&gt;Diffusion Convolutional Recurrent Neural Network&lt;/li&gt;
  &lt;li&gt;Gated Attention Networks for Learning on Large and Spatiotemporal Graphs&lt;/li&gt;
  &lt;li&gt;DeeperGCN&lt;/li&gt;
  &lt;li&gt;Deep Graph Contrastive Representation Learning&lt;/li&gt;
  &lt;li&gt;Graph Neural Networks Inspired by Classical Iterative Algorithms&lt;/li&gt;
  &lt;li&gt;GraphSAINT&lt;/li&gt;
  &lt;li&gt;Label Propagation&lt;/li&gt;
  &lt;li&gt;Combining Label Propagation and Simple Models Out-performs Graph Neural Networks&lt;/li&gt;
  &lt;li&gt;GCNII&lt;/li&gt;
  &lt;li&gt;Latent Dirichlet Allocation on GPU&lt;/li&gt;
  &lt;li&gt;A Heterogeneous Information Network based Cross Domain Insurance Recommendation System for Cold Start Users&lt;/li&gt;
  &lt;li&gt;Five heterogeneous graph models: HetGNN/GTN/HAN/NSHE/MAGNN.  Sparse matrix
multiplication and addition with autograd are also added as a result.&lt;/li&gt;
  &lt;li&gt;Heterogeneous Graph Attention Networks with minibatch sampling&lt;/li&gt;
  &lt;li&gt;Learning Hierarchical Graph Neural Networks for Image Clustering&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;tutorials-for-multi-gpu-and-distributed-training&quot;&gt;Tutorials for Multi-GPU and Distributed Training&lt;/h2&gt;

&lt;p&gt;With a growing interest in applying GNNs on large-scale graphs, we see many
questions from our users about how to utilize multi-GPU or multi-machine for
acceleration. In this release, we published two new tutorials about multi-GPU
training for node classification and graph classification, respectively. There
is also a new tutorial about distributed training across multiple machines. All
of them are available at &lt;a href=&quot;https://docs.dgl.ai/&quot;&gt;docs.dgl.ai&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/posts/2021-07-26-release/multi-gpu-tut.png&quot; alt=&quot;tutorial&quot; width=&quot;800x&quot; class=&quot;aligncenter&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;further-readings&quot;&gt;Further Readings&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;Full release note: &lt;a href=&quot;https://github.com/dmlc/dgl/releases/tag/v0.7.0&quot;&gt;https://github.com/dmlc/dgl/releases/tag/v0.7.0&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</content><author><name>DGLTeam</name></author><category term="release" /><category term="release" /><summary type="html">v0.7 brings improvements on the low-level system infrastructure as well as on the high-level user-facing utilities. Many of them involve contributions from the user community. We are grateful to see such a growing trend and welcome more in the future. Here are the notable updates.</summary></entry><entry><title type="html">v0.6 Release Highlight</title><link href="https://www.dgl.ai/release/2021/02/25/release.html" rel="alternate" type="text/html" title="v0.6 Release Highlight" /><published>2021-02-25T00:00:00+00:00</published><updated>2021-02-25T00:00:00+00:00</updated><id>https://www.dgl.ai/release/2021/02/25/release</id><content type="html" xml:base="https://www.dgl.ai/release/2021/02/25/release.html">&lt;p&gt;The recent DGL 0.6 release is a major update on many aspects of the project
including documentation, APIs, system speed, and scalability. This article
highlights some of the new features and enhancements.&lt;/p&gt;

&lt;h2 id=&quot;a-blitz-introduction-to-dgl-in-120-minutes&quot;&gt;A Blitz Introduction to DGL in 120 minutes&lt;/h2&gt;

&lt;p&gt;The brand new set of tutorials come from our past hands-on tutorials in several
major academic conferences (e.g., KDD’19, KDD’20, WWW’20). They start from an
end-to-end example of using GNNs for node classification, and gradually unveil
the core components in DGL such as DGLGraph, GNN modules, and graph datasets.
The tutorials are now available on &lt;a href=&quot;https://docs.dgl.ai/tutorials/blitz/index.html&quot;&gt;docs.dgl.ai&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/posts/2021-02-25-release/blitz-intro.png&quot; alt=&quot;blitz&quot; width=&quot;800x&quot; class=&quot;aligncenter&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;a-gentle-tutorial-on-mini-batch-training-of-gnns&quot;&gt;A Gentle Tutorial on Mini-batch Training of GNNs&lt;/h2&gt;

&lt;p&gt;The scale of real world data can be massive, which demands training GNNs
stochastically by mini-batches. However, unlike images or text corpus where
data samples are independent, stochastic training of GNNs is more complex
because one must handle the dependencies among samples. We observed that
stochastic training is one of the most-asked topics on our discuss forum. In
0.6, we summarize the answers to those common questions in a set of
&lt;a href=&quot;https://docs.dgl.ai/tutorials/large/index.html&quot;&gt;tutorials&lt;/a&gt; on stochastic
training of GNNs, including the insight into neighbor sampling algorithms,
training loops and code snippets in DGL to realize them.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://data.dgl.ai/tutorial/img/sampling.gif&quot; alt=&quot;sampling&quot; width=&quot;800x&quot; class=&quot;aligncenter&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;more-examples&quot;&gt;More Examples&lt;/h2&gt;

&lt;p&gt;The release includes &lt;strong&gt;13 new examples&lt;/strong&gt;, brings &lt;strong&gt;a total of 72 models&lt;/strong&gt;:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;MixHop: Higher-Order Graph Convolutional Architectures via Sparsified Neighborhood Mixing: &lt;a href=&quot;https://github.com/dmlc/dgl/tree/master/examples/pytorch/mixhop&quot;&gt;https://github.com/dmlc/dgl/tree/master/examples/pytorch/mixhop&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Self-Attention Graph Pooling: &lt;a href=&quot;https://github.com/dmlc/dgl/tree/master/examples/pytorch/sagpool]&quot;&gt;https://github.com/dmlc/dgl/tree/master/examples/pytorch/sagpool&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;GNN-FiLM: Graph Neural Networks with Feature-wise Linear Modulation: &lt;a href=&quot;https://github.com/dmlc/dgl/tree/master/examples/pytorch/GNN-FiLM&quot;&gt;https://github.com/dmlc/dgl/tree/master/examples/pytorch/GNN-FiLM&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;TensorFlow implementation of Simplifying Graph Convolutional Networks: &lt;a href=&quot;https://github.com/dmlc/dgl/tree/master/examples/tensorflow/sgc&quot;&gt;https://github.com/dmlc/dgl/tree/master/examples/tensorflow/sgc&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Graph Representation Learning via Hard and Channel-Wise Attention Networks: &lt;a href=&quot;https://github.com/dmlc/dgl/tree/master/examples/pytorch/hardgat&quot;&gt;https://github.com/dmlc/dgl/tree/master/examples/pytorch/hardgat&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Graph Random Neural Network for Semi-Supervised Learning on Graphs: &lt;a href=&quot;https://github.com/dmlc/dgl/tree/master/examples/pytorch/grand&quot;&gt;https://github.com/dmlc/dgl/tree/master/examples/pytorch/grand&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Hierarchical Graph Pooling with Structure Learning: &lt;a href=&quot;https://github.com/dmlc/dgl/tree/master/examples/pytorch/hgp_sl&quot;&gt;https://github.com/dmlc/dgl/tree/master/examples/pytorch/hgp_sl&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Towards Deeper Graph Neural Networks: &lt;a href=&quot;https://github.com/dmlc/dgl/tree/master/examples/pytorch/dagnn&quot;&gt;https://github.com/dmlc/dgl/tree/master/examples/pytorch/dagnn&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;PointNet: Deep Learning on Point Sets for 3D Classification and Segmentation/PointNet++: Deep Hierarchical Feature Learning on Point Sets in a Metric Space (part segmentation): &lt;a href=&quot;https://github.com/dmlc/dgl/tree/master/examples/pytorch/pointcloud/pointnet&quot;&gt;https://github.com/dmlc/dgl/tree/master/examples/pytorch/pointcloud/pointnet&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Graph Cross Networks with Vertex Infomax Pooling: &lt;a href=&quot;https://github.com/dmlc/dgl/tree/master/examples/pytorch/gxn&quot;&gt;https://github.com/dmlc/dgl/tree/master/examples/pytorch/gxn&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Neural Graph Collaborative Filtering: &lt;a href=&quot;https://github.com/dmlc/dgl/tree/master/examples/pytorch/NGCF&quot;&gt;https://github.com/dmlc/dgl/tree/master/examples/pytorch/NGCF&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Link Prediction Based on Graph Neural Networks: &lt;a href=&quot;https://github.com/dmlc/dgl/tree/master/examples/pytorch/seal&quot;&gt;https://github.com/dmlc/dgl/tree/master/examples/pytorch/seal&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Graph Neural Networks with Convolutional ARMA Filters: &lt;a href=&quot;https://github.com/dmlc/dgl/tree/master/examples/pytorch/arma&quot;&gt;https://github.com/dmlc/dgl/tree/master/examples/pytorch/arma&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The &lt;a href=&quot;https://github.com/dmlc/dgl/tree/master/examples&quot;&gt;official example folder&lt;/a&gt; now indexes the examples by their notable tags such as their targeted tasks and so on.&lt;/p&gt;

&lt;h2 id=&quot;usability-enhancements&quot;&gt;Usability Enhancements&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;Two new APIs &lt;a href=&quot;https://docs.dgl.ai/generated/dgl.DGLGraph.set_batch_num_nodes.html#dgl.DGLGraph.set_batch_num_nodes&quot;&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;DGLGraph.set_batch_num_nodes&lt;/code&gt;&lt;/a&gt; and &lt;a href=&quot;https://docs.dgl.ai/generated/dgl.DGLGraph.set_batch_num_edges.html#dgl.DGLGraph.set_batch_num_edges&quot;&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;DGLGraph.set_batch_num_edges&lt;/code&gt;&lt;/a&gt; for setting batch information manually, which are useful for transforming a batched graph into another or constructing a new batched graph manually.&lt;/li&gt;
  &lt;li&gt;A new API &lt;a href=&quot;https://docs.dgl.ai/api/python/dgl.dataloading.html#dgl.dataloading.pytorch.GraphDataLoader&quot;&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;GraphDataLoader&lt;/code&gt;&lt;/a&gt;, a data loader wrapper for graph classification tasks.&lt;/li&gt;
  &lt;li&gt;A new dataset class &lt;a href=&quot;https://docs.dgl.ai/api/python/dgl.data.html#qm9-dataset&quot;&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;QM9Dataset&lt;/code&gt;&lt;/a&gt;.&lt;/li&gt;
  &lt;li&gt;A new namespace &lt;a href=&quot;https://docs.dgl.ai/api/python/nn.functional.html&quot;&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;dgl.nn.functional&lt;/code&gt;&lt;/a&gt; for hosting NN related utility functions.&lt;/li&gt;
  &lt;li&gt;DGL now supports training with half precision and is compatible with PyTorch’s automatic mixed precision package. See the &lt;a href=&quot;https://docs.dgl.ai/guide/mixed_precision.html&quot;&gt;user guide chapter&lt;/a&gt; for how to use it.&lt;/li&gt;
  &lt;li&gt;(Experimental) Users can now use DistGraph with heterogeneous graph data. This also applies to &lt;code class=&quot;highlighter-rouge&quot;&gt;dgl.sample_neighbors&lt;/code&gt; on DistGraph. In addition, DGL supports distributed graph partitioning on a cluster of machines. See the &lt;a href=&quot;https://docs.dgl.ai/guide/distributed.html&quot;&gt;user guide chapter&lt;/a&gt; for more details.&lt;/li&gt;
  &lt;li&gt;(Experimental) Several new APIs for training sparse embeddings:
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;https://docs.dgl.ai/api/python/nn.pytorch.html#nodeembedding-module&quot;&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;dgl.nn.NodeEmbedding&lt;/code&gt;&lt;/a&gt; is a dedicated class for storing trainable node embeddings that can scale to graphs with millions of nodes.&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;https://docs.dgl.ai/api/python/dgl.optim.html#dgl.optim.pytorch.SparseAdagrad&quot;&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;dgl.optim.SparseAdagrad&lt;/code&gt;&lt;/a&gt; and &lt;a href=&quot;https://docs.dgl.ai/api/python/dgl.optim.html#dgl.optim.pytorch.SparseAdam&quot;&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;dgl.optim.SparseAdam&lt;/code&gt;&lt;/a&gt; are two optimizers for the NodeEmbedding class.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;system-efficiency-improvements&quot;&gt;System Efficiency Improvements&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;With PyTorch backend, DGL will use PyTorch’s native memory management to cache repeated memory allocation and deallocation.&lt;/li&gt;
  &lt;li&gt;A new implementation for &lt;code class=&quot;highlighter-rouge&quot;&gt;nn.RelGraphConv&lt;/code&gt; when &lt;code class=&quot;highlighter-rouge&quot;&gt;low_mem=True&lt;/code&gt; (PyTorch backend). A benchmark on V100 GPU shows it gives a &lt;strong&gt;4.8x&lt;/strong&gt; boost in training speed on AIFB dataset.&lt;/li&gt;
  &lt;li&gt;Faster CPU kernels using AVX512 instructions.&lt;/li&gt;
  &lt;li&gt;Faster GPU kernels on CUDA 11.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;further-readings&quot;&gt;Further Readings&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;Full release note: &lt;a href=&quot;https://github.com/dmlc/dgl/releases/tag/v0.6.0&quot;&gt;https://github.com/dmlc/dgl/releases/tag/v0.6.0&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</content><author><name>DGLTeam</name></author><category term="release" /><category term="release" /><summary type="html">The recent DGL 0.6 release is a major update on many aspects of the project including documentation, APIs, system speed, and scalability. This article highlights some of the new features and enhancements.</summary></entry><entry><title type="html">DGL Empowers Service for Predictions on Connected Datasets with Graph Neural Networks</title><link href="https://www.dgl.ai/news/2020/12/15/neptuneml.html" rel="alternate" type="text/html" title="DGL Empowers Service for Predictions on Connected Datasets with Graph Neural Networks" /><published>2020-12-15T00:00:00+00:00</published><updated>2020-12-15T00:00:00+00:00</updated><id>https://www.dgl.ai/news/2020/12/15/neptuneml</id><content type="html" xml:base="https://www.dgl.ai/news/2020/12/15/neptuneml.html">&lt;p&gt;AWS just announced the availability of &lt;a href=&quot;http://aws.amazon.com/neptune/machine-learning/&quot;&gt;Neptune ML&lt;/a&gt;.
Amazon Neptune is a fast,
reliable, fully managed graph database service that makes it easy to build and
run applications that work with highly connected datasets. Neptune ML is a new
capability that uses graph neural networks (GNNs), a machine learning (ML)
technique purpose-built for graphs, for making easy, fast, and accurate
predictions on graphs. The accuracy of most predictions for graphs increases to
50% with Neptune ML when compared to non-graph methods. Neptune ML uses the
Deep Graph Library (DGL), an open-source library to which AWS contributes that
makes it easy to develop and apply GNN models on graph data. Now, developers
can create, train, and apply ML on Neptune data in hours instead of weeks
without the need to learn new tools and ML technologies.&lt;/p&gt;

&lt;p&gt;We would love to see more commercial vendors build innovation on top of DGL in
the future. For more information about Neptune ML, please visit the &lt;a href=&quot;https://aws.amazon.com/blogs/database/announcing-amazon-neptune-ml-easy-fast-and-accurate-predictions-on-graphs/&quot;&gt;AWS blog&lt;/a&gt;
and &lt;a href=&quot;https://aws.amazon.com/neptune/machine-learning/&quot;&gt;product page&lt;/a&gt;. Watch the
&lt;a href=&quot;https://reinvent.awsevents.com/keynotes/&quot;&gt;re:Invent 2020 Machine Learning Keynote&lt;/a&gt;
by Swami Sivasubramanian for the full announcement.&lt;/p&gt;</content><author><name>DGLTeam</name></author><category term="news" /><category term="news" /><summary type="html">AWS just announced the availability of Neptune ML. Amazon Neptune is a fast, reliable, fully managed graph database service that makes it easy to build and run applications that work with highly connected datasets. Neptune ML is a new capability that uses graph neural networks (GNNs), a machine learning (ML) technique purpose-built for graphs, for making easy, fast, and accurate predictions on graphs. The accuracy of most predictions for graphs increases to 50% with Neptune ML when compared to non-graph methods. Neptune ML uses the Deep Graph Library (DGL), an open-source library to which AWS contributes that makes it easy to develop and apply GNN models on graph data. Now, developers can create, train, and apply ML on Neptune data in hours instead of weeks without the need to learn new tools and ML technologies.</summary></entry><entry><title type="html">v0.5.3 Patch Update</title><link href="https://www.dgl.ai/release/2020/12/04/release.html" rel="alternate" type="text/html" title="v0.5.3 Patch Update" /><published>2020-12-04T00:00:00+00:00</published><updated>2020-12-04T00:00:00+00:00</updated><id>https://www.dgl.ai/release/2020/12/04/release</id><content type="html" xml:base="https://www.dgl.ai/release/2020/12/04/release.html">&lt;p&gt;This is a patch release mainly for supporting CUDA 11.0.  Now DGL supports CUDA 11.0 and PyTorch 1.7 on Linux/Windows/Mac.&lt;/p&gt;

&lt;p&gt;Other fixes include:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Performance fix of graph batching. Affect the &lt;code class=&quot;highlighter-rouge&quot;&gt;dgl.batch&lt;/code&gt; API.&lt;/li&gt;
  &lt;li&gt;Speedup on graph readout. Affect &lt;a href=&quot;https://docs.dgl.ai/api/python/dgl.html#batching-and-reading-out-ops&quot;&gt;all APIs&lt;/a&gt; under &lt;code class=&quot;highlighter-rouge&quot;&gt;dgl.readout&lt;/code&gt;.&lt;/li&gt;
  &lt;li&gt;Speedup in CPU SpMM with sum reducer.&lt;/li&gt;
  &lt;li&gt;Performance optimization that removes redundant copies between CPU and GPU.&lt;/li&gt;
  &lt;li&gt;Fix &lt;code class=&quot;highlighter-rouge&quot;&gt;segment_reduce()&lt;/code&gt; ignoring tailing 0 segments.&lt;/li&gt;
  &lt;li&gt;Fix a crash bug due to unfound attribute.&lt;/li&gt;
  &lt;li&gt;Performance optimization in COO-CSR conversion.&lt;/li&gt;
  &lt;li&gt;Parallelization in heterogeneous graph format conversion.&lt;/li&gt;
  &lt;li&gt;Fix a bug to enable distributed training of RGCN with CPU.&lt;/li&gt;
  &lt;li&gt;Numerous documentation fixes.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;New examples:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Sparse embedding for GATNE-T for large graphs.&lt;/li&gt;
  &lt;li&gt;LINE.&lt;/li&gt;
  &lt;li&gt;SIGN for OGB.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The Chinese user guide has been released for chapter 1 to 4. Further chapters will be released soon.&lt;/p&gt;

&lt;p&gt;Full release note: &lt;a href=&quot;https://github.com/dmlc/dgl/releases/tag/0.5.3&quot;&gt;https://github.com/dmlc/dgl/releases/tag/0.5.3&lt;/a&gt;&lt;/p&gt;</content><author><name>DGLTeam</name></author><category term="release" /><category term="release" /><summary type="html">This is a patch release mainly for supporting CUDA 11.0. Now DGL supports CUDA 11.0 and PyTorch 1.7 on Linux/Windows/Mac.</summary></entry><entry><title type="html">What is new in DGL v0.5 release?</title><link href="https://www.dgl.ai/release/2020/08/26/release.html" rel="alternate" type="text/html" title="What is new in DGL v0.5 release?" /><published>2020-08-26T00:00:00+00:00</published><updated>2020-08-26T00:00:00+00:00</updated><id>https://www.dgl.ai/release/2020/08/26/release</id><content type="html" xml:base="https://www.dgl.ai/release/2020/08/26/release.html">&lt;p&gt;The recent DGL 0.5 release is a major update on many aspects of the project
including documentation, APIs, system speed and scalability. This article
highlights some of the new features and enhancements.&lt;/p&gt;

&lt;h2 id=&quot;more-docs-fewer-codes&quot;&gt;More docs, fewer codes&lt;/h2&gt;

&lt;p&gt;DGL has been through several releases with numerous new APIs and
features. While the development pace is rapid, DGL’s documentation has
been lagging behind. We have been aware of this issue and finally got a hand on
it in this release. There are two major changes. A new
&lt;a href=&quot;https://docs.dgl.ai/en/0.5.x/guide/index.html&quot;&gt;user guide&lt;/a&gt; with dedicated chapters for the
core concepts of DGL and how they connect with the pipeline of training/testing
GNNs. There are currently seven chapters:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;em&gt;Graph&lt;/em&gt;: The chapter explains the basics about the graph data structure, the
usage of the core &lt;code class=&quot;highlighter-rouge&quot;&gt;DGLGraph&lt;/code&gt; class, heterogeneous graph and so on.&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;Message Passing&lt;/em&gt;: The chapter starts from the mathematical definition of the
message passing neural networks and then explains how to express them in DGL.&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;Building GNN Modules&lt;/em&gt;: The chapter walk-throughs the steps to define GNN
layers/modules in DGL for both homogeneous and heterogeneous graphs.&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;Graph Data Pipeline&lt;/em&gt;: The chapter explains how the datasets are organized in
DGL and how to create with your own one.&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;Training Graph Neural Networks&lt;/em&gt;: The chapter provides guidance on training
GNNs in DGL for node, edge and graph prediction tasks.&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;Stochastic Training on Large Graphs&lt;/em&gt;: The chapter introduces
mini-batch training in the GNN domain and the designated DGL APIs.&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;Distributed Training&lt;/em&gt;: The chapter explains DGL’s components for training
graphs scaling beyond one machine.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Besides the user guide, we have re-worked the API document extensively and
organized them by their namespaces. We also took this chance to prune the set
of API, deprecate rare and redundant APIs and consolidate functionalities into
fewer. For example, the creation of a graph in DGL now only involves &lt;code class=&quot;highlighter-rouge&quot;&gt;dgl.graph&lt;/code&gt;
and &lt;code class=&quot;highlighter-rouge&quot;&gt;dgl.heterograph&lt;/code&gt; for homogeneous and heterogeneous graphs, respectively.
Another noticeable simplification is that &lt;code class=&quot;highlighter-rouge&quot;&gt;DGLGraph&lt;/code&gt; is now the only class for
storing graph and feature. It can represent a homogeneous or heterogeneous
graph, a subgraph or a batched graph.&lt;/p&gt;

&lt;h2 id=&quot;more-flexibility-on-dglgraph&quot;&gt;More flexibility on &lt;code class=&quot;highlighter-rouge&quot;&gt;DGLGraph&lt;/code&gt;&lt;/h2&gt;

&lt;p&gt;The 0.5 release enables more flexibility on the core graph structure. First, DGL
now supports creating graphs stored in int32; it not only cuts the memory
consumption by half compared with int64, but also enables many fast operators
only available for int32 provided by cuSPARSE. Second, previous DGL only
provides APIs to control the host device of node/edge features,  while in the new
version, it allows changing the host device of the graph structure too (via
&lt;code class=&quot;highlighter-rouge&quot;&gt;DGLGraph.to&lt;/code&gt;). DGL has implemented many structure-related operators such as
getting degrees, extracting subgraphs on CUDA. Third, to store giant graphs even
more compactly, DGL adds the &lt;code class=&quot;highlighter-rouge&quot;&gt;DGLGraph.formats&lt;/code&gt; API to control the internal
sparse formats of graphs. This could reduce the memory
consumption by half or more especially for storing the graph for sampling
in mini-batch training. You can find the explanations of all these new features in
the dedicated &lt;a href=&quot;https://docs.dgl.ai/guide/graph.html&quot;&gt;user guide chapter&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;faster-and-deterministic-kernels&quot;&gt;Faster and deterministic kernels&lt;/h2&gt;

&lt;p&gt;Alongside the usability improvement, the DGL team always keeps system
performance at heart. We have conducted an extensive code refactoring during
this release to reduce the Python stack overhead and enhance the code
readability. In addition, we upgrade the core CPU/GPU kernels for message
passing computation. Specifically, we found that the message passing in GNNs
can be reduced to two general computational patterns: g-SpMM and g-SDDMM. The
two patterns have a number of choices of parallelization and DGL carefully
chooses the suitable ones based on the sparse format and operator type.
Moreover, DGL by default chooses deterministic implementations for a better
reproducibility. Read the updated &lt;a href=&quot;https://arxiv.org/abs/1909.01315&quot;&gt;white paper&lt;/a&gt;
for more details about the new kernel design.&lt;/p&gt;

&lt;h2 id=&quot;scaling-beyond-one-machine&quot;&gt;Scaling beyond one machine&lt;/h2&gt;

&lt;p&gt;&lt;em&gt;Can DGL scale to giant graphs that cannot be fit in one machine?&lt;/em&gt; This question
has been on our watch from the genesis of the DGL project. Despite several
attempts from the previous releases, 0.5 is the very first release that
thoroughly defines the user-facing APIs and components for distributed
training. The goal is to create a coherent user experience of mini-batch
training from on a single machine to multiple machines, ideally with few or no
code changes. Specifically, this release includes the following new components:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;To split a graph for distributed computation, DGL integrates a light-weight
version of the highly-optimized METIS &lt;em&gt;*graph partition&lt;/em&gt; toolkit.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;DistGraphServer&lt;/strong&gt; stores the partitioned graph structure and node/edge
features on each machine. These servers work together to serve the graph data
to training processes. One can deploy multiple servers on one machine to boost
the service throughput.&lt;/li&gt;
  &lt;li&gt;New &lt;strong&gt;distributed sampler&lt;/strong&gt; that interacts with remote servers and supports
sampling from partitioned graph.&lt;/li&gt;
  &lt;li&gt;For training processes, DGL provides the &lt;strong&gt;DistGraph&lt;/strong&gt;, &lt;strong&gt;DistTensor&lt;/strong&gt; and
&lt;strong&gt;DistEmbedding&lt;/strong&gt; abstractions for accessing graph structures, node/edge
features and embeddings stored remotely. There is also a convenient
&lt;strong&gt;DistDataLoader&lt;/strong&gt; to get mini-batches from the distributed sampler.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;DGL has performed several optimizations within the entire stack. For example,
when the sampler and target server are located in the same machine, they can
communicate with each other through the local shared-memory, instead of using
IPC or TCP/IP communication. More optimizations are coming in the future
releases. To get started, check out the &lt;a href=&quot;https://docs.dgl.ai/guide/distributed.html&quot;&gt;user guide chapter&lt;/a&gt; for distributed
training and examples for training &lt;a href=&quot;https://github.com/dmlc/dgl/tree/master/examples/pytorch/graphsage/experimental&quot;&gt;GraphSAGE&lt;/a&gt;
and &lt;a href=&quot;https://github.com/dmlc/dgl/tree/master/examples/pytorch/rgcn/experimental&quot;&gt;RGCN&lt;/a&gt;
on the ogbn-paper100M dataset.&lt;/p&gt;

&lt;h2 id=&quot;further-readings&quot;&gt;Further readings&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;Our updated white paper available at: &lt;a href=&quot;https://arxiv.org/abs/1909.01315&quot;&gt;https://arxiv.org/abs/1909.01315&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Release note: &lt;a href=&quot;https://github.com/dmlc/dgl/releases/tag/0.5.0&quot;&gt;https://github.com/dmlc/dgl/releases/tag/0.5.0&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</content><author><name>DGLTeam</name></author><category term="release" /><category term="release" /><summary type="html">The recent DGL 0.5 release is a major update on many aspects of the project including documentation, APIs, system speed and scalability. This article highlights some of the new features and enhancements.</summary></entry></feed>