---
layout: post
title: "GNN training acceleration with BFloat16 data type on CPU"
date: 2024-08-18
category: blog
post_image: /assets/images/posts/2022-11-28-ngnn/ngnn_title.png
abstract: |
  Graph neural networks (GNN) have achieved state-of-the-art performance on
  various industrial tasks. However, most GNN operations are memory-bound and
  require a significant amount of RAM. To tackle this problem well known
  technique to reduce tensor size by using small data type is proposed for
  memory efficiency optimization of GNN training on Intel® Xeon® Scalable
  processors with Bfloat16. The proposed approach could achieve outstanding
  optimization on various GNN models, covering a wide range of datasets, which
  speeds up the training by up to 5×.
authors:
  - name: Ilia Taraban
    url: https://github.com/itaraban
tags: blog
---

Graph neural networks (GNN) have achieved state-of-the-art performance on
various industrial tasks. However, most GNN operations are memory-bound and
require a significant amount of RAM. To tackle this problem well known
technique to reduce tensor size by using small data type is proposed for
memory efficiency optimization of GNN training on Intel® Xeon® Scalable
processors with Bfloat16. The proposed approach could achieve outstanding
optimization on various GNN models, covering a wide range of datasets, which
speeds up the training by up to 5×.

## Bfloat16 data type

Bfloat16 is a half-precision data type. It differs from the default float data
type only in mantissa length, which is 7 bits long compared to 23 bits.

![bfloat16](/assets/images/posts/2024-08-10-bfloat16/bfloat16.png)

Bfloat16 was developed by the Google Brain team and is currently used widely in
DNN and other AI applications. A lot of devices natively support bfloat16
starting from GPUs and AI accelerators and ending with CPUs. Even compilers such
as GCC, and LLVM are enabling this data type in the latest [C/C++ standards](https://en.cppreference.com/w/cpp/types/floating-point).
According to the Google Brain team exponent is more valuable for training and ML
operations, so such reduction of mantissa bits will preserve the accuracy of the
model and at the same time provide the same performance as other half-precision
data types. Another advantage of using bfloat16 data type is the simplicity of
conversion between bfloat16 and float.

## Bfloat16 CPU acceleration

Starting from the 3rd Gen Intel® Xeon® Scalable processor (codenamed Cooper Lake)
x86 CPU natively supports bfloat16. It was enabled via the Intel® Advanced Vector
Extensions-512 (Intel® AVX-512): AVX512_BF16 vector instruction set, which like
other AVX512 instructions provides basic operations: dot product and conversion
functions.
In the latest 4th Gen Intel® Xeon® Scalable processors (codename Sapphire Rapids),
the Intel® [AMX](https://www.intel.com/content/www/us/en/products/docs/accelerator-engines/advanced-matrix-extensions/overview.html)
instruction set was introduced to further improve 16-bit and 8-bit matrix
performance. In this instruction set “tile” instructions were added, which operate
on special "tile" 2D registers. Currently, this instruction set has only tile
matrix multiply unit (TMUL) support, that can perform matrix multiplication for
bfloat16 and int8 data types.
In the next Intel Xeon generations, starting from Granite Rapids, in addition to
bfloat16, fp16 will be supported.

## Bfloat16 in DGL

Recently bfloat16 support was added to DGL library (starting from [DGL version 1.0.0](https://github.com/dmlc/dgl/releases/tag/1.0.0)
for Nvidia GPU and [DGL version 1.1.0](https://github.com/dmlc/dgl/releases/tag/1.1.0)
for CPU), so it is possible to use it in model training and inference for both
CPU and GPU devices.
Examples of DGL API, which will help to convert the graph and the model to
bfloat16 data type:

```python
# Convert graph, model, and graph features to bfloat16
g = dgl.to_bfloat16(g)
feat = feat.to(dtype=torch.bfloat16)
model = model.to(dtype=torch.bfloat16)
```

The following example trains [GraphSAGE](https://snap.stanford.edu/graphsage/)
with bfloat16 using the provided API:

```python
import torch
import torch.nn as nn
import torch.nn.functional as F
import dgl
from dgl.data import CiteseerGraphDataset
from dgl.nn import SAGEConv
from dgl.transforms import AddSelfLoop


class SAGE(nn.Module):
    def __init__(self, in_size, hid_size, out_size):
        super().__init__()
        self.layers = nn.ModuleList()
        # two-layer SAGE
        self.layers.append(SAGEConv(in_size, hid_size, "gcn"))
        self.layers.append(SAGEConv(hid_size, out_size, "gcn"))
        self.dropout = nn.Dropout(0.5)


    def forward(self, graph, features):
        h = self.dropout(features)
        for l, layer in enumerate(self.layers):
            h = layer(graph, h)
            if l != len(self.layers) - 1:
                h = F.relu(h)
                h = self.dropout(h)
            return h


# Data loading
transform = AddSelfLoop()
data = CiteseerGraphDataset(transform=transform)


g = data[0]
g = g.int()
train_mask = g.ndata['train_mask']
feat = g.ndata['feat']
label = g.ndata['label']


in_size = feat.shape[1]
hid_size = 16
out_size = data.num_classes
model = SAGE(in_size, hid_size, out_size)


# Convert model and graph to bfloat16
g = dgl.to_bfloat16(g)
feat = feat.to(dtype=torch.bfloat16)
model = model.to(dtype=torch.bfloat16)


model.train()


# Create optimizer
optimizer = torch.optim.Adam(model.parameters(), lr=1e-2, weight_decay=5e-4)
loss_fcn = nn.CrossEntropyLoss()


for epoch in range(100):
    logits = model(g, feat)
    loss = loss_fcn(logits[train_mask], label[train_mask])


    loss.backward()
    optimizer.step()


    print('Epoch {} | Loss {}'.format(epoch, loss.item()))
```

## Experimental results

The most popular examples have been tested – [GCN](https://arxiv.org/abs/1312.6203),
[GraphSAGE](https://arxiv.org/abs/1706.02216). For full graph training, basic
datasets were chosen, while for the mini-batch approach datasets from [OGB](https://ogb.stanford.edu/docs/nodeprop/)
were used, the sizes of which significantly exceed the sizes of the basic ones.
For instance, ogbn-products has around 2.5 million nodes and 61 million edges,
whereas ogbn-papers100M has 111 million nodes and 1.6 million edges. Table 1.
demonstrates accuracy which is similar for float and bfloat16 or has not been
changed significantly.

<table style="text-align: center;">
   <tr>
      <th>Model</th>
      <th>Dataset</th>
      <th>Test accuracy(float)</th>
      <th>Test accuracy(bfloat16)</th>
   </tr>
   <tr>
      <td>gcn</td>
      <td>citeseer</td>
      <td>71%</td>
      <td>71%</td>
   </tr>
   <tr>
      <td>gcn</td>
      <td>cora</td>
      <td>81%</td>
      <td>81%</td>
   </tr>
   <tr>
      <td>gcn</td>
      <td>pubmed</td>
      <td>79%</td>
      <td>79%</td>
   </tr>
   <tr>
      <td>graphsage</td>
      <td>citeseer</td>
      <td>71%</td>
      <td>71%</td>
   </tr>
   <tr>
      <td>graphsage</td>
      <td>cora</td>
      <td>81%</td>
      <td>81%</td>
   </tr>
   <tr>
      <td>graphsage</td>
      <td>pubmed</td>
      <td>78%</td>
      <td>78%</td>
   </tr>
   <tr>
      <td>gcn minibatch</td>
      <td>ogbn-paper100M</td>
      <td>57%</td>
      <td>57%</td>
   </tr>
   <tr>
      <td>gcn minibatch</td>
      <td>ogbn-products</td>
      <td>78%</td>
      <td>78%</td>
   </tr>
   <tr>
      <td>graphsage minibatch</td>
      <td>ogbn-paper100M</td>
      <td>62%</td>
      <td>61%</td>
   </tr>
   <tr>
      <td>graphsage minibatch</td>
      <td>ogbn-products</td>
      <td>76%</td>
      <td>74%</td>
   </tr>
</table>

The plots below show results on [AWS r6i](https://aws.amazon.com/ec2/instance-types/r6i/)
powered by 3rd Generation Intel Xeon Scalable processors (codename Ice Lake),
which does not have native bfloat16 instruction, and results on [AWS r7iz](https://aws.amazon.com/ec2/instance-types/r7iz/)
4th Generation Intel Xeon Scalable-based (Sapphire Rapids) instances, which has
native support for both AVX512_BF16 and AMX. In both experiments, the number of
threads is limited to 16, which is the best-known number of threads for a single
run on Intel® Xeon®.

![plot_1](/assets/images/posts/2024-08-10-bfloat16/plot_1.png){: width="800x" .aligncenter}
![plot_2](/assets/images/posts/2024-08-10-bfloat16/plot_2.png){: width="800x" .aligncenter}

The efficiency of GNN training has been enhanced on both types of Intel® Xeon®
instances with bfloat16. Notably, for basic datasets on the AWS r6i, there was
an improvement in performance of up to 32%. Similarly, for basic datasets on the
r7iz accelerated by Intel® AMX machine, the use of bfloat16 led to an improvement
in training performance of up to 92%.
When discussing the results of [OGB](https://ogb.stanford.edu/docs/nodeprop/)
datasets, which are notably larger in size, performance improved by up to 2.89
times on the r6i and up to 5.04 times on the r7iz. On the provided plots it was
demonstrated that all training steps experienced improvements in performance.
The most significant impact was observed in the forward pass, which was up to
12.7 times faster when utilizing bfloat16.

![plot_3](/assets/images/posts/2024-08-10-bfloat16/plot_3.png){: width="800x" .aligncenter}
![plot_4](/assets/images/posts/2024-08-10-bfloat16/plot_2.png){: width="800x" .aligncenter}
![plot_5](/assets/images/posts/2024-08-10-bfloat16/plot_5.png){: width="800x" .aligncenter}
![plot_6](/assets/images/posts/2024-08-10-bfloat16/plot_6.png){: width="800x" .aligncenter}

## Conclusion
Using the bfloat16 data type is strongly recommended for GNN training on Sapphire
Rapids and earlier generations of Intel Xeon Scalable processors to enhance
performance. Even on Ice Lake, bfloat16 enhances the efficiency of memory-bound
operations and reduces training costs.

Nevertheless, certain methods within DL frameworks may not fully support or
optimally use CPU bfloat16 instructions. In such situations, we advise evaluating
both float and bfloat16 performance over a small number of epochs to determine
the optimal choice.

